[{"content":"More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba Introduction 本文主要讲述了盘古分布式存储十年来的一个演变历程，主要分为了三个阶段，分别是1.0，2.0的阶段一和2.0的阶段二。\n在盘古1.0，此时的盘古主要是提供面向卷的存储服务(volume-oriented storage service provision)，在这个阶段在硬件上使用的是传统的hdd，操作系统上使用的是内核空间的文件系统，最终主要的性能瓶颈时HDD和网络带宽。这个阶段有点像google的GFS 在盘古2.0的第一阶段，主要工作就是通过拥抱新硬件来解决1.0时期的性能瓶颈问题，通过引入SSD和RDMA技术，以提供高性能低延迟的存储服务。在这个阶段， 存储架构上采用通用的追加型持久层，并且将chunk的设计为独立类型。 在os层面将原本的内核空间文件系统转换为用户空间（user-space storage operatiing system USSOS），使用run-to-completion线程模型 提供SLA保证 在盘古2.0的第二阶段，将业务模式更新为性能导向，并且尝试打破网络，内存，CPU上的性能瓶颈，采用的解决方式分别为： 减少流量放大率 使用RDCA(remote direct cache access) 减少序列化和反序列化过程的计算负担，将部分cpu的计算职能移动到FPGA上，引入 CPU 等待指令来同步超线程 Background Overview of Pangu 作为大规模的分布式存储系统，主要由三部分组成：Pangu Core,Pangu Service,Pangu Monitoring,而核心就是盘古Core，架构图如下，乍眼一看和GFS差不多： client主要负责和上层的服务，如OSS,NAS进行对接，接受请求并和master与chunkserver之间进行通信，来完成请求。并且在备份管理，SLA保证，数据一致性上都起到了重要作用 master和GPS当中的master一样，主要是管理元数据的，分为两部分，namespace service和stream meta service，namespace负责文件的信息，目录树和namespace，stream则是负责文件到chunk的一个映射。stream是一组chunk的抽象，在同一个stream当中的chunk属于同一个文件，通过这种方式来完成文件到chunk的一个映射。不过在这片文中并没有具体展开master的实现细节，盘古发过很多篇paper，可能在别的中有所提及 Chunkserver 作为真正存储文件的服务器，使用了 user-space storage file system，和追加类型的存储引擎 (类 bitcask 或者 LSM-Tree 类型，其中会有 gc 的过程)。在备份上，最初使用 3 备份的方式，不过在 2.0 的设计过程当中，逐步使用纠删码来代替掉 3 备份以缩小流量放大率 设计目标\n低延迟 高吞吐 对所有业务提供统一的高性能支持 Phase One: Embracing SSD and RDMA Append-only File System 在持久化存储上，盘古提供统一的，只追加的持久层，原本对于不同的服务，盘古提供不同的接口，这样额外增加了系统的复杂性。如今，盘古引入了统一的文件类型 FlatLogFile，提供了只追加的语义，对于上层服务来说，提供的是一个key-value类型的接口。并且会有gc机制来处理历史数据。 重量级客户端：如上面所说的，客户端负责和 chunkserver 和 master 之间进行通信来提供服务，除此之外，还会负责复制和备份的相关操作，(3 备份或者纠删码) chunkserver\n实现了独立的(self-contained)chunk布局，即chunk存储数据和元数据，而不是像经典的文件系统如linux ext4那样分为block和inode，这样做的好处是可以一次写入所有的数据，而不是原本的两次。\n一个chunk当中含有多个基础单位，每个基础单位包含了数据，padding填充，和footer三部分，而footer当中存储chunk的元数据，如chunk id，chunk length CRC校验码，布局如下： Metadata Operation Optimization\n主要提出了六个方向：\n并行元数据处理，如客户端可以进行并行的路径解析 变长的大型chunk，有三个好处：减少元数据的数量，降低由于client频繁访问chunks带来的IO延迟，提升SSD的寿命。而过大的chunk则会导致碎片化的空间浪费，因此采用了变长的方式(1MB-2GB)，但实际上95%的chunk大小都为64MB(和GFS直接设定为64MB差不多) 在client缓存chunk信息，在client端建立一个关于metadata的缓冲池，当缓存未命中或者缓存了陈旧数据时(chunkserver给予回复时一并告知)，和master进行通信来获取最新的metadata 批处理的形式来处理chunk信息：client将一小段时间内的多个chunk的请求进行聚合然后批量发给master 预测性提前获取：获取一个chunk内容时，预测性的获取相关的chunk，存储在客户端，从而减少对chunk server发起请求 数据搭载以减少 1 个 RTT，将 chunk 创建请求和要写入的数据合并为一个请求并将其发送到 chunkserver。 Chunkserver USSOS 文中指出，传统的kernel-space文件系统会存在频繁的系统中断导致消耗CPU资源的情况，和某些数据在user space和kernel space当中存储了两份。为解决这类问题，通过绕过内核(kernel-bypassing)的方式设计了USSOS\nuser-level的内存管理： 建立一块使用huge-page的空间作为网络栈和存储栈之间的共享内存，从而减少拷贝的开销\n任务调度：\n由于采用了run-to-completion的线程模型，对于长时间运行会阻塞后面其他任务的，移动至后台线程运行 优先级调度：建立不同的优先级队列，根据QoS目标将不同的任务放到对应的队列当中 NAPI：借鉴 Linux 的 NAPI，采用 polling（轮询） + event-driven（事件驱动） 相结合的方式，网卡对应了一个文件描述符，当有任务到达时通过文件描述符触发中断唤醒网卡，即事件驱动，网卡处理任务，并进入轮询模式，不断询问是否还有后续任务，如果有则可以快速处理，无需进行中断处理；如果一段时间没有后续任务，就回到事件驱动模式，等待中断 Append-Only USSFS\n对Append-Only进行一个总结：\n充分利用self-contained(自包含？独立？)的chunk来减少数据操作次数 不存在如inode和文件目录之间的关系，所有的操作都需要记录在log files当中，而相关的metadata可以通过重做日志来重建 使用轮询模式代替Ext4等中断通知机制，最大限度发挥SSD的性能 High Performance SLA Guarantee chasing\n当2 x MinCopy \u0026gt; MaxCopy时，允许MaxCopy副本中的MinCopy成功写入chunkservers时，它允许客户端向应用程序返回成功，即三个副本的情况下，可以两个chunkserver完成写入就允许返回success，此时client会将chunk在内存当中保留时间段t(ms级)：\n如果在这个时间段内能够成功得到响应，即可将chunk从内存当中移除，结束此次请求 如果无法得到响应，取决于未完成写入的部分的大小k： 如果 \u0026gt; k，则封存这个chunk，之后不对这个chunk进行写入操作，从另外两个server当中复制数据到一个新的chunkserver当中 如果 \u0026lt; k，则尝试对 chunk 3 进行重试 chasing设计的出发点并不是容错，而是为了在保证数据不丢失的情况下，尽可能的降低响应的延迟，使用chasing这个词来进行描述，指的就是前两个chunkserver已经完成了写入，而第三个server还在补齐进度的这样一个过程，而在这个补齐的过程当中，是存在第三个备份在client的，在chasing的过程当中，之后client chunkserver1 chunkserver2全部挂掉之后，数据才会彻底丢失。\nNon-stop write\n这一部分补充了容错机制，和chasing组成了完整的写入规则。如果只是单纯的写入失败，服务器仍可正常读取的话，那么就封存当前的chunk，分配一个新的chunk来写入未完成的部分，而如果是chunk当中的数据损坏，或者说chunkserver宕机，那么就从已成功写入的chunk当中使用后台流量去拷贝一份到新的chunk当中\nBackup Read\nclient在收到响应之前，会向其他的chunkserver发送读请求，以防止原本的chunkserver宕机，从而减少读请求的延迟\nBlacklisting\n为提高服务质量，设置了黑名单制度，分两种情况：\n无法提供服务的添加到确定性黑名单当中，如果可以提供服务则移除 延迟过高的则加入到非确定性黑名单当中，基于延迟确定是否移除 Phase Two: Adapting to PerformanceOriented Business Model 这个阶段主要考虑如何解决性能的瓶颈问题，分别是网络瓶颈，内存瓶颈和CPU瓶颈\nNetWork Bottleneck 除了简单的扩大网络带宽以外，这一部分主要对流量放大率进行优化，减少需要传输的数据量：\n通过纠删码(EC)来替代原本的三备份形式，EC(4,2)配置指的是使用将数据分为四个分片，使用其中两个分片就可以恢复原本的数据 允许在只存储相当于原始数据1.5倍的总数据量的情况下，容忍任意两个分片的丢失 对FlatLogFile进行压缩：LZ4算法 动态分配带宽：如接受请求和后台带宽，晚上进行GC任务较多时就给后台分配较大的带宽 Memory Bottleneck 使用多块较小容量的内存以充分利用内存通道的带宽，来代替一整块内存 将背景流量也使用RDMA，提高传输效率 RDCA CPU Bottleneck 混合RPC：以往使用protobuf进行序列化和反序列化这个过程需要耗费30%的CPU开销，而这种情况只会出现在少量的rpc类型当中，因此采用混合rpc，部分情况直接传输原始结构，不进行序列化 使用cpu wait以支持超线程 软硬件协同设计，即将一部分的计算工作转移至FPGA来完成，以减少CPU的负载。 ","permalink":"http://itfischer.space/en/posts/tech/pangu2.0/","summary":"More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba Introduction 本文主要讲述了盘古分布式存储十年来的一个演变历程，主要分为了三个阶段，分别是1.0，2.0的阶段一和2.0的阶段二。 在盘","title":"More Than Capacity: Performance-oriented Evolution of Pangu in Alibaba"},{"content":"由于Miniob的整体架构并没有做出很大的变化，并且目前知乎上也已经有其他人写的架构分析文章，我在这里就不过多介绍架构相关的了，只说一下我自己的一些实现和遇到的bug以及对应的修复方式。\n负责题目 负责题目：\ndate drop-table update join-tables null text big write big query update-mvcc 初赛附加题 共计210分 由于队友龙哥太猛了，一个人搞完了所有带嵌套子语句的题，并且做了好几个mid的题，所以留给我实现的内容也不太多，并且由于我未来研究生阶段主要做的是存储方向的，因此这次初赛当中我也主要负责存储相关的内容，如record-manager的相关改造，和新类型的支持。\n此外，在实现null和update-mvcc时，发现了Miniob目前存在的两个bug，和来哥沟通之后，也是提了pr把我的解决方案给合并到了主分支当中，也算是做一些开源贡献了。\n赛程安排 九月份就把去年ob的代码给拉下来了，当时主要在搞预推免的事，加上懒了，一直在本地吃灰。真正开始搞的时候大概是开赛前一周，当时双开了rcore,最后选择是献祭rcore全力搞ob，rcore写完rustlings之后便没有再推进。\n开赛前一周 纯坐牢，配环境就花了一天，sudo执行bash脚本编译出来的需要sudo才能启动，而vscode的调试模式又不支持sudo，(后来发现把bash脚本里面的下载第三方库的部分注释掉可以不sudo执行，呆)，无奈换了clion。后面又因为队友的一些语法在Mac支持不是很好，换成了linux作为开发机，之后花了一点时间缕清架构和执行流程，只做了date和drop-table。\n第一周 继续坐牢，依旧没写几个题，大多数时间花在了研究lex和bison(本科阶段没学过编译原理)，和expression的相关内容，只写完了join tables和接手了队友写了一半的update\n第二周 算是熟悉整个系统和sql解析了，进度上快了一些，写完了null,完善了update的多字段支持，和text，顺手搞了附加题\n第三周 花了两天时间搞了一下mvcc，通过所有测试，提前收队。\n思路分享 drop table 大概是最简单的一个题了，只需要把create table给逆向操作就可以了，官网也给出了详细的指导，大致思路分为三部分：\n删除xxx.table 表的元数据文件 删除xxx.data数据文件 删除xxx.index索引 除此之外做一些简单的异常处理就可以了，校验一下表名是否存在等。 Date 今年miniob的代码结构相比于22年要优化了不少，但是Date涉及到要改动的地方依旧不少，主要有：\n在lex和yacc当中添加对 date和日期格式的匹配支持 在value当中添加date类型和表示形式 考虑Date的具体存储方法 为 Date 添加初始化，校验，比较等功能 解析 主要需要添加两个类型的解析，一个是date关键字，一个是yyyy-mm-dd格式的字符串匹配，前者直接定义一个token即可，后者可以采用正则的形式来处理，对于日期字符串的正则匹配，需要注意的是范围需要从0000-00-00到9999-99-99都匹配上，否则如13月这种错误的日志就会匹配到普通字符串上，从而导致后期无法进行校验处理。其他的按照官方手册实现即可\n存储与表示\n在存储上，我选择了使用一个yyyymmdd的int32来进行存储这样只需要四个字节，比字符串要节省空间。\n在内存表示上，我选择了定义一个Date类，并将相关的相关的逻辑操作都封装在这里面，如类型转换，合法性校验，比较，to_string等。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class Date { public: Date() = default; Date(const char *str); Date(int data); // Date(int year,int month,int day); bool isValid() const { return valid; } bool operator\u0026lt;(const Date \u0026amp;that) const; bool operator==(const Date \u0026amp;that) const; int compare(const Date \u0026amp;that) const; std::string to_string() const; int to_int() const; void parse_date_int(int date); bool isDateValid() const; bool valid{}; int year{}; int month{}; int day{}; std::string format(const std::string \u0026amp;formatStr) const； }; Value 在value当中，添加了一个Date字段来存储一个Date类，并且把转换为int的date存放到num_value_当中，之后.data()用来传输数据和持久化时直接返回这个num_value_即可。\n到这date基本就实现完了，似乎今年miniob的代码重构了不少，往年说还需要更改B+树部分的今年都不需要了。\ntypecast 说Join之前先说一下typecast，今年typecast没有作为一个单独的题目出现，但是也并没有给提供很完善的实现，在实现join时发现了join对typecast强依赖，涉及到很多int和string，float和string之间的join。主要需要为string和其他的类型之间提供转换。\n在github的wiki当中给出了标准，照着实现就好\n字符串转数字\n如果字符串刚好是一个数字，则转换为对应的数字（如'1\u0026rsquo;转换为1，\u0026lsquo;2.1\u0026rsquo;转换为2.1）; 如果字符串的前缀是一个数字，则转换前缀数字部分为数字（如'1a1\u0026rsquo;转换为1，\u0026lsquo;2.1a\u0026rsquo;转换为2.1）; 如果字符串前缀不是任何合法的数字，则转换为0（不需要考虑前导符号 \u0026lsquo;+\u0026rsquo; \u0026lsquo;-\u0026rsquo;）； 问题： 如果转换数字溢出怎么处理?（不考虑） 是否考虑十六进制/八进制?（不考虑） Join Tables 算是mid里面最简单的几个之一了，但是依赖不少其他的相关内容，如typecast和expression，今年的miniob已经实现了select tables，因此只需要再次基础上实现join即可。\n先从解析开始说，join主要分为三部分，左表，右表，和条件，通过inner join on来切分即可，并且由于可能存在多张表之间的join，因此这个解析为一个递归的过程，可以先递归的解析inner join 后面的内容，之后再将最初的左表添加到其中。在条件的解析上，队友定义了conditionTree的解析方式，可以很方便的解析多个condition，我这里就直接拿来用了。解析的格式如下：\n1 2 inner_join_list: | INNER JOIN table_name ON condition_tree inner_join_list 最终的解析结果可以得到n个table和n-1个conditon_tree，之后在select_stmt的create当中按照和filter_stmt相同的方法来创建一个join_stmt即可。\n最后改写一下join_operator当中的逻辑即可，这一部分反而是工作量最小的。 23年的数据量并不大，并不存在前两年的6表join的情况，因此只需要实现最基础的NLJ，甚至我们在重构expression的时候取消了谓词下推依旧可以通过测试。\nbug\n在调试过程中，发现了一个miniob当前存在的bug，join operator当中在匹配时，每当右表匹配完一轮之后都要调用table_scan的close()和open()函数，而在这其中，open()当中又会调用一个set schema的函数，在这其中，是采用push_back的方式进行初始化的，但是在close()当中又没有清空掉field数组，就导致，每次open，都会向里面重新添加一遍field，从而导致在进行多轮遍历之后，tuple里面会有几十个field之多，不过在物化的过程中，只会根据record去获取value，这样并不会影响正确性，但是会导致平白无故的添加一堆无用的数据，我在print tuple.to_string时都惊了，也可能正是因为不影响正确性，这个bug也没有被发现，正确的做法应该是在set_schema时先重置再添加\n1 2 3 4 5 6 7 8 9 void set_schema(const Table *table, const std::vector\u0026lt;FieldMeta\u0026gt; *fields) { table_ = table; this-\u0026gt;speces_.clear(); this-\u0026gt;speces_.reserve(fields-\u0026gt;size()); for (const FieldMeta \u0026amp;field : *fields) { speces_.push_back(new FieldExpr(table, \u0026amp;field)); } } Update update比较简单，先删后增即可，注意在big query和big write当中需要支持update多个字段，其他的参照delete实现即可，应该是最简单operator了，用来熟悉一下整个sql的流程不错。\nbig write / query update加上多字段支持之后直接过掉big query和big write，直接+60，恰分恰爽了。\nnull 参考往年的思路，选择在sys_field后添加一个null_field来表示null(个人感觉单纯用一个magic number表示null也不是不行，单纯想过test应该是可以过的)。\nnull_field为一个长度为32的bitmap，可以表示后面32个field是否为null，完全足够，在存储的时候直接使用一个4个字节的chars进行存储即可。\nnull有这么2个问题：\nupdate时如果从index = 0开始物化value，会读到null，之后再make_record时就会把null又给扔进去当成了一个普通的field，导致record里面有两个null_field，需要给一个sys_field_num + null_field_num的一个offset，另外一种解决方法就是重写cell_at函数，在其中给一个offset，不对外暴露sys_field和null_field Null 的设置时机，刚从磁盘当中读取出来的 record 是不具有 null 属性的，需要在一个合适的时机通过 bitmap 来恢复其 null 属性，最开始定义在 project operator 里面，后面发现有很多依赖 null 的 operator 在 project operator 下面，因此把这个设置 null 的过程移动到了 RowTuple 到 set_record 和 cell_at 里面，在 set_record 时，从中解析出 bitmap，之后 cell_at 时根据 bitmap 去判断是否为 null，是否允许为 null 到逻辑也在这里处理，不过既然能正确写入，这里一般是不会出问题的 此外，null还需要Field和Value为其提供支持，添加字段来表示是否允许为null和是否为null，总之null需要改动的部分非常多，当时commit时看了一眼大概修改了20个文件。。麻了 今年null的测试依赖也很多，需要实现了update-select，和unique之后才能过，而unique又依赖multi index。\ntext 逻辑上比较简单，但是坑不少。\n逻辑上采用溢出页的方式处理比较简单，写入时按需额外去申请几个page用来存储text数据，record本身存储page_num，读取时再根据page_num去获取到真实的text内存。 今年的要求变了，text的最大长度变成了和MySQL一致的64KB，但是本身的PAGE_SIZE也从4KB变成了8KB，因此如果算上header的话，需要9个溢出页来进行存储，最长的情况为前8个page全部存满，最后一个page存部分。\n因此record当中的field就需要9个int来存储，整个field占36个字节，没有用上的page在存储时就向record里面添加INVALID_PAGE_NUM，读取的时候读取到INVALID_PAGE_NUM就截断即可。 但是实现的时候在长度上有两个问题，一个是obclient无法接受超过4KB的text，超长会截断，这个需要下载readline，之后重新编译，还有一个是发送网络请求时的buffer size为8KB，全文搜索长度，之后在后面加个0就行。\n在进行删除和更新上，需要首先删除溢出页当中的内容，之后再删除掉该条record，删除的话可以有两种思路，一是将该page通过init_empty_page进行初始化，之后加入到free_list当中，而是可以使用dispose将其删除，之后用来重新分配。不过我都没实现，谁让不检测呢 :)\nUpdate mvcc 先说说MVCC本身，多版本并发控制，但是严格意义上来说并不是一个完整的并发控制协议，MVCC最大的特点是保证了读写之间不冲突，写请求不会因为正在读而阻塞写入，而读请求可以根据自身版本去读取到对自己可见的版本数据，不会被正在执行的写入阻塞。\n之所以说MVCC并不是一个完整的并发控制，是因为MVCC并没有办法解决写写之间的冲突，因此需要其他的手段来加以辅助，常见的手段有基于时间戳或者基于锁的，Miniob当中的实现方式比较粗暴，只要有两个事务在更新同一个record，那么后一个事务在更新时就返回failture。\n而MVCC当中的版本是基于tid的，在开启了MVCC模式之后，每一条record会生成两个sys_field，分别存储begin和end，来标识一个事务的可见性，这里似乎并没有一个统一的标准，只要能满足MVCC协议本身的要求即可，在Miniob当中的设置如下：\nrecord通过begin和end 两个id进行状态管理，begin用于表示事务开始的时间，end为事务结束的时间，对于一条record,当一个事务开始时，新的record的begin设置为-trx_id，end为max_int32,表示事务写入但未提交，而删除时则将end设置为-trx_id，表示删除但未提交，写入操作提交时，将begin设置为trx_id，删除操作提交时将end设置为trx_id，最终会产生五种状态：\nbegin_xid end_xid 自身可见 其他事务可见 说明 -trx_id MAX 可见 不可见 由当前事务写入，但还未提交，对其他事务不可见 trx_id MAX 已提交 对新事务可见 写入并且已经提交，对之后的新事务可见 任意正数 trx_id 已提交 旧事务可见，新事务不可见 在当前事务中进行删除，并事务提交 任意正数 -trx_id 不可见 可见 在当前事务中进行删除，未提交 -trx_id -trx_id 不可见 不可见 由当前事务进行写入，并且又被当前事务删除，并且未提交 其中，已提交指的就是当前事务已经结束，自然不存在什么可见不可见的问题。而Miniob当中的隔离级别应当是serializable，因为未提交的事务的 begin \u0026lt; 0，因此是永远无法读取到新写入的record，因此是不存在幻读情况的。\n对于record是否可见的判断，在visit_record当中提供了一段代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 if (begin_xid \u0026gt; 0 \u0026amp;\u0026amp; end_xid \u0026gt; 0) { if (trx_id_ \u0026gt;= begin_xid \u0026amp;\u0026amp; trx_id_ \u0026lt;= end_xid) { rc = RC::SUCCESS; } else { rc = RC::RECORD_INVISIBLE; } } else if (begin_xid \u0026lt; 0) { // begin xid 小于0说明是刚插入而且没有提交的数据 rc = (-begin_xid == trx_id_) ? RC::SUCCESS : RC::RECORD_INVISIBLE; } else if (end_xid \u0026lt; 0) { // end xid 小于0 说明是正在删除但是还没有提交的数据 if (readonly) { // 如果 -end_xid 就是当前事务的事务号，说明是当前事务删除的 rc = (-end_xid != trx_id_) ? RC::SUCCESS : RC::RECORD_INVISIBLE; } else { // 如果当前想要修改此条数据，并且不是当前事务删除的，简单的报错 // 这是事务并发处理的一种方式，非常简单粗暴。其它的并发处理方法，可以等待，或者让客户端重试 // 或者等事务结束后，再检测修改的数据是否有冲突 rc = (-end_xid != trx_id_) ? RC::LOCKED_CONCURRENCY_CONFLICT : RC::RECORD_INVISIBLE; } } 当前存在的问题\n简单来说，就是事务提交时，先插入再删除的记录行为没有得到正确的表达，产生了预期之外的表现\n缺少begin \u0026lt; 0 \u0026amp;\u0026amp; end \u0026lt; 0的处理，导致当前事务中写入的记录无法删除 commit时存在问题，insert之后，delete的记录无法添加到operation里面，operation为一个set,set的逻辑是对于冲突的key拒绝进行添加，会导致即便修复了1, 在commit之后会出现begin = commit_id,end = -trx_id的情况，理论上这种情况不应该出现，因为delete的commit行为没有得到正确执行，如果delete的commit行为得到正确执行，就会变成begin \u0026gt; 0 \u0026amp;\u0026amp; end \u0026gt; 0的情况，就可以按照版本去读取了。 对于上面说到的第一点，少了对begin \u0026lt; 0 \u0026amp;\u0026amp; end \u0026lt; 0的判断，在这种情况下，为当前事务写入，又被当前事务删除，但是事务还未提交，此时应当对当前事务是不可见的，但是根据目前的条件结构，会返回SUCCESS，这样会导致在当前事务中进行delete但是仍可见的问题，或者当前事务中进行update，但是旧数据还存在的问题。需要补充一条逻辑，在这种情况下返回不可见\n如下面的例子：\n原本(4,4,\u0026lsquo;a\u0026rsquo;)的record是由上一个事务写入的，并且提交(begin_xid \u0026gt; 0)，此时更新就不会出现问题，而(4,761,\u0026lsquo;a\u0026rsquo;)的record是当前事务写入的，还未提交，begin_xid \u0026lt; 0,删除之后end_xid \u0026lt; 0，上面漏了对这种情况的判断，从而导致其又可见。\n对于说到的第二点，表现形式就是如果当前的事务commit之后，这条记录就又可见了 下图是我用初始版的miniob做的测试，可以确认的确存在这个问题 修复方式 描述完产生的问题，再来说一下为什么会有这种问题和解决方式。 产生的原因主要有两个：\nset逻辑错误，原本想的是对于同一条record的操作，后面的覆盖前一条，最后提交时只会执行最后一条，但是c++当中的std::set对于重复key的插入操作会直接拒绝，导致没能够正确的覆盖 覆盖逻辑：因为采用覆盖的方式执行，因此对于在当前事务当中插入的数据，在commit时delete会覆盖掉insert的动作，就会导致一个问题：insert的commit动作没有正确得到执行，出现了begin \u0026lt; 0 end \u0026gt; 0的情况，在原本的tid校验当中没有判断，而commit前的begin \u0026lt; 0 end \u0026lt; 0的情况同样也没有判断 大概想了两版修复方式： 第一版比较复杂，逻辑上感觉更符合mvcc的含义：\n由当前事务写入的记录无法被删除：\n当前事务写入的记录为begin = -trx_id，end = MAX,之后由当前记录进行删除之后，变为begin = -trx_id,end = -trx_id，当前visit_record当中缺少对于这种情况的判断\n解决方案：添加对begin \u0026lt; 0 \u0026amp;\u0026amp; end \u0026lt; 0的判断，意义是由当前事务写入，并由当前事务删除，状态为对所有事务都不可见\ncommit动作没有被正常执行\n当前事务使用rid作为key，使用std::unordered_set进行存储，保存当前事务的operation,留在提交时进行执行，同一个rid的operation应当后者覆盖前者，但是set对于已经存在了的key无法正确插入，无法正确覆盖。\n解决方案： 在进行delete操作时进行特判，如果原本存在对应的key的话(即当前事务进行了一次插入操作)，就删除原本的key，重新插入，以达到覆盖的效果\ninsert由于即便insert相同的record,rid也不一致，无需处理。 其他的事务进行的插入不是同一个set,不会产生影响 当前事务插入的其他的记录由于 rid 不一致，无需处理 垃圾回收问题：\n如果在当前事务中删除已经提交的事务写入的记录，不会出现任何问题，因为插入操作在进行提交时会将begin设置为commid_tid，从而有了正确的版本信息，之后delete时，只需要修改end = tid即可，而无论进行回滚还是提交，都可以保证一个正确的状态：\n提交：end = commid_tid，可以根据版本去正确读取 回滚：end = MAX，恢复到刚写入的状态 如果是当前事务写入又由当前事务提交，则存在一定问题，由于在同一个事务当中，同一条记录后面会覆盖前面，从而导致如果进行写入又删除的话，begin会保留-tid的状态，此时无论进行commit还是rollback都只会执行delete的operation，从而该条记录的begin会一直保持-tid，给垃圾回收和正确读取带来困扰。\n可以考虑：\n如果是提交操作，那么就将begin = end = tid，意义为当前事务写入又由当前事务删除，不会影响读取 如果是回滚操作，那么就调用table→delete_record进行真正的删除，由于该记录从未对外暴露过，因此删除不会影响其他事务的读取，并且在逻辑上也符合回滚的概念。 第二版的实现方式比较的取巧，针对问题出现的源头进行修复：就是在trx.delete_record加个特判，如果是自己写入的进行删除，就直接调用delete_record,并且也不添加到operation_里面，并且删除之前的insert操作的operation，由于是自己创建的record，并且又被自己删除，因此就当作其从来没有存在过。\n在这种情况下，既不影响回滚，也不影响gc，最终实现也是采用第二种方式 回到正题，update-mvcc这题其实是不怎么需要实现的，大多数人在完成了unique之后，update-mvcc是可以直接通过的，因为目前miniob已经给出了mvcc的insert和delete实现，组合一下即可。不过由于我们队unique index实现的是一个fake版本，在mvcc模式下并没有办法提供唯一性保证，而我又不想去重构，因此我们队分了两条线进行，队友去重构unique index，我特判表名然后实现一个内存当中的mvcc版的unique index，其实就是把visit_record当中的逻辑对着unique_index实现了一遍，不过最后是我先搞出来了hhh。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 RC Table::insert_record(Record \u0026amp;record) { int decode_index_num; if (table_meta_.name() == mvcc_table_name_) { int index_offset = table_meta_.sys_field_num() + table_meta_.null_field_num(); memcpy(\u0026amp;decode_index_num,record.data() + index_offset * 4,4); if (mvcc_unique_index_.contains(decode_index_num)) { if (is_duplicated(decode_index_num)) { return RC::INVALID_ARGUMENT; } } } else { // ... } } bool Table::is_duplicated(int index_num) { if (!mvcc_unique_index_.contains(index_num)) { return false; } auto current_index = mvcc_unique_index_[index_num]; int begin_tid = current_index.first; int end_tid = current_index.second; bool is_visible; bool is_conflict; if (begin_tid \u0026gt; 0 \u0026amp;\u0026amp; end_tid \u0026gt; 0) { if (trx_-\u0026gt;id() \u0026gt;= begin_tid \u0026amp;\u0026amp; trx_-\u0026gt;id() \u0026lt;= end_tid) { is_visible = true; } else { is_visible = false; } } else if (begin_tid \u0026lt; 0) { if (-begin_tid == trx_-\u0026gt;id() \u0026amp;\u0026amp; end_tid \u0026lt; 0) { is_visible = false; } else if (-begin_tid == trx_-\u0026gt;id()) { is_visible = true; } else { is_visible = false; } } else if (end_tid \u0026lt; 0) { // 由于unique index只需要管理插入，因此不存在read_only的情况 if (-end_tid == trx_-\u0026gt;id()) { is_visible = false; } else { is_conflict = true; } } // 正在有其他的尝试插入，直接拒绝 // 如果存在一条先前可见的数据，也拒绝掉 if (is_conflict || is_visible) { return true; } return false; } 在完成了unique index之后还存在一个问题，就是mvcc模式下,update时，下层的table_scan会把新写入的record又给读出来，从而导致又insert一遍，再加上我最开始visit_record当中begin \u0026lt; 0 end \u0026lt; 0的bug没修，从而导致其会逃过unique的限制，表现结果就是一个update更新了两次。 问题的源头是table_scan会读取到新写入的record，不过当时没仔细看到底为什么，只是修了visit_record和在update当中加了一层限制，即如果原记录和新记录完全一致的话，就不进行更新，这样勉强算是把窟窿给堵上了，最后也是通过了测试。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 if (begin_xid == -trx_id_) { // fix：此处是为了修复由当前事务插入而又被当前事务删除时无法正确删除的问题： // 在当前事务中创建的记录从来未对外暴露过，未来方便今后添加垃圾回收功能，这里选择直接删除真实记录 // 就认为记录从来未存在过，此时无论是commit还是rollback都能得到正确的结果，并且需要清空之前的insert operation,避免事务结束时执行 auto delete_operation = Operation{Operation::Type::INSERT, table, record.rid()}; std::unordered_set\u0026lt;Operation\u0026gt;::size_type delete_result = operations_.erase(delete_operation); ASSERT(delete_result == 1, \u0026#34;failed to delete insert operation,begin_xid=%d, end_xid=%d, tid=%d, rid:%s\u0026#34;, begin_xid, end_xid, trx_id_, record.rid().to_string().c_str()); rc = table-\u0026gt;delete_record(record); ASSERT(rc == RC::SUCCESS, \u0026#34;failed to delete record in table.table id =%d, rid=%s, begin_xid=%d, end_xid=%d, current trx id = %d\u0026#34;, table.-\u0026gt;table_id(), record.rid().to_string().c_str(), begin_xid, end_xid, trx_id_); return rc; } pair\u0026lt;OperationSet::iterator, bool\u0026gt; ret = operations_.insert(Operation(Operation::Type::DELETE, table, record.rid())); if (!ret.second) { LOG_WARN(\u0026#34;failed to insert operation(deletion) into operation set: duplicate\u0026#34;); return RC::INTERNAL; } ","permalink":"http://itfischer.space/en/posts/tech/oceanbase%E5%88%9D%E8%B5%9B/","summary":"由于Miniob的整体架构并没有做出很大的变化，并且目前知乎上也已经有其他人写的架构分析文章，我在这里就不过多介绍架构相关的了，只说一下我自","title":"OceanBase初赛"},{"content":"SPFresh: Incremental In-Place Update for Billion-Scale Vector Search Introduction 本文聚焦于两个方面，一个是采用in-place(即原地更新)的方式更新索引，而另一个则是如何维护高质量的索引。对于之前的ANNS算法，无论是基于图的还是基于簇(cluster)的，通常采用的是重建索引的方式，即积累一段时间的新向量，到达一定程度后重新构建整个索引。但这样带来的问题就是会消耗大量的资源，并且构建索引的过程也会给查询带来延迟。\n这种原地更新是有必要的，在目前的场景下，VDBMS会接收到大量的增量信息，而如果每次都采用重建索引的方式就会带来大量的额外开销。\n目前无论是基于图的，还是簇的，都会在内存当中维护一个\u0026quot;shortcut\u0026quot;，对于图来说，可以是向量的量化形式(DiskANN)，而对于簇的形式，则使用的是质心(SPANN)。在进行搜索时首先使用 shortcut 进行初步搜索，之后再去进行进一步的搜索。因此 shortcut 的质量就会决定最终的搜索结果，如果 shortcut 不能够很好的代表对应的详细数据，如质心无法正确表示 posting list，那最后的搜索质量就会下降，即发生了数据分布倾斜。\n在SPFresh当中，提出了LIRE,a Lightweight Incremental RE-balancing protocol，其核心思想就是利用已经存在的有着良好分区特性的索引。对这类索引的更新只会在小范围影响自己和邻居，因此需要做的就是平衡好此次更新涉及到的这一个小范围。\n目前有三个挑战：\n为了保持较短的搜索延迟，LIRE需要及时拆分和合并分区来维持分区的均匀分布 为了保持较高的搜索精度，LIRE 需要识别导致索引中数据不平衡的最小向量集。应重新分配这些向量以保持高索引质量。 LIRE 的实现应该是轻量级的 对应的解决措施：\n主动拆分和合并分区来维持分区大小的均匀分布 定义了两个基本准则来保证re-balance之后的分区不会违反NPA(rule of nearest neighbor posting assignment) 通过前馈管道的形式进行解耦成两阶段，将reassign的过程从更新的路径上移走 使用基于SSD的user-space的存储引擎来绕过传统存储堆栈，优化存储性能 Background 向量搜索和ANNS在很多地方都已经提及了，这里就不详细描述了，贴一张图 向量索引组织形式\n这里讨论了两个比较主流的索引组织形式，分别是基于图的(Fine-grained graph-based vector indices)和基于簇的(Coarse-grained cluster-based vector indices)，在架构上都是内存-磁盘的混合架构。\nFine-grained graph-based vector indices\n基于图的细粒度向量索引，在一张图当中，使用顶点来代表向量，使用边来表示两个向量的距离，如果两个向量足够近的话，就使用一条边来连接。在进行K临近搜索时，使用贪心算法遍历图，优先搜索距离最近的邻居。\n部分算法的实现选择全部基于内存来实现，这样做的代价就是会带来大量的内存开销。一小部分算法会选择使用内存-磁盘的混合架构，如DiskANN，DiskANN采用的解决方案是在内存当中存储使用PQ量化压缩之后的顶点，以便节省内存开销，并且用于作为一级索引来加速查询。\nCoarse-grained cluster-based vector indices\n基于簇的粗粒度向量索引,将相近的向量管理在同一个簇当中，同一个簇当中的向量使用全连接图，而不同簇之间不存在边。\n比较有代表性的就是SPANN，SPANN在内存当中存储簇的质心，先搜索出K临近的质心，之后再去搜索质心对应的簇，找到符合要求的向量。从而在实现高搜索性能的同时，保证了较低的内存开销。\n相比于局部敏感性哈希和kmeans，SPANN在创建簇时对簇进行了大小上的平衡(基于分层的实现)，并且采用的是重建索引的方式来完成更新(out-of-place)，(在目前的改进方案中，不需要使用out-of-place，但是为了防止数据倾斜的发生，依旧需要周期性的重建索引)代价就是重建索引的高开销，包括时间和空间，时间上不必多说，空间上需要在更新期维护一个副索引，搜索时需要两个索引都进行搜索(就像redis的渐进式rehash，维护了两个哈希表，搜索时检测两个哈希表) 但是如果直接使用in-placed的更新方法，直接在某个簇内追加向量，那么就必然会导致最后的数据分区倾斜的问题，这也是in-place更新方式最先需要解决的。\n设计目标\n在维护大规模索引时保证低资源开销 搜索和更新都实现高吞吐量和低延迟 新向量可以高概率被检索到。 为此，设计了SPFresh，在索引数据结构中进行原地、增量式的更新，以适应数据分布的偏移\nLIRE Protocol Design LIRE基于 SPANN，SPANN的相关内容如下[[SPANN]]\nLIRE: Lightweight Incremental RE-balancing 能够形成良好分区的关键时遵循nearest partition assignment(NPA)：每个向量应该放入最近的posting list中，以便对应的质心能够很好表示该簇当中所有的向量。如下图，A在达到一定大小之后需要分裂，黄色的向量距离最近的质心是B，而B中绿色的向量距离最近的是A2.因此就不能简单的将A拆分为两个分区，而是应该视为对A以及周边簇中所有向量的重新分配。 为了保证不违反NPA原则，LIRE定义了五种基本操作：Insert,Delete,Merge,Split,Reassign\nInsert \u0026amp; Delete\nInsert遵循原本的SPANN的设计，直接将vector插入到对应的簇当中，删除的话可以保证该向量之后都对用户不可见，但没必要真正删除掉，使用一个🪦标记一下即可，等到后续再延迟删除。 Insert和Delete是外部接口，其他作为内部借口由insert delete触发调用\nSplit\n当posting list的长度超出限制，就将其分割成两个较小的部分，这一步会导致违反NPA，因此需要reassign操作。在最初会首先清除当中已经被删除的向量，而如果此时长度还是超出阈值，则进行split。\nMerge\n当posting list过小时，就需要对其进行合并，合并的实际过程是删除当前的posting list和对应的质心，将所有的向量添加到周围临近的posting list当中。这一步同样有可能违反NPA，因此也需要进行reassign\nReassign\n重分配的过程涉及到操作磁盘上的posting list，所以应当尽可能的缩小涉及的范围。对于merge操作，只需要对于被删除中心对应的向量即可。但是由于split创建了两个新的质心，因此就复杂的多，为了不违反NPA原则，定义了两个必要条件(假定距离使用欧几里得距离进行讨论),如果满足其中之一，则需要考虑进行重分配：\n$$D(v,A_o)\\le D(v,A_i) ,\\forall i\\in 1,2$$D为距离函数，$A_o$代表旧的被分割的质心，$A_i$为新创建的质心，这一条意味着如果原本的质心为离向量v最近的质心，并不代表新创建的两个质心为离v最近的质心，周围可能存在更近的质心，样例即为上图。相反的，如果新建的质心距离更短，那么就无需担心(不等条件传递，$D(v,A_i)\\le D(v,A_0)) \\le D(v,B)$ $$D(v,Ai) \\le D(v,A_o) , \\exists i \\in 1,2$$这种情况下,对于B而言需要考虑对其中的元素进行重分配，道理同样是不等条件传递，可能诞生新的更近质心。 由于需要对分裂和周边的postling list的所有向量进行检测，开销是巨大的，为了最小化开销，LIRE 仅通过选择几个 $A_o$ 的最近的postling list来检查附近的postling list进行重新分配检查，在此基础上应用两个条件检查来生成最终的重新分配集。第 5 节中的实验表明，只需少量附近的postling list进行两个必要的条件检查就足以维持索引质量。 Split-Reassign Convergence 在这一部分进行证明：虽然合并和分割的过程会出现级联现象，但是最终会收敛到一个稳定的状态。\n首先merge操作在到达最低阈值之后就会停止，显而易见。\n对于insert操作，假设insert会引发一系列的更改，记为$C_i,C_{i+1},\u0026hellip;,C_{i + N}$,证明收敛性就是证明N是一个有限数。对于集合C而言，有以下的特点：\n$|C| \\le |V|$,V代表整个数据集，C为质心集合，显而易见c的数量 小于等于v的数量 $|C_{i+1}| = |C_i| + 1$: 每次分裂都会删除一个旧的集合，生成两个新的集合，因此每次分裂就是多处一个集合 基于属性二递推有：$|C_{i + N}| = |C_i| = N$，根据属性一，所以有$N \\le V - |C_i|$，因此，既然V是一个有限数，那么N同样是个有限数。\nSPFresh Design and Implementation Overall Architecture 整体架构如图：分别有一个轻量的in-place updater，一个低开销的Local Rebuilder和一个快速存储的Block Controller Updater\n将新的向量添加到对应的posting list的后面，并且维护一个map来记录被删除的向量，即墓碑机制，还用于追踪每个向量的副本。通过增加版本号，它将旧副本标记为已删除。\n如果内存中的版本号大于磁盘上的版本号，则向量已过时，可借此进行GC，使用版本号可以推迟和批量进行GC，从而控制向量清除的I/O开销。实际数据的删除是在local rebuilder部分异步进行的。 向量插入完成后，更新程序会检查posting list的长度，如果长度超过分割限制，则将分割作业发送到local rebuilder\nLocal Rebuilder\n在其中维护了一个任务队列，当中存储split merge reassign任务，然后分发任务给多个后线程去执行。\nsplit由updater触发，当发现长度超过阈值时，并且会根据vector map去删除已经删除的向量。 merge 由Searher触发，当发现长度小于最低阈值时 reassign由split和merge触发 当merge和split完成之后，SPFresh就会去更新内存当中的SPTAG index Block Controller\n存储posting list，提供读写服务，使用SPDK来提供一种直接操作SSD block的接口，来避免使用一些存储引擎导致的读/写放大\nLocal Rebuilder Design 设计local rebuilder的目的就是为了将merge，split，reassign等操作从updater的执行流程当中移除，避免不必要的阻塞，目前update分成了两部分，前端的Updater和后端的Local Rebuilder。\n平衡算法：split过程当中，Local Rebuilder 利用SPANN当中提出的中的多约束平衡聚类算法来生成高质量的质心和平衡的posting list(SPANN当中认为这个算法在N过大的情况下效果不好，使用的是分层平衡，由于只是分成两个，不存在SPANN当中面临的N过大的问题，没必要使用分层)。\nvector map\n前文当中提到了使用一个vector map来记录版本信息，这里详细说一下。\n首先，在SPANN当中指出，为了提升召回率，对于边界上的向量会存储多个备份在不同的postling list(A,B,C)当中。但是在reassign的时候，由于SPFresh希望减少reassign过程的开销，不会处理所以的邻居，例如，在A当中删除某个索引，触发了merge，指波及到了A B D，那么C当中的副本应该被删除但是没删除，后续有可能在找到质心C时又读取到这个向量。\n因此，这这里引入了版本控制，一方面是为了GC，即真正删除的可以通过map找到并清除，另一方面就是为了处理这种漏网之鱼：reassign向量时，我们会在版本映射中增加其版本号，并将原始向量数据及其新版本号附加到目标posting list中\nConcurrent Rebuild 读取postling list不需要锁。因此，识别用于reassign的向量是无锁的，因为它仅搜索索引并检查两个必要条件 在进行reassign的过程当中，是有可能出现将向量添加到一个被删除的postling list当中，此时终止掉ressign并对该向量重新执行。在我们的实验中，只有不到 0.001% 的插入请求遇到了拆分导致的posting缺失问题。因此，中止和重新执行的开销很小。 在reassign过程当中，通过对版本号使用CAS原语来保证并发安全，如果失败同样是放弃并重新执行 Block Controller Design Block Controller基于SPDK实现，最大的特点是直接操作SSD 的block，而不是使用如LSM-Tree等存储引擎，避免无意义的开销\n存储数据布局\n如下图，由三部分组成 Block Mapping、Free Block Pool、Concurrent I/O Request Queue。 Block Mapping存储一个posting ID和SSD block metadata的一个映射，包含偏移量和长度。\nposting采用的是一组tuple的形式进行组织，形式为\u0026lt;vector id, version number, raw vector\u0026gt;,大致会占用3-4个block\nConcurrent I/O Request Queue 使用 SPDK的循环buffer实现的，发送异步的读写请求来达到高吞吐和低延迟。\n对于posting的操作提供了 GET ParallelGET APPEND PUT四个接口。 这里说一说APPEND，APPEND采用的是追加写的方式，只会操作涉及到的最后一个block，以此减少读/写放大的问题。 APPEND首先分配一个新的block，如果posting当中最后一个block不为空就先读进来，然后将新的向量添加到末尾，之后写回到磁盘上，顺带更新内存当中的映射关系 Crash Recovery 这里使用的是snapshot + WAL的形式，快照包含质心索引，updater当中的version map，Block Controller当中的block mapping 和block pool。\nEvaluation 这里简单说一下实现的思路和验证的目标。\n模拟每天大约 1% 的数据删除和插入，由于各类算法的不同，把更新操作需要满足的 QPS 作为指标，提供不同线程数做后台任务，如merge split rebuild index等，然后提供相同的线程数来做查询，比较查询的 QPS 以及 recall，内存占用等，结果如下。 这里花了一部分阐述查询尾延和准确性的问题，SPFresh基于SPANN，在SPANN当中，一段时间的写入会导致数据分区倾斜的问题，因此无论是P99.9的尾延，还是准确率，在一段时间后与SPFresh之间出现了的差距。 另外一个就是验证了在不同的更新策略下，召回率和延迟之间的一个trade-off,而这一部分出现不同的性能表现同样是因为数据分区倾斜 Reassign Range\n在文章的前半部分说明了为了防止在reassign的过程当中造成过大的开销，会限制参与reassign的邻居数量，在实验部分也给出了参数选择的依据，可以看到在64到128时，性能提升微乎其微，因此最终选择64作为参数(不过感觉64也挺大的)\n","permalink":"http://itfischer.space/en/posts/tech/spfresh/","summary":"SPFresh: Incremental In-Place Update for Billion-Scale Vector Search Introduction 本文聚焦于两个方面，一个是采用in-place(即原地更新)的方式更新索引，而另一个则是如何维护高质量的索引。对于之前的A","title":"SPFresh: Incremental In-Place Update for Billion-Scale Vector Search"},{"content":"Manu: A Cloud Native Vector Database Management System Introduction Manu或者其前身Milvus，定位目标是云原生的向量数据库，需要提供基础的向量存储和检索服务，同时，相比传统的云原生DBMS，主要有以下的特点：\n无需支持复杂的事务：传统DBMS通常需要支持多表，多行的复杂事务处理逻辑，但是这在VDBMS当中是没有必要的， 因为非结构化数据通常通过AI模型转化为单一的向量，最基本的ACID就足够了 提供一个可调节的性能和一致性之间的trade-off 提供细粒度的弹性伸缩以避免硬件资源的浪费：VDBMS 涉及到很多硬件加速，如 GPU 或者 FPGA，以及大量的内存。通过将函数层或者逻辑层与硬件层进行解耦，以避免非峰值期的浪费 (参考 snowflake 的设计)，弹性和资源应该在函数层进行管理而不是系统层 log as data\nManu的设计准则为日志即数据，整个系统通过一个日志的订阅/发布系统进行管理，通过日志来完成各个层之间的解耦(decoupling of read from write, stateless from stateful, and storage from computing)\n日志机制上使用了MVCC和time-tick，后面进行详细介绍\n在向量检索上，支持量化、倒排索引、临近图等结构\nBackground And Motivation 这里前半部分说的还是VDBMS那几个老生常谈的特性。其他的话\n首先说明了VDBMS目前的架构是不成熟或者会持续发展的，由于目前VDBMS主要是给AI或者数据驱动类型应用服务的，因此随着这些上层应用的发展，VDBMS也会带动着一同革新。 无需支持复杂事务，基础ACID即可 矢量数据库应用程序需要灵活的性能一致性 trade-off。如在视频推荐的场景下，拿不到最新上传的视频是可以接受的，但是让用户长时间等待是万万不可的。可以通过配置最大的延迟时间来提高系统的吞吐量。这里的设计就有点像搜索引擎了，少检索出那么几条数据又如何呢，这也是早年 GFS 采取弱一致性的原因。 做总结，Manu的设计目标如下：\n支持长时间的迭代更新 一致性可调 良好的弹性 高可用性 高性能 强适应性 The Manu System Schema, Collection, Shard, and Segment schema：数据类型上支持基本数据类型 + vector，一条记录由user field和sys field两部分组成: 其中，label和numerical attribute组成了过滤字段，label就是传统意义上的标签，如衣服，食物等，numerical attribute即为和该条记录相关的数值属性，如身高，价格等。\nsys field的LSN对于用户是不可见的，用于系统内部使用\ncollection: 作为entity的集合，对应DBMS当中的表的角色，但最大的区别是不存在关系的概念，不会和其他的collection之间存在relation，同样也不支持关系代数\nshard: 在插入时，entity会根据ID hash到多个shard当中，但是在manu当中，shard并部署数据的存储(placement)单元\nsegment： 每个shard当中的entity通过segment来进行管理，segment可以有两种状态：\ngrowing：可以向其中添加新的entity，到达512MB转换为sealed，或者一段时间没有向其中添加新的数据 Sealed：不接受新的 entity，为只读的状态 小的segment可以进行合并\n对于shard和segment，这里说的比较简陋，在后面日志部分有详细描述，shard是一个较大的单位，对应了一个存储节点的概念，记录通过哈希找到自己所属的存储节点，而segment则是数据的存储或者管理单元，有点类似于chunk的这个概念。\nSystem Architecture 根据上面描述的种种云原生向量数据库的需求，Manu在架构上进行细粒度的解耦，分为了多个层，每层各司其职，这里的解耦比像snowflake那样的存算分离要彻底一些。\nAccess layer\n位于整个系统的最上层，采用无状态(stateless) + 缓存的设计方式，代理客户端的请求，并将其转发到对应的下层节点上。\nCoordinator layer\n在这一层当中存在四个Coordinator，分别负责不同的工作： Root，Query，Data，Index\nRoot主要处理创建/删除collection的操作(对应 sql当中的ddl)，并且维护collection的元数据 Data负责维护一个collection的详细信息，如segment的路由或者路径，并且可以和底层的data node合作将更新的数据转换为binlog Index 维护索引的元数据和创建索引工作 Query 显而易见 Worker layer\n负责真实的计算任务，节点为无状态的，只会从存储节点获得一份copy来进行计算，各节点之间不存在交互。为了保证可以根据需求扩充，将节点根据任务类型划分为了query data index三种类型\nStorage layer\n使用etcd来存储系统状态和元数据 使用AWS S3作为对象存储，存储segment/shard Log Backbone 日志作为骨架，将解耦的系统组件连接成一个整体。日志使用wal + binlog的组合，wal作为增量部分，bin作为基础部分，二者相互补充。data node订阅wal，然后将row-based的wal转换为column-based的binlog。\n所有会更改系统状态的操作都会记录到日志当中，包括ddl、dml，和一些system coordination的操作(向内存当中加载collecion)。而只读请求不会进入日志。 日志采用的是逻辑日志而非物理日志 日志系统的架构如下： logger通过一致性哈希进行管理，相比于普通的哈希函数，可以得到一个较好的负载均衡并且在添加或者删除节点时影响到的节点较少。 每一个shard对应哈希环上的一个逻辑哈希桶(其中对应多个物理哈希桶)，并且和wal channel一一对应 对于一次请求，首先根据ID进行哈希到对应的logger，再使用TSO(实现方式为混合时钟)进行分配一个全局的LSN，根据LSN划分到对应的segment当中，然后将其写入到WAL当中，此外还会维护一个entity ID到segment ID的一个映射，使用rocksdb进行存储，并缓存在logger当中。 WAL channel本身提供一个发布订阅功能，本身相当于一个消息队列，在实现上使用kafka等，logger相当于发布端，data node订阅该channel，将row-based的wal转换为column-based的binlog。而之所以使用column-based的形式，还是VDBMS的检索服务本质上还是有点类似OLAP场景，列式存储可以提供较好的存储和IO性能。 组件间的消息也通过日志传递，例如，数据节点宣布何时将段写入存储，索引节点宣布何时构建索引。这是因为日志系统提供了一种简单可靠的机制来广播系统事件 Tunable Consistency Manu引入了“delta一致性”（delta consistency），这种一致性模型位于强一致性和最终一致性之间。在这种模型下，读操作返回的是最多在delta时间单位之前产生的最后一个值。这里的“delta”是一个可调整的时间参数，可以根据需要设置。值得注意的是，强一致性和最终一致性可以看作是这个模型的特殊情况，其中delta分别为零和无限大\n为了实现delta一致性，分为两方面：\n首先实现TSO来生成全局唯一的LSN，这里使用了混合时钟，物理部分记录物理时间，逻辑部分记录时间顺序。逻辑部分用于确定事件的先后顺序，而物理部分用于和delta配合来设置容忍的延迟。不过这里TSO是单机实现的，有可能成为整个系统的瓶颈。虽然都使用了HLC，到那时和cockroachdb是不同的。 二是使用了time-tick的策略，简单来说这个实现有点像机枪的曳光弹，每隔几发就添加一发曳光弹来进行标识 作为日志订阅者的query node，需要知道三件事:1)用户设定的delta，2)上次更新的时间，3)查询请求的时间 使用time-tick策略，在日志当中会定期的插入带有时间的特殊日志标志，将最后一次的时间记为 $L_s$ ，而请求又会带上$L_r$，只有满足了$L_r - L_s \u0026lt; \\delta$ 的情况下查询才会返回，否则一直等待 Index Building Manu支持的索引类型如下： Manu当中并没有提出什么新颖的索引类型，在这一段，主要阐述了两种索引构建场景。分别是批处理和流处理：\n批处理发生在对于整个collection构建索引的情况，在这种情况下，index coordinator知道collection涉及的所有的segment的路径，然后指示index node去创建索引。 流处理则是增量情况，索引是在不停止搜索服务的情况下实时异步构建的。当一个Segment积累了足够数量的向量后，其驻留数据节点会密封该Segment并将其作为binlog写入对象存储。然后数据协调器通知索引协调器，索引协调器指示索引节点为该段建立索引 Manu通过bitmap来记录被segment当中删除的向量，而被删除的向量数量达到一定程度后，会重新构建索引 Vector Search 先说几个特性：\n可以使用距离或者相似度函数进行搜索，如欧几里得距离，内积，角距离等 使用cost-based模型来选择执行策略 支持多向量检索 查询加速\nCollection被分为segment，然后在各个segment上进行并行查询，在每个段上进行top-k查询，之后再使用一个聚合函数处理各个段上的top-k向量，聚合得到一个全局的top-k结果\n数据来源\n像之前说的那样，Manu使用的是存算分离的架构，计算节点获取一份copy进行计算。因此查询节点共有三个数据来源：WAL，索引，binlog：\n对于还处于growing持续写入的segment，查询节点可以通过订阅WAL的方式，然后在其中进行暴力扫描得到结果，以求最小延迟。但是暴力扫描的开销依旧很高，给出的对策是将segment划分为切片(slice)，一个切片包含1w条vector，一旦一个切片完成了写入，就对其建立临时索引以加速搜索，大约可以提升10x 而从growing转换为sealed的segment，就可以对其建立完整的索引，并存储在AWS S3当中，之后通知查询节点去加载完整索引来替换掉临时索引 当查询节点之间的segment分布发生变化时，查询节点会访问 binlog 来获取数据，这种情况可能发生在扩展、负载均衡、查询节点故障和恢复期间 当查询节点发生故障时，或者节点被移除时（scaling down)其负责的任务对应的segment和索引就会被加载到另外一个正常的节点当中 当新节点被添加到其中时，同样也会有类似的操作 原子性\nManu 不确保段重新分配是原子的，并且一个段可以驻留在多个查询节点上。这不会影响正确性，因为代理会删除查询的重复结果向量。\n剩余的部分可能大多都是些向量特点并不是那么鲜明的内容和一些用例分析了，这里就不做过多的分析了。\nSummary 在我看来《Manu: A Cloud Native Vector Database Management System》这一篇文章工程实践性比较强的文章了，文中并没有对于向量数据库的核心，即向量检索和索引方面提出什么创新性的设计，而是更注重架构方面，从架构方面出发，讲述了向量数据库和云原生架构之间的一个化学反应。同时zilliz也算是比较出名的向量数据库的初创了，还是比较看好其发展的。\n","permalink":"http://itfischer.space/en/posts/tech/manu/","summary":"Manu: A Cloud Native Vector Database Management System Introduction Manu或者其前身Milvus，定位目标是云原生的向量数据库，需要提供基础的向量存储和检索服务，同时，相比传统的云原生DB","title":"Manu: A Cloud Native Vector Database Management System"},{"content":"raft-example etcd/raft整体架构 etcd/raft选举流程 etcd/raft日志复制\n","permalink":"http://itfischer.space/en/posts/tech/etcd/etcd-raft/","summary":"raft-example etcd/raft整体架构 etcd/raft选举流程 etcd/raft日志复制","title":"etcd/raft"},{"content":"Raft日志 日志存储 Raft日志在存储上分为两部分，一部分为新写入或者新生成的日志，暂时存储于内存当中，还未来得及进行稳定存储。而另一部分则是目前已经进行稳定存储的。\n在Log接口中可以看到，分为对应于Storage和unstable\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type raftLog struct { // storage contains all stable entries since the last snapshot. storage Storage // unstable contains all unstable entries and snapshot. // they will be saved into storage. unstable unstable // committed is the highest log position that is known to be in // stable storage on a quorum of nodes. committed uint64 // applied is the highest log position that the application has // been instructed to apply to its state machine. // Invariant: applied \u0026lt;= committed applied uint64 logger Logger // maxNextEntsSize is the maximum number aggregate byte size of the messages // returned from calls to nextEnts. maxNextEntsSize uint64 } 由于etcd/raft的底层只负责raft逻辑处理，而存储与通信交给了上层，因此在这个过程当中产生的日志和快照均通过unstable进行暂时性的存储，之后通过Ready()函数，以批处理的方式，一次性的将一段时间内的msg、log、hardState、Snapshot全部返回给上层，进行节点间的通信与存储。可以在raft-example当中看到相关的逻辑\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 for { select { case \u0026lt;-ticker.C: rc.node.Tick() // store raft entries to wal, then publish over commit channel case rd := \u0026lt;-rc.node.Ready(): rc.wal.Save(rd.HardState, rd.Entries) if !raft.IsEmptySnap(rd.Snapshot) { rc.saveSnap(rd.Snapshot) rc.raftStorage.ApplySnapshot(rd.Snapshot) rc.publishSnapshot(rd.Snapshot) } rc.raftStorage.Append(rd.Entries) rc.transport.Send(rd.Messages) applyDoneC, ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)) if !ok { rc.stop() return } rc.maybeTriggerSnapshot(applyDoneC) rc.node.Advance() case err := \u0026lt;-rc.transport.ErrorC: rc.writeError(err) return case \u0026lt;-rc.stopc: rc.stop() return } } unstable unstable的结构体定义如下：\n1 2 3 4 5 6 7 8 9 type unstable struct { // the incoming unstable snapshot, if any. snapshot *pb.Snapshot // all entries that have not yet been written to storage. entries []pb.Entry offset uint64 logger Logger } 其中，offset用于将unstable当中的下标转换为全局下标\n部分相关函数：\n函数 功能 maybeFirstIndex 求出日志当中第一个Entry的Index maybeLastIndex 求出日志当中最后一个Entry的Index maybeTerm 求出给定Index对应的Term stableTo 已经通过Ready将部分日志持久化，对状态进行更新，通过Advance调用 stableSnap 快照持久化 shrinkEntriesArray 根据底层切片的size与cap的大小情况，适当缩小切片，释放内存 truncateAppend 向原有的日志当中添加新的日志，产生冲突的则将旧的覆盖 slice 检查范围后返回一段的日志 其中，maybeFirstIndex maybeLastIndex maybeTerm都是根据Snapshot进行判断， 如果当前存在不稳定的Snapshot，就可以根据Snapshot当中的元数据推断出相对整个raftlog的数据，如果不存在Snapshot则无法推断从而返回一个false stableTo、stableSnap反映了整个存储模块的交互逻辑，当部分日志通过Ready()交给上层进行持久化存储之后，就可以调用Advance来推进整个流程，之后Advance会调用到stableTo和stableSnap将已经持久化的日志从内存当中释放（这里的持久化是指交给Storage进行存储，Storage的具体存储逻辑之后进行分析) Storage Storage定义为接口的形式，而官方给出实现为MemoryStorage，将全部的数据全部存放于内存当中，但是通过WAL的形式保证能够稳定存储，结构体定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // MemoryStorage implements the Storage interface backed by an // in-memory array. type MemoryStorage struct { // Protects access to all fields. Most methods of MemoryStorage are // run on the raft goroutine, but Append() is run on an application // goroutine. sync.Mutex // hardState即为Raft当中需要持久化保存的字段，Term、Vote、Commit // 对应的有SoftState，即无需持久化保存，可以通过节点之间相互通信恢复 hardState pb.HardState snapshot pb.Snapshot // ents[i] has raft log position i+snapshot.Metadata.Index ents []pb.Entry } 相关的WAL逻辑可以在上层的raft server(raft example)当中实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 case rd := \u0026lt;-rc.node.Ready(): rc.wal.Save(rd.HardState, rd.Entries) if !raft.IsEmptySnap(rd.Snapshot) { rc.saveSnap(rd.Snapshot) rc.raftStorage.ApplySnapshot(rd.Snapshot) rc.publishSnapshot(rd.Snapshot) } rc.raftStorage.Append(rd.Entries) rc.transport.Send(rd.Messages) applyDoneC, ok := rc.publishEntries(rc.entriesToApply(rd.CommittedEntries)) if !ok { rc.stop() return } rc.maybeTriggerSnapshot(applyDoneC) rc.node.Advance() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func (w *WAL) Save(st raftpb.HardState, ents []raftpb.Entry) error { w.mu.Lock() defer w.mu.Unlock() // short cut, do not call sync if raft.IsEmptyHardState(st) \u0026amp;\u0026amp; len(ents) == 0 { return nil } mustSync := raft.MustSync(st, w.state, len(ents)) // TODO(xiangli): no more reference operator for i := range ents { if err := w.saveEntry(\u0026amp;ents[i]); err != nil { return err } } if err := w.saveState(\u0026amp;st); err != nil { return err } curOff, err := w.tail().Seek(0, io.SeekCurrent) if err != nil { return err } if curOff \u0026lt; SegmentSizeBytes { if mustSync { // gofail: var walBeforeSync struct{} err = w.sync() // gofail: var walAfterSync struct{} return err } return nil } return w.cut() } MemoryStorage中的[]Entry采用了dummy head的实现方式，当无快照时，第一个存储一个Index、Term均为0的Entry，而当有快照时，第一个日志存储lastInclduedIndex和lastIncludedTerm。\n除此之外，剩余的逻辑都比较简单，大多都是做一些对齐、截断和越界的相关处理\nraftLog raftLog的结构体定义如下，其中封装了Storage和unstable：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 type raftLog struct { // storage contains all stable entries since the last snapshot. storage Storage // unstable contains all unstable entries and snapshot. // they will be saved into storage. unstable unstable // committed is the highest log position that is known to be in // stable storage on a quorum of nodes. committed uint64 // applied is the highest log position that the application has // been instructed to apply to its state machine. // Invariant: applied \u0026lt;= committed applied uint64 logger Logger // maxNextEntsSize is the maximum number aggregate byte size of the messages // returned from calls to nextEnts. maxNextEntsSize uint64 } 相关函数\n函数 功能 maybeAppend 匹配Term并解决冲突之后将新的日志追加到unstable当中 findConflict 根据给定的[]Entry找到冲突的位置 findConflictByTerm 根据给定的Term找到冲突的位置，用于进行快速回退 nextEnts [committed + 1,applied]即形成了共识但是还未交给上层去执行的,将这些进行返回 hasPendingSnapshot 即unstable、新产生的Snapshot commitTo commit之后更新状态 appliedTo apply之后更新状态 isUpToDate 检查Term与Index，在Leader选举时进行安全性检查 matchTerm\\term 对于不冲突且饱含、不冲突但不包含、冲突且饱含的三种形式进行区分 term冲突校验\n主要通过一下三个函数进行实现：\nterm：通过给定的Index来获取该Index的Entry的Term，并且会做边界检查，如果超出范围的情况，则返回一个0\nmatchTerm比较term返回的term与给定term是否相同，如果相同则为true，而下层越界访问会返回一个0，从而在这里比较得到一个false\nfindConflict：像注释当中所说的那样：\n如果不存在冲突，且新日志当中所有的内容已经被原本日志包含，无需进行任何操作，返回一个0 如果不存在冲突，但是新日志当中有旧日志当中不存在内容，返回第一条不存在的Entry用于追加 如果存在冲突，则返回冲突的第一个Index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 func (l *raftLog) findConflict(ents []pb.Entry) uint64 { for _, ne := range ents { if !l.matchTerm(ne.Index, ne.Term) { if ne.Index \u0026lt;= l.lastIndex() { l.logger.Infof(\u0026#34;found conflict at index %d [existing term: %d, conflicting term: %d]\u0026#34;, ne.Index, l.zeroTermOnErrCompacted(l.term(ne.Index)), ne.Term) } // 如果不存在冲突，但是存在新的内容，无需进行覆盖，将新的内容追加到原本的日志之后 // 新的内容在term()当中会因为超范围返回0，因此matchTerm会返回false，但是本质上不存在冲突 // 如果存在冲突，则返回冲突的第一条日志，之后进行覆盖。 return ne.Index } } // 新旧日志不存在冲突，并且旧日志当中包含了所有给定的日志，无需进行添加，返回0 return 0 } 快速回退\n快速回退分为两部分，一部分为在follower端进行冲突的查询，找到产生冲突的位置之后返回给Leader，另一部分则是Leader根据follower的hint进行重新发送，这里只看一下日志层面的实现findConflictByTerm，follower和Leader均会调用该函数，而整个流程留到之后再进行分析\n如果发送来的Index，超出了follower的范围，无法根据Term进行搜索，warning follower\\leader包含Index，之后进行term的匹配 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 func (l *raftLog) findConflictByTerm(index uint64, term uint64) uint64 { if li := l.lastIndex(); index \u0026gt; li { // follower的进度过旧，完全不包含Leader发送而来的日志 // NB: such calls should not exist, but since there is a straightfoward // way to recover, do it. // // It is tempting to also check something about the first index, but // there is odd behavior with peers that have no log, in which case // lastIndex will return zero and firstIndex will return one, which // leads to calls with an index of zero into this method. l.logger.Warningf(\u0026#34;index(%d) is out of range [0, lastIndex(%d)] in findConflictByTerm\u0026#34;, index, li) return index } for { logTerm, err := l.term(index) if logTerm \u0026lt;= term || err != nil { break } index-- } return index } 日志追加\n通过maybeAppend和append两个函数进行实现，append不做Term冲突校验，只是简单的检查commit情况，并将日志追加到unstable当中，作为底层被maybeAppend调用。\nmaybeAppend则会根据传入的index与Term进行匹配，并通过findConflict找到冲突的位置：\n如果没有冲突也不存在新日志，则不进行任何的处理 如果在已经commit的日志范围内产生冲突，出现异常，Panic 而如果是一般情况的冲突，即在未commit的地方产生冲突，或者只是传入的Entry当中有需要追加的部分，偏移之后调用append进行追加 maybeAppend由follower的handAppendEntries进行调用，其中会携带目前Leader的commit进度，follower对其进行更新。follower最新的commit为min(committed,lastnewi)\n为什么取min？\n当前的follower为一个较为落后的follower，Leader已经和其他的follower形成了共识，得到了一个committed的进度，但是这个进度还未发送给该follower，此时leader给follower复制日志时，如果复制的日志条目超过了单个消息的上限，则可能出现leader传给follower的committed值大于该follower复制完这条消息中的日志后的最大index，此时，该follower的新committed值为lastnewi。 follower能够跟上leader，leader传给follower的日志中有未确认被法定数量节点稳定存储的日志，此时传入的committed比lastnewi小，该follower的新committed值为传入的committed值。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func (l *raftLog) maybeAppend(index, logTerm, committed uint64, ents ...pb.Entry) (lastnewi uint64, ok bool) { if l.matchTerm(index, logTerm) { lastnewi = index + uint64(len(ents)) ci := l.findConflict(ents) switch { case ci == 0: // 没有冲突也没有新日志，不处理 case ci \u0026lt;= l.committed: l.logger.Panicf(\u0026#34;entry %d conflict with committed entry [committed(%d)]\u0026#34;, ci, l.committed) default: offset := index + 1 // 从产生冲突的新日志处开始进行覆盖 l.append(ents[ci-offset:]...) } //为什么取min？ // 1. leader给follower复制日志时，如果复制的日志条目超过了单个消息的上限， //则可能出现leader传给follower的committed值大于该follower复制完这条消息中的日志后的最大index // 此时，该follower的新committed值为lastnewi。 // 2. follower能够跟上leader，leader传给follower的日志中有未确认被法定数量节点稳定存储的日志， //此时传入的committed比lastnewi小，该follower的新committed值为传入的committed值。 l.commitTo(min(committed, lastnewi)) return lastnewi, true } return 0, false } 进度跟踪 Raft原文当中使用nextIndex[] matchIndex[]进行追踪follower的日志进度。\n1 2 3 4 5 6 7 8 9 10 11 12 if args.Term == rf.currentTerm { if reply.Success { // 发送成功则为发送的所有日志条目全部匹配，match直接加上len(Entries) // next 永远等于match + 1 match := args.PrevLogIndex + len(args.LogEntries) next := match + 1 rf.commitIndex = rf.lastIncludedIndex rf.matchIndex[server] = match rf.nextIndex[server] = next Debug(dLog2, \u0026#34;S%d update server is %d nextIndex:%d lastIncludedIndex is:%d preLogIndex is %d lenLog is %d\u0026#34;, rf.me, server, next, rf.lastIncludedIndex, args.PrevLogIndex, len(args.LogEntries)) } 在etcd/raft当中，为了实现节藕，单独封装了一个Progress结构体用于记录在Leader的视角下，follower处于何种进度。最终在Leader处维护一个 []progress数组来表示当前所有的follower的进度，progress结构体的定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 type Progress struct { Match, Next uint64 // State defines how the leader should interact with the follower. // // When in StateProbe, leader sends at most one replication message // per heartbeat interval. It also probes actual progress of the follower. // // When in StateReplicate, leader optimistically increases next // to the latest entry sent after sending replication message. This is // an optimized state for fast replicating log entries to the follower. // // When in StateSnapshot, leader should have sent out snapshot // before and stops sending any replication message. State StateType // PendingSnapshot is used in StateSnapshot. // If there is a pending snapshot, the pendingSnapshot will be set to the // index of the snapshot. If pendingSnapshot is set, the replication process of // this Progress will be paused. raft will not resend snapshot until the pending one // is reported to be failed. PendingSnapshot uint64 // RecentActive is true if the progress is recently active. Receiving any messages // from the corresponding follower indicates the progress is active. // RecentActive can be reset to false after an election timeout. // // TODO(tbg): the leader should always have this set to true. RecentActive bool // ProbeSent is used while this follower is in StateProbe. When ProbeSent is // true, raft should pause sending replication message to this peer until // ProbeSent is reset. See ProbeAcked() and IsPaused(). ProbeSent bool // Inflights is a sliding window for the inflight messages. // Each inflight message contains one or more log entries. // The max number of entries per message is defined in raft config as MaxSizePerMsg. // Thus inflight effectively limits both the number of inflight messages // and the bandwidth each Progress can use. // When inflights is Full, no more message should be sent. // When a leader sends out a message, the index of the last // entry should be added to inflights. The index MUST be added // into inflights in order. // When a leader receives a reply, the previous inflights should // be freed by calling inflights.FreeLE with the index of the last // received entry. Inflights *Inflights // IsLearner is true if this progress is tracked for a learner. IsLearner bool } 字段 作用 Match、Next 对应原本Raft论文当中的MatchIndex[]与NextIndex[]，表示follower当前的进度 State 当前follower的状态，决定Leader以何种方式与follower进行通讯 PendingSnapshot 对应在raftlog当中的PendingSnapshot，在StateSnapshot阶段下使用 RecentActive Check Quorum时使用的字段，表示当前的follower是否处于活跃状态 ProbeSent StateProbe状态下用于阻塞发送 learner 表示当前节点是否处于learner状态 主要来看一下State与PendingSnapshot，对于State，在etcd的注视当中描述的也非常明确：\nProbe所表示的为Leader，对于当前follower的最后一条日志的Index并不知晓，可能是follower宕机许久之后重启。此时在Leader方面，将其视为Probe状态，在这种状态下，leader每次心跳期间仅为follower发送一条MsgApp​消息，之后边立即陷入阻塞状态，等待follower的回应之后才会重新从阻塞当中恢复 StateReplicate表示当前的follower能够正确的跟随住Leader的进度，此时可以按照流水线的形式不断的给Follower发送消息，可以采取比较激进的优化方式直接对Leader的信息进行更新 StateSnapshot表示当前的follower已经落后Leader过多，所需的日志已经被快照化，此时所需要的就是发送快照。 状态转换 初始状态\n接下来讨论一下Progress当中的State如何进行状态转换\n当一个节点发生状态转换时，（这里的状态转换指的是Leader、Follower、Candidate）会调用Reset函数，在其中遍历所有的节点，对其进行初始化，而由于StateType定义为uint64​类型的字段，因此初始化为0，即StateProbe，这样的做法也说得通，当一个Leader刚通过选举上线时，他对其他节点的状态是不清楚的，先将其他的所有的follower的状态设置为StateProbe​因此在发送时先探测follower的LastIndex。而之后会在``becomeLeader 当中将自身的状态设置为StateReplicate​\n1 2 3 4 5 6 7 8 9 10 11 r.prs.Visit(func(id uint64, pr *tracker.Progress) { *pr = tracker.Progress{ Match: 0, Next: r.raftLog.lastIndex() + 1, Inflights: tracker.NewInflights(r.prs.MaxInflight), IsLearner: pr.IsLearner, } if id == r.id { pr.Match = r.raftLog.lastIndex() } }) 1 2 3 4 5 func(r *raft) becomeLeader() { // ... r.prs.Progress[r.id].BecomeReplicate() // ... } StateProbe -\u0026gt;StateReplicate\n而当follower能够正确的跟随Leader的进度时，就可以从SateProbe转向StateReplicate进行快速的跟随复制，因此在Leader接收到MsgAppResp时，并且当中的reject为false时，并且该条消息为一个非过失的消息时，调用becomeReplicate​进行状态转换：\n1 2 3 4 5 6 7 8 9 10 11 12 if (m.reject) { // ... } else { if pr.MaybeUpdate(m.Index) { switch { case pr.State == tracker.StateProbe: // 在Probe状态下，follower成功接收到Leader发送的RPC并且成功给予回应 // 此时Leader可以确认follower可以跟上进度，因此取消阻塞，并且转变为流水线 // 模式优化发送速度 pr.BecomeReplicate() } } 其中MaybeUpdate用于判断当前的消息是否为一条过时的消息，并更新状态。即已经返回的Index已经是Match之后了的，对于过期的消息，对其进行忽略\n1 2 3 4 5 6 7 8 9 10 func (pr *Progress) MaybeUpdate(n uint64) bool { var updated bool if pr.Match \u0026lt; n { pr.Match = n updated = true pr.ProbeAcked() } pr.Next = max(pr.Next, n+1) return updated } StateReplicate -\u0026gt; StateProbe\n即原本Follower能够紧随Leader的进度，之后如果存在网络故障或者消息在网络中乱序发送，Follower无法与Leader的进度进行对齐，会返回Leader一个reject和rejectHint，用于进行fast backup，此时就代表Follower已经无法紧随Leader的日志进度了，需要从Replicate向Probe进行转换：\n1 2 3 4 5 6 7 8 9 10 11 if (m.reject) { // 如果回退失败则说明为一条过期的日志，不做处理 // 如果回退成功，并且对方原本为StateReplicate，转换为Probe进行探测，并且再向其发送一次MsgApp if pr.MaybeDecrTo(m.Index, nextProbeIdx) { r.logger.Debugf(\u0026#34;%x decreased progress of %x to [%s]\u0026#34;, r.id, m.From, pr) if pr.State == tracker.StateReplicate { pr.BecomeProbe() } r.sendAppend(m.From) } } 除此之外， 还在处理MsgUnreachable当中从StateReplicate转换为Probe，暂时没太弄明白调用的时机，不过个人猜测是当follower长时间未响应Leader的调用，就会添加一条MsgUnreachable的消息，来表示Follower长期收不到Leader的消息，转换到Probe状态\nStateProbe -\u0026gt; StateSnapshot \u0026amp; StateReplicate -\u0026gt; StateSnapshot\n进入到StateSnapshot​当中的情况非常简单，不区分状态，只要在Leader要发送AppendEntries时发现自己应当发送给对方的日志已经被快照化，此时就转变为StateSnapshot​ 开始发送快照，这个情况在StateProbe​与StateReplicate​的状态下均有可能发生，尤其是StateProbe​ 状态下，更容易出现Leader与Follower之间的差距过大，从而要发送给Follower的日志被快照化的情况。\nStateSnapshot -\u0026gt; StateReplicate\n在StateSnapshot​的状态下，如果如果需要发送的Snapshot在Follower的进度已经匹配上，此时就可以转换为StateReplicate​进行快速发送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 if pr.MaybeUpdate(m.Index) { switch { case pr.State == tracker.StateProbe: // 在Probe状态下，follower成功接收到Leader发送的RPC并且成功给予回应 // 此时Leader可以确认follower可以跟上进度，因此取消阻塞，并且转变为流水线 // 模式优化发送速度 pr.BecomeReplicate() case pr.State == tracker.StateSnapshot \u0026amp;\u0026amp; pr.Match \u0026gt;= pr.PendingSnapshot: // TODO(tbg): we should also enter this branch if a snapshot is // received that is below pr.PendingSnapshot but which makes it // possible to use the log again. r.logger.Debugf(\u0026#34;%x recovered from needing snapshot, resumed sending replication messages to %x [%s]\u0026#34;, r.id, m.From, pr) // Transition back to replicating state via probing state // (which takes the snapshot into account). If we didn\u0026#39;t // move to replicating state, that would only happen with // the next round of appends (but there may not be a next // round for a while, exposing an inconsistent RaftStatus). pr.BecomeProbe() pr.BecomeReplicate() StateSnapshot -\u0026gt; StateProbe\n在Leader处理SnapshotStatus消息时，如果当前的状态为StateSnapshot，则会转换为StateProbe，\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 case pb.MsgSnapStatus: if pr.State != tracker.StateSnapshot { return nil } // TODO(tbg): this code is very similar to the snapshot handling in // MsgAppResp above. In fact, the code there is more correct than the // code here and should likely be updated to match (or even better, the // logic pulled into a newly created Progress state machine handler). if !m.Reject { pr.BecomeProbe() r.logger.Debugf(\u0026#34;%x snapshot succeeded, resumed sending replication messages to %x [%s]\u0026#34;, r.id, m.From, pr) } else { // NB: the order here matters or we\u0026#39;ll be probing erroneously from // the snapshot index, but the snapshot never applied. pr.PendingSnapshot = 0 pr.BecomeProbe() r.logger.Debugf(\u0026#34;%x snapshot failed, resumed sending replication messages to %x [%s]\u0026#34;, r.id, m.From, pr) } 而该消息是由Node当中的ReportSnapshot​方法进行封装的,不过该方法在raft-example当中并没有调用，真正的调用位于etcd/server当中，用法为：leader节点的使用者还需要主动调用Node​的ReportSnapshot​方法告知leader节点快照的应用状态，leader会将该follower的状态转移到StateProbe​状态，暂时不进行展开\n以上就是涉及到的所有的状态切换问题，可以总结如下：\n最后再列一下各个状态转移的函数：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // BecomeProbe transitions into StateProbe. Next is reset to Match+1 or, // optionally and if larger, the index of the pending snapshot. func (pr *Progress) BecomeProbe() { // If the original state is StateSnapshot, progress knows that // the pending snapshot has been sent to this peer successfully, then // probes from pendingSnapshot + 1. if pr.State == StateSnapshot { pendingSnapshot := pr.PendingSnapshot pr.ResetState(StateProbe) pr.Next = max(pr.Match+1, pendingSnapshot+1) } else { pr.ResetState(StateProbe) pr.Next = pr.Match + 1 } } // BecomeReplicate transitions into StateReplicate, resetting Next to Match+1. func (pr *Progress) BecomeReplicate() { pr.ResetState(StateReplicate) pr.Next = pr.Match + 1 } // BecomeSnapshot moves the Progress to StateSnapshot with the specified pending // snapshot index. func (pr *Progress) BecomeSnapshot(snapshoti uint64) { pr.ResetState(StateSnapshot) pr.PendingSnapshot = snapshoti } Progress函数 现在回到主线继续分析Progress的相关函数：\nIsPaused:\n对于三种状态，均存在不同的阻塞条件：\nStateProbe下，发送完一条日志之后手动设置Probe = true​,之后如果检测到true之后就会在IsPaused当中阻塞 StateReplicate下，如果当前的滑动窗口，或者说消息队列已满的话，则阻塞 StateSnapshot下，对于相同的快照，只应向对方发送一次，而对方如果收到之后就会脱离StateSnapshot(调用Node.ReportSnapshot)，因此在StateSanpshot下应当无条件阻塞 1 2 3 4 5 6 7 8 9 10 11 12 func (pr *Progress) IsPaused() bool { switch pr.State { case StateProbe: return pr.ProbeSent case StateReplicate: return pr.Inflights.Full() case StateSnapshot: return true default: panic(\u0026#34;unexpected state\u0026#34;) } } ProbeAcked:\nStateProbe状态下确认回应，脱离阻塞\n1 2 3 4 5 6 // ProbeAcked is called when this peer has accepted an append. It resets // ProbeSent to signal that additional append messages should be sent without // further delay. func (pr *Progress) ProbeAcked() { pr.ProbeSent = false } MaybeUpdate:\n在收到AppMsgResp之后调用，对日志Match的进度进行更新，并且调用ProbeAcked表示接收到响应解除阻塞。此外，在AppendEntries当中也会进行调用，对Leader自身对match进行更新\n1 2 3 4 5 6 7 8 9 10 11 12 13 // MaybeUpdate is called when an MsgAppResp arrives from the follower, with the // index acked by it. The method returns false if the given n index comes from // an outdated message. Otherwise it updates the progress and returns true. func (pr *Progress) MaybeUpdate(n uint64) bool { var updated bool if pr.Match \u0026lt; n { pr.Match = n updated = true pr.ProbeAcked() } pr.Next = max(pr.Next, n+1) return updated } MaybeDecrTo:\n在收到Follower发来的MsgAppResp之后，如果当中的reject为true，那么就根据返回的rejectHint进行快速回退\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 // MaybeDecrTo adjusts the Progress to the receipt of a MsgApp rejection. The // arguments are the index of the append message rejected by the follower, and // the hint that we want to decrease to. // // Rejections can happen spuriously as messages are sent out of order or // duplicated. In such cases, the rejection pertains to an index that the // Progress already knows were previously acknowledged, and false is returned // without changing the Progress. // // If the rejection is genuine, Next is lowered sensibly, and the Progress is // cleared for sending log entries. func (pr *Progress) MaybeDecrTo(rejected, matchHint uint64) bool { if pr.State == StateReplicate { // The rejection must be stale if the progress has matched and \u0026#34;rejected\u0026#34; // is smaller than \u0026#34;match\u0026#34;. if rejected \u0026lt;= pr.Match { return false } // Directly decrease next to match + 1. // // TODO(tbg): why not use matchHint if it\u0026#39;s larger? pr.Next = pr.Match + 1 return true } // 探测模式一个心跳周期只能发送一次Entry，如果pr.Next - 1 != rejected则证明发送的时候 // 出现错误，则证明其不是这个心跳周期发送出去的信息 // The rejection must be stale if \u0026#34;rejected\u0026#34; does not match next - 1. This // is because non-replicating followers are probed one entry at a time. if pr.Next-1 != rejected { return false } pr.Next = max(min(rejected, matchHint+1), 1) pr.ProbeSent = false return true } Tracker 在Progress结构体之上，还封装了一个PorgressTracker​，定义在tracker.go​当中，封装了一些对于ProgressMap的集体操作，记录当前所有节点的状态与进度，和当前集群正在使用的配置信息。并且关于投票的相关信息也在这其中进行更新与记录，其中的操作并不复杂，暂时不做展开\n1 2 3 4 5 6 7 8 9 10 11 12 // ProgressTracker tracks the currently active configuration and the information // known about the nodes and learners in it. In particular, it tracks the match // index for each peer which in turn allows reasoning about the committed index. type ProgressTracker struct { Config Progress ProgressMap Votes map[uint64]bool MaxInflight int } ectd/raft日志复制 节点身份确认 在一个集群启动之后，首先会通过becomeFollower​全部初始化为follower，之后超时通过becomeCandidate​开始选举，最后becomeLeader​当选。在这个三个函数当中均使用reset​对自身状态进行重置，之后Leader再将自身的设置为StateReplicate，但是这个设置并不是很重要，Leader主要根据Follower的State去进行相关的逻辑操作，这里更像是让其在逻辑上通顺，而没有什么实质性作用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (r *raft) reset(term uint64) { // ... r.prs.ResetVotes() r.prs.Visit(func(id uint64, pr *tracker.Progress) { *pr = tracker.Progress{ Match: 0, Next: r.raftLog.lastIndex() + 1, Inflights: tracker.NewInflights(r.prs.MaxInflight), IsLearner: pr.IsLearner, } if id == r.id { pr.Match = r.raftLog.lastIndex() } }) } func (r *raft) becomeLeader() { // ... r.prs.Progress[r.id].BecomeReplicate() // ... } 之后所有的逻辑依旧像选举那样都在Step当中处理，首先Step进行通用处理之后再调用对应Stepxxx​，在这个过程当中触发了日志复制的各种操作,不过在这个过程当中由于对于日志的操作在各个角色稻种不尽相同，因此Step当中没有涉及到日志相关的操作，直接看stepxxx，这里如果分step进行讨论难免会有些割裂，因此不如从某个Step入手，去分析发送和接收的整个过程。\n心跳相关消息处理 通过Leader的tickHeartbeat触发调用，处理的逻辑也比较的简单，只需要生成一条MsgHeartbeat广播给所有的follower，当中携带了一条commited字段，并且取了min(pr.match,leader.committed)​,在Raft论文或者对应的实现MIT6.824当中，Heartbeat只要求了令follower去重置超时时间，跟随Leader。\n而在ectd/raft当中，可以顺带更新follower的committed进度，取min也是为了防止follower的进度落后于leader，还未获取到leader最新的日志，如果发送的为match，则代表将follower当前所有的日志全部设置为committed。并且follower并不需要维护progress，因此只需要更新日志当中的committed进度即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func (r *raft) bcastHeartbeat() { lastCtx := r.readOnly.lastPendingRequestCtx() if len(lastCtx) == 0 { r.bcastHeartbeatWithCtx(nil) } else { r.bcastHeartbeatWithCtx([]byte(lastCtx)) } } // sendHeartbeat sends a heartbeat RPC to the given peer. func (r *raft) sendHeartbeat(to uint64, ctx []byte) { // Attach the commit as min(to.matched, r.committed). // When the leader sends out heartbeat message, // the receiver(follower) might not be matched with the leader // or it might not have all the committed entries. // The leader MUST NOT forward the follower\u0026#39;s commit to // an unmatched index. commit := min(r.prs.Progress[to].Match, r.raftLog.committed) m := pb.Message{ To: to, Type: pb.MsgHeartbeat, Commit: commit, Context: ctx, } r.send(m) } // 在candidate和follower状态均会调用，转变为follower func (r *raft) handleHeartbeat(m pb.Message) { r.raftLog.commitTo(m.Commit) r.send(pb.Message{To: m.From, Type: pb.MsgHeartbeatResp, Context: m.Context}) } MsgHeartbeatResp​\n在Leader接收到了Follower的MsgHeartbeatResp之后，主要做一下的相关处理：\n如果当前为Probe模式并且处于阻塞模式，ProbeSent设置为false解除阻塞 RecentActive设置为True，用于进行Check Quorum 如果消息队列已满，可以从头释放 根据Tracker当中的Match进度和当前的日志情况，考虑是否需要向follower发送新的日志 做一些线性一致性的相关处理 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 case pb.MsgHeartbeatResp: pr.RecentActive = true pr.ProbeSent = false // free one slot for the full inflights window to allow progress. if pr.State == tracker.StateReplicate \u0026amp;\u0026amp; pr.Inflights.Full() { pr.Inflights.FreeFirstOne() } if pr.Match \u0026lt; r.raftLog.lastIndex() { r.sendAppend(m.From) } if r.readOnly.option != ReadOnlySafe || len(m.Context) == 0 { return nil } if r.prs.Voters.VoteResult(r.readOnly.recvAck(m.From, m.Context)) != quorum.VoteWon { return nil } rss := r.readOnly.advance(m) for _, rs := range rss { if resp := r.responseToReadIndexReq(rs.req, rs.index); resp.To != None { r.send(resp) } } 日志提议 Leader发送 上层通过调用Node接口的Propose函数向Raft发起一条提议，将一个操作封装成一条日志，之后用于达成共识，该方法会封装成一条MsgProp的消息，作为添加新日志的起始点。\n这里上层所调用的为stepWait函数，并且传下来的为一个Entry数组，为批处理的形式，上层写的channel并发编程暂时没太看懂，先搁置一下。\n这里所进行的大部分操作都是遍历当前新的提议日志组当中是否有配置更改的消息，除此之外，涉及到的逻辑并不多：\n首先检测是否为空日志 检测当前的Leader是否在当前配置所规定的集群当中 之后首先把日志追加到自身的unstable当中，之后在广播发送给其他的follower 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 case pb.MsgProp: if len(m.Entries) == 0 { r.logger.Panicf(\u0026#34;%x stepped empty MsgProp\u0026#34;, r.id) } if r.prs.Progress[r.id] == nil { // If we are not currently a member of the range (i.e. this node // was removed from the configuration while serving as leader), // drop any new proposals. return ErrProposalDropped } if r.leadTransferee != None { r.logger.Debugf(\u0026#34;%x [term %d] transfer leadership to %x is in progress; dropping proposal\u0026#34;, r.id, r.Term, r.leadTransferee) return ErrProposalDropped } // 对于上方propose的先处理配置问题，之后将Entries添加并向follower广播发送 // 对于其中包含confChange到信息，进行特殊处理 for i := range m.Entries { e := \u0026amp;m.Entries[i] var cc pb.ConfChangeI if e.Type == pb.EntryConfChange { var ccc pb.ConfChange if err := ccc.Unmarshal(e.Data); err != nil { panic(err) } cc = ccc } else if e.Type == pb.EntryConfChangeV2 { var ccc pb.ConfChangeV2 if err := ccc.Unmarshal(e.Data); err != nil { panic(err) } cc = ccc } if cc != nil { alreadyPending := r.pendingConfIndex \u0026gt; r.raftLog.applied alreadyJoint := len(r.prs.Config.Voters[1]) \u0026gt; 0 wantsLeaveJoint := len(cc.AsV2().Changes) == 0 var refused string if alreadyPending { refused = fmt.Sprintf(\u0026#34;possible unapplied conf change at index %d (applied to %d)\u0026#34;, r.pendingConfIndex, r.raftLog.applied) } else if alreadyJoint \u0026amp;\u0026amp; !wantsLeaveJoint { refused = \u0026#34;must transition out of joint config first\u0026#34; } else if !alreadyJoint \u0026amp;\u0026amp; wantsLeaveJoint { refused = \u0026#34;not in joint state; refusing empty conf change\u0026#34; } if refused != \u0026#34;\u0026#34; { r.logger.Infof(\u0026#34;%x ignoring conf change %v at config %s: %s\u0026#34;, r.id, cc, r.prs.Config, refused) m.Entries[i] = pb.Entry{Type: pb.EntryNormal} } else { r.pendingConfIndex = r.raftLog.lastIndex() + uint64(i) + 1 } } } if !r.appendEntry(m.Entries...) { return ErrProposalDropped } r.bcastAppend() return nil appendEntry\n在appendEntry当中，首先对上层传来的日志进行初始化，判断当前Leader与Follower的进度差距，即Leader目前还有多少未commit的日志，如果当前未commit的日志过多，则证明Leader与Follower的差距过大，此时如果继续进行提议，那么就会进一步拉大Leader与Follower成差距，因此拒绝继续提议。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 func (r *raft) increaseUncommittedSize(ents []pb.Entry) bool { var s uint64 for _, e := range ents { s += uint64(PayloadSize(e)) } // 如果当前未提交的日志过多，则证明Follower难以跟上Leader的发送速度 // 因此返回false，之后会拒绝新的提议，防止Leader与Follower之间的差距更大 if r.uncommittedSize \u0026gt; 0 \u0026amp;\u0026amp; s \u0026gt; 0 \u0026amp;\u0026amp; r.uncommittedSize+s \u0026gt; r.maxUncommittedSize { // If the uncommitted tail of the Raft log is empty, allow any size // proposal. Otherwise, limit the size of the uncommitted tail of the // log and drop any proposal that would push the size over the limit. // Note the added requirement s\u0026gt;0 which is used to make sure that // appending single empty entries to the log always succeeds, used both // for replicating a new leader\u0026#39;s initial empty entry, and for // auto-leaving joint configurations. return false } r.uncommittedSize += s return true } 之后通过将日志追加到Leader自身的日志当中，并通过MaybeUpdate对自身的Match进行更新，对于自身，写入了自然能够Match，之后再通过maybeCommit去统计当前能够通过投票的日志的进度如何，对自身的commited进度进行更新\n1 2 3 4 5 6 7 // maybeCommit attempts to advance the commit index. Returns true if // the commit index changed (in which case the caller should call // r.bcastAppend). func (r *raft) maybeCommit() bool { mci := r.prs.Committed() return r.raftLog.maybeCommit(mci, r.Term) } 1 2 3 4 5 // Committed returns the largest log index known to be committed based on what // the voting members of the group have acknowledged. func (p *ProgressTracker) Committed() uint64 { return uint64(p.Voters.CommittedIndex(matchAckIndexer(p.Progress))) } appendEntry的完整代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 func (r *raft) appendEntry(es ...pb.Entry) (accepted bool) { li := r.raftLog.lastIndex() // 上层传入的日志当中不清楚下层的Term和Index情况，因此在此处进行封装 for i := range es { es[i].Term = r.Term es[i].Index = li + 1 + uint64(i) } // Track the size of this uncommitted proposal. if !r.increaseUncommittedSize(es) { r.logger.Debugf( \u0026#34;%x appending new entries to log would exceed uncommitted entry size limit; dropping proposal\u0026#34;, r.id, ) // Drop the proposal. return false } // use latest \u0026#34;last\u0026#34; index after truncate/append li = r.raftLog.append(es...) r.prs.Progress[r.id].MaybeUpdate(li) // Regardless of maybeCommit\u0026#39;s return, our caller will call bcastAppend. r.maybeCommit() return true } bcastAppend\n通过progress遍历所有的节点，尝试发送AppendEntries，这两段的逻辑都比较简单，重点在于封装的maybeSendAppend​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // bcastAppend sends RPC, with entries to all peers that are not up-to-date // according to the progress recorded in r.prs. // 封装一个发送AppendEntries的函数作为参数传递给Visit，之后Visit遍历并调用该函数 func (r *raft) bcastAppend() { r.prs.Visit(func(id uint64, _ *tracker.Progress) { if id == r.id { return } r.sendAppend(id) }) } // sendAppend sends an append RPC with new entries (if any) and the // current commit index to the given peer. func (r *raft) sendAppend(to uint64) { r.maybeSendAppend(to, true) } maybeSendAppend​\n对于日志与快照的发送均通过该函数进行，大致步骤如下：\n首先通过progress检查对方是否处于阻塞状态，如果阻塞，则拒绝发送\n尝试获取到需要发送给对方的日志和对应的Term，并在这过过程判断需要发送的日志是否过旧已经被快照化\n在上一步当中如果判断出了已经被快照化，那么就转变为发送快照：\n如果发送对方并没有通过RecentActive的检查的话，再加上对方目前已经长时间落后于Leader，可以认定为其已经宕机或者产生网络分区，无法和leader进行通信，因此拒绝发送\n封装MsgSnap，准备发送日志，并将对方的状态转换为StateSnaphot\n如之前所讨论的：在follower转为StateSnapshot后，只有两种跳出StateSnapshot的方法：\nfollower节点应用快照后会发送MsgAppResp消息，该消息会报告当前follower的last index。如果follower应用了快照后last index就追赶上了其match index，那么leader会直接将follower的状态转移到StateRelicate状态，为其继续复制日志。 leader节点的使用者还需要主动调用Node的ReportSnapshot方法告知leader节点快照的应用状态，\nleader会将该follower的状态转移到StateProbe状态（与方法1重复的消息会被忽略）。 否则，发送日志，在完成MsgApp的封装之后，根据当前的状态去更新状态\n如果当前为StateProbe状态，那么犹豫StateProbe在一次心跳信息当中只能发送一次日志，因此将StateSent设置为true，在该心跳周期当中进行阻塞，等待收到MsgAppResp之后设置为false，从阻塞当中恢复\n如果当前为StateReplicate状态，那么就证明follower可以跟上Leader的进度，因此可以以流水线的形式快速发送，无需等待回应即可更新对方的Next进度\n1 2 3 // OptimisticUpdate signals that appends all the way up to and including index n // are in-flight. As a result, Next is increased to n+1. func (pr *Progress) OptimisticUpdate(n uint64) { pr.Next = n + 1 } 最后的最后，调用send封装一条Msg交给上层进行发送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 func (r *raft) maybeSendAppend(to uint64, sendIfEmpty bool) bool { pr := r.prs.Progress[to] // 对方由于某些原因处于阻塞状态，拒绝发送 if pr.IsPaused() { return false } m := pb.Message{} m.To = to term, errt := r.raftLog.term(pr.Next - 1) ents, erre := r.raftLog.entries(pr.Next, r.maxMsgSize) if len(ents) == 0 \u0026amp;\u0026amp; !sendIfEmpty { return false } // 可能由于对方进度过慢导致日志内容已经被快照化，此时尝试发送快照 if errt != nil || erre != nil { // send snapshot if we failed to get term or entries if !pr.RecentActive { r.logger.Debugf(\u0026#34;ignore sending snapshot to %x since it is not recently active\u0026#34;, to) return false } m.Type = pb.MsgSnap snapshot, err := r.raftLog.snapshot() if err != nil { if err == ErrSnapshotTemporarilyUnavailable { r.logger.Debugf(\u0026#34;%x failed to send snapshot to %x because snapshot is temporarily unavailable\u0026#34;, r.id, to) return false } panic(err) // TODO(bdarnell) } if IsEmptySnap(snapshot) { panic(\u0026#34;need non-empty snapshot\u0026#34;) } m.Snapshot = snapshot sindex, sterm := snapshot.Metadata.Index, snapshot.Metadata.Term r.logger.Debugf(\u0026#34;%x [firstindex: %d, commit: %d] sent snapshot[index: %d, term: %d] to %x [%s]\u0026#34;, r.id, r.raftLog.firstIndex(), r.raftLog.committed, sindex, sterm, to, pr) pr.BecomeSnapshot(sindex) r.logger.Debugf(\u0026#34;%x paused sending replication messages to %x [%s]\u0026#34;, r.id, to, pr) } else { m.Type = pb.MsgApp m.Index = pr.Next - 1 m.LogTerm = term m.Entries = ents m.Commit = r.raftLog.committed if n := len(m.Entries); n != 0 { switch pr.State { // optimistically increase the next when in StateReplicate case tracker.StateReplicate: // 当前正以流水线的方法优化日志的复制速度，即follower可以确保跟上leader的进度 // 因此可以直接设置nest last := m.Entries[n-1].Index pr.OptimisticUpdate(last) pr.Inflights.Add(last) case tracker.StateProbe: // 在Probe模式下，一次只能发送一条日志，并且需要等待对方的回应之后才能继续发送 // 在此处设置为true用于在IsPaused()当中进行阻塞，直到收到follower的确认 pr.ProbeSent = true default: r.logger.Panicf(\u0026#34;%x is sending append in unhandled state %s\u0026#34;, r.id, pr.State) } } } r.send(m) return true } Follower处理与响应 在Candidate和Follower状态下都会接收并处理MsgApp，如果是Candidate，就多了一步转换为follower的步骤，之后调用handleAppendEntries​进行处理：\n如果发送来的日志已经committed，那么则为一条陈旧消息，返回给Leader自己当前的进度即可 之后调用raftLog的maybeAppend，在其中处理冲突并且返回是否添加成功，最终更新commit的进度，关于commit，在上面讲述raftlog时已经说明过为什么要min(lastnewi,committed),如果能够成功添加就可以直接向Leader发送MsgAppResp了，返回当前的最新的日志的Index表明进度 如果添加失败，则代表存在冲突，在follower端按照快速回退的方式根据Term找到冲突的位置，即第一个Term \u0026lt;= Msg.Term并且Index \u0026lt;= Msg.Index的位置之后将Reject字段设置为true，设置HintIndex、HintTerm返回给Leader 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 func (r *raft) handleAppendEntries(m pb.Message) { if m.Index \u0026lt; r.raftLog.committed { r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed}) return } if mlastIndex, ok := r.raftLog.maybeAppend(m.Index, m.LogTerm, m.Commit, m.Entries...); ok { r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: mlastIndex}) } else { r.logger.Debugf(\u0026#34;%x [logterm: %d, index: %d] rejected MsgApp [logterm: %d, index: %d] from %x\u0026#34;, r.id, r.raftLog.zeroTermOnErrCompacted(r.raftLog.term(m.Index)), m.Index, m.LogTerm, m.Index, m.From) // Return a hint to the leader about the maximum index and term that the // two logs could be divergent at. Do this by searching through the // follower\u0026#39;s log for the maximum (index, term) pair with a term \u0026lt;= the // MsgApp\u0026#39;s LogTerm and an index \u0026lt;= the MsgApp\u0026#39;s Index. This can help // skip all indexes in the follower\u0026#39;s uncommitted tail with terms // greater than the MsgApp\u0026#39;s LogTerm. // // See the other caller for findConflictByTerm (in stepLeader) for a much // more detailed explanation of this mechanism. hintIndex := min(m.Index, r.raftLog.lastIndex()) hintIndex = r.raftLog.findConflictByTerm(hintIndex, m.LogTerm) hintTerm, err := r.raftLog.term(hintIndex) if err != nil { panic(fmt.Sprintf(\u0026#34;term(%d) must be valid, but got %v\u0026#34;, hintIndex, err)) } r.send(pb.Message{ To: m.From, Type: pb.MsgAppResp, Index: m.Index, Reject: true, RejectHint: hintIndex, LogTerm: hintTerm, }) } } Leader接收MsgAppResp 到这里也是日志复制的最后一步，Leader根据Follower的响应来决定后续的行为，接收到消息首先可以将该follower标记为RecentActive，之后先来说存在冲突的情况：\n存在冲突：\n存在冲突，即MsgAppResp当中的Reject为true，对应的还会有Follower返回的HintIndex(RejectHint)与HintTerm(Logterm)，在follower的handAppendEntries当中，如果follower的进度过旧，导致index out of range的话，Term就会设置为0，因此首先判断Term是否为0，如果不为0的话就证明在follower当中找到了对应的冲突位置，可以根据返回的HintIndex、HintTerm在Leader当中找到对应的位置，设置为Next，用于下一次发送。\n之后尝试对Leader的Next记录进行回退，如果回退失败则说明为一条过期的日志，不处理，否则将follower的状态记录为StateProbe，进行探测发送，理想的情况下可以一轮探测就让follower和Leader正确跟随对齐进度，再重新转换为StateReplicate。\n不存在冲突​\n先尝试根据返回的Index对match进行更新，如果更新失败，则证明Leader的Match进度快于Follower返回的，证明：\n该消息为一条过期的消息，不进行处理 该MsgAppResp由快照应用快照后产生的消息，此时follower仍未跟上其MatchIndex，无法完成匹配 该消息为StateProbe状态发送的确认消息，由于StateProbe状态只进行探测，一次性只在一次心跳周期当中发送一条日志，无法进行日志复制跟随状态，所以此时仍没有Match上，不做处理，等到之后收到下一个周期的HeartbeatResp再解除阻塞 如果能够更新成功：\n如果当前为StateProbe，那么则证明通过探测对齐了进度，此时就可以转换为StateReplicate去快速发送日志更新状态 如果当前为StateSnapshot，并且Match的进度超过了Snapshot，此时同样可以转换为StateReplicate 而StateReplicate下只需释放部分消息队列的容量即可 处理完状态转换之后，Leader可以尝试通过检测各个follower的复制进度来决定commit，这个动作为异步的，由于引入了ProgressTracker，则可以根据当前记录的信息时不时的触发maybeCommit来推进commit进度，无需向MIT-6.824那样发送完AppendEntries RPC之后在这里等待响应以确定commit，如果commit的进度得到推进，就可以额在广播一次AppendEntries将commit进度携带于其中，发送给follower\n如果commit index没有更新，还需要进一步判断该follower之前是否是阻塞的，如果是那么为该follower发送一条日志复制消息以更新其committed​索引，因为在该节点阻塞时可能错过了committed​索引的更新消息。\n之后尝试通过for循环去不断的发送AppendEntries。因为日志复制部分有流控逻辑，因此这里的循环不会成为死循环。这样做可以尽可能多地为节点复制日志，以提高日志复制效率。\n1 2 3 4 5 6 7 func (r *raft) maybeSendAppend(to uint64, sendIfEmpty bool) bool { // ... if len(ents) == 0 \u0026amp;\u0026amp; !sendIfEmpty { return false } // ... } 至此，所有关于MsgAppResp的处理全部完成，这一部分尤其是reject进行快速回退的过程当中etcd给出了大量的样例用于说明不同情况下的对齐进度和对齐方式，个人认为非常有必要一读，以更好的理解整个回退的过程。\n除此之外，在发送过程当中，快照发送的过程与响应的过程夹杂在日志的处理过程打红中顺便给说明了，但是对于follower应当如何应用快照还未涉及到，因此再来看一下快照应用的过程，这个过程也比较简单，贴一下代码即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 func (r *raft) handleSnapshot(m pb.Message) { sindex, sterm := m.Snapshot.Metadata.Index, m.Snapshot.Metadata.Term if r.restore(m.Snapshot) { r.logger.Infof(\u0026#34;%x [commit: %d] restored snapshot [index: %d, term: %d]\u0026#34;, r.id, r.raftLog.committed, sindex, sterm) r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.lastIndex()}) } else { // 自身的进度快于发送的Snaoshot，将自己commit的最后一条日志发送给Leader，便于其更新Match与Next r.logger.Infof(\u0026#34;%x [commit: %d] ignored snapshot [index: %d, term: %d]\u0026#34;, r.id, r.raftLog.committed, sindex, sterm) r.send(pb.Message{To: m.From, Type: pb.MsgAppResp, Index: r.raftLog.committed}) } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 func (r *raft) restore(s pb.Snapshot) bool { if s.Metadata.Index \u0026lt;= r.raftLog.committed { return false } if r.state != StateFollower { // This is defense-in-depth: if the leader somehow ended up applying a // snapshot, it could move into a new term without moving into a // follower state. This should never fire, but if it did, we\u0026#39;d have // prevented damage by returning early, so log only a loud warning. // // At the time of writing, the instance is guaranteed to be in follower // state when this method is called. r.logger.Warningf(\u0026#34;%x attempted to restore snapshot as leader; should never happen\u0026#34;, r.id) r.becomeFollower(r.Term+1, None) return false } // More defense-in-depth: throw away snapshot if recipient is not in the // config. This shouldn\u0026#39;t ever happen (at the time of writing) but lots of // code here and there assumes that r.id is in the progress tracker. found := false cs := s.Metadata.ConfState for _, set := range [][]uint64{ cs.Voters, cs.Learners, cs.VotersOutgoing, // `LearnersNext` doesn\u0026#39;t need to be checked. According to the rules, if a peer in // `LearnersNext`, it has to be in `VotersOutgoing`. } { for _, id := range set { if id == r.id { found = true break } } if found { break } } if !found { r.logger.Warningf( \u0026#34;%x attempted to restore snapshot but it is not in the ConfState %v; should never happen\u0026#34;, r.id, cs, ) return false } // Now go ahead and actually restore. // 在自身的日志当中可以找到Snapshot当中最后的Index和Term，对该Snapshot进行忽略 if r.raftLog.matchTerm(s.Metadata.Index, s.Metadata.Term) { r.logger.Infof(\u0026#34;%x [commit: %d, lastindex: %d, lastterm: %d] fast-forwarded commit to snapshot [index: %d, term: %d]\u0026#34;, r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term) // 快照当中的为已经提交的日志，因此可能需要对本地的commit进度进行更新 r.raftLog.commitTo(s.Metadata.Index) return false } r.raftLog.restore(s) // Reset the configuration and add the (potentially updated) peers in anew. r.prs = tracker.MakeProgressTracker(r.prs.MaxInflight) cfg, prs, err := confchange.Restore(confchange.Changer{ Tracker: r.prs, LastIndex: r.raftLog.lastIndex(), }, cs) if err != nil { // This should never happen. Either there\u0026#39;s a bug in our config change // handling or the client corrupted the conf change. panic(fmt.Sprintf(\u0026#34;unable to restore config %+v: %s\u0026#34;, cs, err)) } assertConfStatesEquivalent(r.logger, cs, r.switchToConfig(cfg, prs)) pr := r.prs.Progress[r.id] pr.MaybeUpdate(pr.Next - 1) // TODO(tbg): this is untested and likely unneeded r.logger.Infof(\u0026#34;%x [commit: %d, lastindex: %d, lastterm: %d] restored snapshot [index: %d, term: %d]\u0026#34;, r.id, r.raftLog.committed, r.raftLog.lastIndex(), r.raftLog.lastTerm(), s.Metadata.Index, s.Metadata.Term) return true } 相关优化 引用：etcd/raft\n快速回退 快速回退在Raft原本论文和MIT6.824的课上都做出了相关说明，在上文中也分析了整个过程，即通过follower去找到和自身冲突的位置，之后返回给Leader，不过在Raft原文当中，采用的是XTerm + XLen + XIndex的组合方式来解决问题:\nXTerm:冲突的term号 XIndex：XTerm的第一个Index Xlen：follower的日志的长度 case：\ns1 4 5 5 s2 4 6 6 6 s1 4 4 4 s2 4 6 6 6 s1 4 s2 4 6 6 6 对于第一个case：s2第一次发送AppendEntry后接收到的XTerm为5，XIndex为2（index从1开始），leader当中并没有term = 5，因此后续的AppendEntry可以直接跳转到Index = 2开始发送 对于第二个case：leader当中含有XTerm = 4，因此leader就从XTerm的最后一个Entry开始进行同步 对于第三个case：follower当中不存在leader的entry，通过XLen来找到最后一个Entry进行同步 MIT6.824当中的实现：\nfollower\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 // figure 2 AppendEntries RPC Receiver implementation 2 // log is too short if rf.getLastIndex() \u0026lt; args.PrevLogIndex { reply.Success = false reply.Conflict = true reply.XTerm = -1 reply.XIndex = -1 reply.XLen = rf.getLastIndex() Debug(dLog, \u0026#34;S%d follower\u0026#39;s log is to short,index:%d,prevLogIndex%d\u0026#34;, rf.me, rf.getLastIndex(), args.PrevLogIndex) Debug(dLog, \u0026#34;S%d Conflict Xterm %d,XIndex %d,XLen %d\u0026#34;, rf.me, reply.XTerm, reply.XIndex, reply.XLen) return } Debug(dLog, \u0026#34;S%d log is %s\u0026#34;, rf.me, rf.log.String()) // figure 2 AppendEntries RPC Receiver implementation 2 if rf.restoreLogTerm(args.PrevLogIndex) != args.PrevLogTerm { reply.Success = false reply.Conflict = true // set XTerm XLen XIndex reply.XTerm = rf.restoreLogTerm(args.PrevLogIndex) for idx := args.PrevLogIndex; idx \u0026gt; rf.lastIncludedIndex; idx-- { // the first index of the conflict term if rf.restoreLogTerm(idx-1) != reply.XTerm { reply.XIndex = idx break } } reply.XLen = len(rf.log.Entries) Debug(dLog, \u0026#34;S%d Conflict Xterm %d, XIndex %d,XLen %d\u0026#34;, rf.me, reply.XTerm, reply.XIndex, reply.XLen) return } leader​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 } else if reply.Conflict { // 如果存在冲突则通过XTerm XLen XIndex进行快速定位 // XTerm == -1则证明 follower当中日志过短，不存在和发送的同term的entry，直接按长度从最末尾进行重写 if reply.XTerm == -1 { rf.nextIndex[server] = reply.XLen } else { // xIndex为0则证明 follower当中没找到，即只有一个term的情况,需要leader进行定位 // follower : 4 4 4 // leader : 4 4 5 5 5 if reply.XIndex == 0 { last := rf.findLastLogByTerm(reply.XTerm) rf.nextIndex[server] = last } else { // XIndex找到了则直接使用XIndex的即可 // 也有可能是follower的日志已经快照化，让leader重新探查 rf.nextIndex[server] = reply.XIndex } } } 在etcd/raft当中，考虑到故障并不会经常发生，将follower的最后一条日志作为该字段的值，因此回退到follower的最后一条日志之后由Leader去一条条检查日志。\n并行写入 在原本的Raft当中，对于新提议的日志，首先需要将新日志首先写入到本地存储之后再为follower复制日志，发送网络请求的过程被写入持久化的操作阻塞，造成延迟。\n通过并行写入的方式，leader可以在为follower复制日志的同时将日志写入到本地的稳定存储之中，leader自己的match index表示其写入到了稳定存储的最后一条日志的索引。当当前term的某日志被大多数的match index覆盖时，leader便可以使commit index前进到该节点处。\n批处理 批处理的操作主要体现在以下几个方面：\npropose时stepWait，积攒一批propose，最后通过Step向下提交的为一个[]Entry 通过Ready所向上层返回的也是一批数据，一段时间内需要进行持久化的HardState、Entry、Snapshot、需要发送给其他节点的[]Messages leader在为follower进行日志复制时也会成组发送，这个在6.824当中也有，算是基本实现 流水线 和体系机构当中的概念相同，在StateReplicate的状态下，Leader认为Follower可以稳定跟随Leader的进度，因此在不收到响应之前，立即对Leader所匹配的NextIndex进行更新，之后继续处理下一条，在同一时间，Leader所在发送的还未结束的指令不止一条。\n在正常且稳定的情况下，消息应恰好一次且有序到达。但是在异常情况下，可能出现消息丢失、消息乱序。当follower收到过期的日志复制请求时，会拒绝该请求，随后follower会回退其nextIndex以重传之后的日志。\nSummary 相比于Raft选举的过程当中，日志复制的整个流程和要考虑的东西也就复杂的多，再引入了Step、ProgressTracker、以及存储与通信单独处理，将存储、通信、状态跟踪与确认和整个复制流程解藕之后架构上还是比较清晰的。不过由于分布式的不确定性，整个实现上基本遵循了防御性编程的理念，以确保系统处于预期的状态。\n‍\n","permalink":"http://itfischer.space/en/posts/tech/etcd/raft%E6%97%A5%E5%BF%97/","summary":"Raft日志 日志存储 Raft日志在存储上分为两部分，一部分为新写入或者新生成的日志，暂时存储于内存当中，还未来得及进行稳定存储。而另一部分则","title":"etcd/raft日志复制"},{"content":"etcd-raft整体架构 引用：http://blog.mrcroxx.com/posts/code-reading/etcdraft-made-simple/2-overview/\nRaftNode 就像在raft-example当中分析的那样，Raft Package只负责Raft 的具体逻辑的实现，而存储通信等模块就交给了上层的Raft Server，而二者之间的桥梁即为interface node​ Raft Package对于raft进行进一步的封装，对外只暴露一个node借口，开发者可以使用Node接口当中的函数来使用Raft，而对于Node接口，Raft Package由定义了两个实现的结构体，分别是node 和rawnode，二者的主要区别为node通过channel保证了线程安全问题。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 type Node interface { // Tick increments the internal logical clock for the Node by a single tick. Election // timeouts and heartbeat timeouts are in units of ticks. Tick() // Campaign causes the Node to transition to candidate state and start campaigning to become leader. Campaign(ctx context.Context) error // Propose proposes that data be appended to the log. Note that proposals can be lost without // notice, therefore it is user\u0026#39;s job to ensure proposal retries. Propose(ctx context.Context, data []byte) error // ProposeConfChange proposes a configuration change. Like any proposal, the // configuration change may be dropped with or without an error being // returned. In particular, configuration changes are dropped unless the // leader has certainty that there is no prior unapplied configuration // change in its log. // // The method accepts either a pb.ConfChange (deprecated) or pb.ConfChangeV2 // message. The latter allows arbitrary configuration changes via joint // consensus, notably including replacing a voter. Passing a ConfChangeV2 // message is only allowed if all Nodes participating in the cluster run a // version of this library aware of the V2 API. See pb.ConfChangeV2 for // usage details and semantics. ProposeConfChange(ctx context.Context, cc pb.ConfChangeI) error // Step advances the state machine using the given message. ctx.Err() will be returned, if any. Step(ctx context.Context, msg pb.Message) error // Ready returns a channel that returns the current point-in-time state. // Users of the Node must call Advance after retrieving the state returned by Ready. // // NOTE: No committed entries from the next Ready may be applied until all committed entries // and snapshots from the previous one have finished. Ready() \u0026lt;-chan Ready // Advance notifies the Node that the application has saved progress up to the last Ready. // It prepares the node to return the next available Ready. // // The application should generally call Advance after it applies the entries in last Ready. // // However, as an optimization, the application may call Advance while it is applying the // commands. For example. when the last Ready contains a snapshot, the application might take // a long time to apply the snapshot data. To continue receiving Ready without blocking raft // progress, it can call Advance before finishing applying the last ready. Advance() // ApplyConfChange applies a config change (previously passed to // ProposeConfChange) to the node. This must be called whenever a config // change is observed in Ready.CommittedEntries, except when the app decides // to reject the configuration change (i.e. treats it as a noop instead), in // which case it must not be called. // // Returns an opaque non-nil ConfState protobuf which must be recorded in // snapshots. ApplyConfChange(cc pb.ConfChangeI) *pb.ConfState // TransferLeadership attempts to transfer leadership to the given transferee. TransferLeadership(ctx context.Context, lead, transferee uint64) // ReadIndex request a read state. The read state will be set in the ready. // Read state has a read index. Once the application advances further than the read // index, any linearizable read requests issued before the read request can be // processed safely. The read state will have the same rctx attached. // Note that request can be lost without notice, therefore it is user\u0026#39;s job // to ensure read index retries. ReadIndex(ctx context.Context, rctx []byte) error // Status returns the current status of the raft state machine. Status() Status // ReportUnreachable reports the given node is not reachable for the last send. ReportUnreachable(id uint64) // ReportSnapshot reports the status of the sent snapshot. The id is the raft ID of the follower // who is meant to receive the snapshot, and the status is SnapshotFinish or SnapshotFailure. // Calling ReportSnapshot with SnapshotFinish is a no-op. But, any failure in applying a // snapshot (for e.g., while streaming it from leader to follower), should be reported to the // leader with SnapshotFailure. When leader sends a snapshot to a follower, it pauses any raft // log probes until the follower can apply the snapshot and advance its state. If the follower // can\u0026#39;t do that, for e.g., due to a crash, it could end up in a limbo, never getting any // updates from the leader. Therefore, it is crucial that the application ensures that any // failure in snapshot sending is caught and reported back to the leader; so it can resume raft // log probing in the follower. ReportSnapshot(id uint64, status SnapshotStatus) // Stop performs any necessary termination of the Node. Stop() } 结构体当中的注视已经给的较为清晰，挑几个重点说一下：\n首先是Tick，正如之前所说的那样，etcd当中的raft模块使用的是逻辑时钟，即需要手动调用某些函数来推进时钟，即为此处的Tick，该Tick留给上层在serverChannels当中进行调用。\n在Node当中既有一个Tick()函数的实现，其向一个channel当中发送一个空结构体作为通知，而这个通知就被node.run()当中的一个select接收到，之后调用下层的raft的tick最终驱动时钟\n而rn的Tick()为一个函数类型的字段，会根据当前raftnode的身份而选择不同的Tick()，如TickElection()、TickHeartBeat()等\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 func (n *node) Tick() { select { case n.tickc \u0026lt;- struct{}{}: case \u0026lt;-n.done: default: n.rn.raft.logger.Warningf(\u0026#34;%x A tick missed to fire. Node blocks too long!\u0026#34;, n.rn.raft.id) } } // in node.run() select { case \u0026lt;-n.tickc: n.rn.Tick() } Campaign和Propose分别是用来进行选举和将上层kvstore传下来的请求封装为日志，这个留到之后的选举流程和日志再进行分析\nStep()为整个Raft模块的核心，整个Raft状态机的改变或者更新都由Step函数来完成\nReady()之前已经分析过，在Raft Package当中已经完成了的工作，之后需要上层的raft server进行处理的都通过Ready进行传递，如需要持久化存储的HardState、Entries，已经提交了需要进行Apply的committedEntries，以及需要发送给其他节点的Messages[]\nAdvance()，通常和Ready()成对出现，当上层的Raft Server处理完一批通过Ready返回的内容之后，通过Advance()来通知下层的Raft Node已经完成，推进进度\nReadIndex()用于保证线性一致性，读取状态具有一个读取索引。一旦应用程序的进度超过了读取索引，任何在读取请求之前发出的线性可读请求就可以安全地处理。\nRaft Package 还是先看一下相关的结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 type raft struct { id uint64 Term uint64 Vote uint64 readStates []ReadState // the log raftLog *raftLog maxMsgSize uint64 maxUncommittedSize uint64 // TODO(tbg): rename to trk. prs tracker.ProgressTracker state StateType // isLearner is true if the local raft node is a learner. isLearner bool msgs []pb.Message // the leader id lead uint64 // leadTransferee is id of the leader transfer target when its value is not zero. // Follow the procedure defined in raft thesis 3.10. leadTransferee uint64 // Only one conf change may be pending (in the log, but not yet // applied) at a time. This is enforced via pendingConfIndex, which // is set to a value \u0026gt;= the log index of the latest pending // configuration change (if any). Config changes are only allowed to // be proposed if the leader\u0026#39;s applied index is greater than this // value. pendingConfIndex uint64 // an estimate of the size of the uncommitted tail of the Raft log. Used to // prevent unbounded log growth. Only maintained by the leader. Reset on // term changes. uncommittedSize uint64 readOnly *readOnly // number of ticks since it reached last electionTimeout when it is leader // or candidate. // number of ticks since it reached last electionTimeout or received a // valid message from current leader when it is a follower. electionElapsed int // number of ticks since it reached last heartbeatTimeout. // only leader keeps heartbeatElapsed. heartbeatElapsed int checkQuorum bool preVote bool heartbeatTimeout int electionTimeout int // randomizedElectionTimeout is a random number between // [electiontimeout, 2 * electiontimeout - 1]. It gets reset // when raft changes its state to follower or candidate. randomizedElectionTimeout int disableProposalForwarding bool tick func() step stepFunc logger Logger // pendingReadIndexMessages is used to store messages of type MsgReadIndex // that can\u0026#39;t be answered as new leader didn\u0026#39;t committed any log in // current term. Those will be handled as fast as first log is committed in // current term. pendingReadIndexMessages []pb.Message } 在其中除了一个raft本身的结构体，还有一个config结构体，在在上层的startRaft当中调用配置Raft的初始信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 type Config struct { // ID is the identity of the local raft. ID cannot be 0. ID uint64 // ElectionTick is the number of Node.Tick invocations that must pass between // elections. That is, if a follower does not receive any message from the // leader of current term before ElectionTick has elapsed, it will become // candidate and start an election. ElectionTick must be greater than // HeartbeatTick. We suggest ElectionTick = 10 * HeartbeatTick to avoid // unnecessary leader switching. ElectionTick int // HeartbeatTick is the number of Node.Tick invocations that must pass between // heartbeats. That is, a leader sends heartbeat messages to maintain its // leadership every HeartbeatTick ticks. HeartbeatTick int // Storage is the storage for raft. raft generates entries and states to be // stored in storage. raft reads the persisted entries and states out of // Storage when it needs. raft reads out the previous state and configuration // out of storage when restarting. Storage Storage // Applied is the last applied index. It should only be set when restarting // raft. raft will not return entries to the application smaller or equal to // Applied. If Applied is unset when restarting, raft might return previous // applied entries. This is a very application dependent configuration. Applied uint64 // MaxSizePerMsg limits the max byte size of each append message. Smaller // value lowers the raft recovery cost(initial probing and message lost // during normal operation). On the other side, it might affect the // throughput during normal replication. Note: math.MaxUint64 for unlimited, // 0 for at most one entry per message. MaxSizePerMsg uint64 // MaxCommittedSizePerReady limits the size of the committed entries which // can be applied. MaxCommittedSizePerReady uint64 // MaxUncommittedEntriesSize limits the aggregate byte size of the // uncommitted entries that may be appended to a leader\u0026#39;s log. Once this // limit is exceeded, proposals will begin to return ErrProposalDropped // errors. Note: 0 for no limit. MaxUncommittedEntriesSize uint64 // MaxInflightMsgs limits the max number of in-flight append messages during // optimistic replication phase. The application transportation layer usually // has its own sending buffer over TCP/UDP. Setting MaxInflightMsgs to avoid // overflowing that sending buffer. TODO (xiangli): feedback to application to // limit the proposal rate? MaxInflightMsgs int // CheckQuorum specifies if the leader should check quorum activity. Leader // steps down when quorum is not active for an electionTimeout. CheckQuorum bool // PreVote enables the Pre-Vote algorithm described in raft thesis section // 9.6. This prevents disruption when a node that has been partitioned away // rejoins the cluster. PreVote bool // ReadOnlyOption specifies how the read only request is processed. // // ReadOnlySafe guarantees the linearizability of the read only request by // communicating with the quorum. It is the default and suggested option. // // ReadOnlyLeaseBased ensures linearizability of the read only request by // relying on the leader lease. It can be affected by clock drift. // If the clock drift is unbounded, leader might keep the lease longer than it // should (clock can move backward/pause without any bound). ReadIndex is not safe // in that case. // CheckQuorum MUST be enabled if ReadOnlyOption is ReadOnlyLeaseBased. ReadOnlyOption ReadOnlyOption // Logger is the logger used for raft log. For multinode which can host // multiple raft group, each raft group can have its own logger Logger Logger // DisableProposalForwarding set to true means that followers will drop // proposals, rather than forwarding them to the leader. One use case for // this feature would be in a situation where the Raft leader is used to // compute the data of a proposal, for example, adding a timestamp from a // hybrid logical clock to data in a monotonically increasing way. Forwarding // should be disabled to prevent a follower with an inaccurate hybrid // logical clock from assigning the timestamp and then forwarding the data // to the leader. DisableProposalForwarding bool } Message 整个Raft的状态机的切换都是通过Step函数，而Step函数调用时会传入一个message，因此不妨先看一下message\ngrpc当中的消息传输为protobuf的形式，message通过grpc进行传输，首先在raft.proto当中定义，之后可以生成对应的go结构体。\n所有的rpc的消息都通过Message来承载，如RequestVote,AppendEntries，通过一个MessageType进行区分。\nMessage也并不完全用于消息传输，对于本机的操作涉及到修改状态机的也可以通过封装成一个Message来交给Step处理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Message struct { Type MessageType `protobuf:\u0026#34;varint,1,opt,name=type,enum=raftpb.MessageType\u0026#34; json:\u0026#34;type\u0026#34;` To uint64 `protobuf:\u0026#34;varint,2,opt,name=to\u0026#34; json:\u0026#34;to\u0026#34;` From uint64 `protobuf:\u0026#34;varint,3,opt,name=from\u0026#34; json:\u0026#34;from\u0026#34;` Term uint64 `protobuf:\u0026#34;varint,4,opt,name=term\u0026#34; json:\u0026#34;term\u0026#34;` // logTerm is generally used for appending Raft logs to followers. For example, // (type=MsgApp,index=100,logTerm=5) means leader appends entries starting at // index=101, and the term of entry at index 100 is 5. // (type=MsgAppResp,reject=true,index=100,logTerm=5) means follower rejects some // entries from its leader as it already has an entry with term 5 at index 100. LogTerm uint64 `protobuf:\u0026#34;varint,5,opt,name=logTerm\u0026#34; json:\u0026#34;logTerm\u0026#34;` Index uint64 `protobuf:\u0026#34;varint,6,opt,name=index\u0026#34; json:\u0026#34;index\u0026#34;` Entries []Entry `protobuf:\u0026#34;bytes,7,rep,name=entries\u0026#34; json:\u0026#34;entries\u0026#34;` Commit uint64 `protobuf:\u0026#34;varint,8,opt,name=commit\u0026#34; json:\u0026#34;commit\u0026#34;` Snapshot Snapshot `protobuf:\u0026#34;bytes,9,opt,name=snapshot\u0026#34; json:\u0026#34;snapshot\u0026#34;` Reject bool `protobuf:\u0026#34;varint,10,opt,name=reject\u0026#34; json:\u0026#34;reject\u0026#34;` RejectHint uint64 `protobuf:\u0026#34;varint,11,opt,name=rejectHint\u0026#34; json:\u0026#34;rejectHint\u0026#34;` Context []byte `protobuf:\u0026#34;bytes,12,opt,name=context\u0026#34; json:\u0026#34;context,omitempty\u0026#34;` } 消息的类型可以在 当中查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 type MessageType int32 // MsgHup用于进行选举，当超时是向step函数传入MsgHup // MsgBeat用于让Leader向其follower去发送心跳信息 // MsgProp用于AE，传入step之后Leader首先先向自己写入Entries，之后向follower广播 // MsgApp当中包含要添加的Entries，如果candidate收到之后，会将自身转换为follower // const ( MsgHup MessageType = 0 MsgBeat MessageType = 1 MsgProp MessageType = 2 MsgApp MessageType = 3 MsgAppResp MessageType = 4 MsgVote MessageType = 5 MsgVoteResp MessageType = 6 MsgSnap MessageType = 7 MsgHeartbeat MessageType = 8 MsgHeartbeatResp MessageType = 9 MsgUnreachable MessageType = 10 MsgSnapStatus MessageType = 11 MsgCheckQuorum MessageType = 12 MsgTransferLeader MessageType = 13 MsgTimeoutNow MessageType = 14 MsgReadIndex MessageType = 15 MsgReadIndexResp MessageType = 16 MsgPreVote MessageType = 17 MsgPreVoteResp MessageType = 18 ) Step etcd当中将所有和Raft状态机相关的操作全部集中于Step当中，而具体应当进行何种操作则根据Step当中传入的Message当中的MessageType进行判断，而对于Message的调用，一般在Node当中或直接或间接的对其进行调用。\n对于Step，首先定义一个统一的Step函数，在其中进行一些预处理或者综合处理，即无论当前的节点的身份为何都可能需要进行的操作，之后在单独定义几个step函数，针对不同身份的节点。在Step当中，综合处理完之后仍需处理的就调用单独的step。而如果有一些操作就是针对某些特定身份，也可以直接调用对应的stepxxx，无需调用Step。\n针对特定身份的step如下，作为一个函数类型的变量定义在结构体当中，在状态切换时进行设置\n1 2 3 func stepLeader(r *raft, m pb.Message) error {...} func stepCandidate(r *raft, m pb.Message) error {...} func stepFollower(r *raft, m pb.Message) error {...} 1 2 3 4 5 6 7 8 func (r *raft) becomeFollower(term uint64, lead uint64) { r.step = stepFollower r.reset(term) r.tick = r.tickElection r.lead = lead r.state = StateFollower r.logger.Infof(\u0026#34;%x became follower at term %d\u0026#34;, r.id, r.Term) } 1 2 3 4 5 6 7 8 9 switch m.Type { case xxx: // ... default: err := r.step(r, m) if err != nil { return err } ‍\nCampaign:Campaign只会由candidate出发，对step​的直接调用，最终会调用到stepCandidate​\n1 2 3 func (n *node) Campaign(ctx context.Context) error { return n.step(ctx, pb.Message{Type: pb.MsgHup}) } Tick​：算是对Step的间接调用，主要顺序如下：\n调用Tick时向channel当中发送一条通知 该通知被运行node.run()当中的协程获取到，调用下层raft的Tick，该Tick同样为一个函数类型的变量，如果当前为Leader，那么就会调用到tickHeartBeat​ 在tickHeartBeat​当中调用到最终的Step，对状态机进行操作 1 2 3 4 5 6 7 8 func (n *node) Tick() { select { case n.tickc \u0026lt;- struct{}{}: case \u0026lt;-n.done: default: n.rn.raft.logger.Warningf(\u0026#34;%x A tick missed to fire. Node blocks too long!\u0026#34;, n.rn.raft.id) } } 1 2 case \u0026lt;-n.tickc: n.rn.Tick() 1 2 3 func (rn *RawNode) Tick() { rn.raft.tick() } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 func (r *raft) tickHeartbeat() { r.heartbeatElapsed++ r.electionElapsed++ if r.electionElapsed \u0026gt;= r.electionTimeout { r.electionElapsed = 0 // 检查当前活跃的follower是否满足quorum 即大多数 if r.checkQuorum { r.Step(pb.Message{From: r.id, Type: pb.MsgCheckQuorum}) } // If current leader cannot transfer leadership in electionTimeout, it becomes leader again. if r.state == StateLeader \u0026amp;\u0026amp; r.leadTransferee != None { r.abortLeaderTransfer() } } if r.state != StateLeader { return } if r.heartbeatElapsed \u0026gt;= r.heartbeatTimeout { r.heartbeatElapsed = 0 r.Step(pb.Message{From: r.id, Type: pb.MsgBeat}) } } ‍\n‍\n","permalink":"http://itfischer.space/en/posts/tech/etcd/etcd-raft%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84/","summary":"etcd-raft整体架构 引用：http://blog.mrcroxx.com/posts/code-reading/etcdraft-ma","title":"etcd-raft整体架构"},{"content":"Raft选举流程 引用：http://blog.mrcroxx.com/posts/code-reading/etcdraft-made-simple/3-election/\nRaft选举流程优化 对于Raft的基本实现，呈现在Diego Ongaro的《In Search of an Understandable Consensus Algorithm (Extended Version)》一文当中。实现了Raft的基本功能。而又在Ongaro的博士论文当中，对于Raft的选举流程提出了三种优化，分别是Pre-Vote,Check Quorum、Leader Lease。\nPreVote 想象这样的一种场景，在一个五个节点的Raft集群当中发生了网络分区， 分为了一个三个节点的分区和一个两个节点的分区。\n三个节点的分区当中如果不发生宕机的话，由于满足Quorum数，Leader可以一直维持他的任期，因此Term一直不变。但是两个当中的节点数不足，因此一直会尝试Leader Election的过程， 又因为节点数不足，选举一直不会成功，而在这个过程当中，节点每次进行选举都会自增自身的Term号。\n当网络分区恢复之后，两个分区又能够互通，此时3个节点的分区当中的Leader会收到来自二节点的RequestVote，并且Term高于自身的Term，从而会导致Leader转换为Follower，之后在重新进行一轮选举。\n在这种情况下，会导致系统的Term无意义的增大，并且在网络分区恢复时会额外进行一次选举。\n通过PreVote的机制，可以解决以上问题，在正式投票之前，先进行一轮遇投票，在预投票阶段不对自己的Term进行自增，如果在预投票当中能够选出Leader，即超过1/2的节点给出赞成投票，那么就可以对自己的Term进行自增，进行正式的选举过程。此时就可以保证在上面的例子当中右边的分区由于一直无法选举出Leader，从而不会对Term进行自增，之后网络分区恢复之后，Term也一定 \u0026lt;=主分区的Term，从而可以很自然的加入到主分区当中，不会引起当前Leader的退位。\nCheck Quorum 还是以刚才的例子，但是这次在出现网络分区前Leader位于右边的分区当中，虽然此时新的请求再也无法达成共识，但是由于Raft的节点不会主动的检测当前集群的状态，因此此时的Leader仍认为自己为Leader，还会不断的接受新的请求和发送心跳信息。\n如果像MIT6.824当中那样通过Raft日志来保证强一致性，即每条请求无论读写都将其写入到Raft日志当中形成共识，当达成共识之后再给予客户端响应。此时在少数分区当中的Leader接受了请求也无法达成共识，从而会超时向客户端报错。之后客户端就可以判断当前的Leader为不可用状态。此时没有出现任何问题。\n但是用于网络应用通常为读多写少的类型，此时使用Log Read会导致读的性能很差，因此可以采用在Leader进行本地读的方式，提高读取性能。\n但是此时如果发生上述的网络分区问题时，就会读取到旧的数据，违反线性一致性。\n在开启Check Quorum后，Leader会周期性的向Follower去发送一条信息，去确认当前集群当中的存活的Follower数量，是否满足Quorum，如果不满足，则证明此时发生了网络分区，并且当前的Leader处于节点数少的那个分区，此时Leader就应当退位变为Follower。通过Check Quorum机制可以尽早的发现网络分区的问题，但是依旧不能完全解决stale read的问题，需要其他的手段来保证强一致性。\nLeader Lease 在分布式环境下，可能会出现部分网络分区的问题，即A-B B-C之前都能相互通信，但是A-C之间不通。\n此时节点2会因为收不到节点1的消息而尝试重新进行选举，而在节点2和节点1发生网络部分分区到节点2超时重新选举这段时间内如果1 3没有写入新的Log，节点2就可以拿到自身和节点3的选票，从而成为Leader。由于选举导致的节点Term增加就会从2 传到 3，之后1再向3发送消息就会因为Term低于节点3而变为Follower，最终节点2会成为整个集群新的Node，但是集群的状态也原本并没有什么差异，可以视为一个无意义的选举。\n通过Leader Lease可以解决该问题：当节点在Election Timeout超时之前，如果收到了Leader的消息，那么他就不会再去响应其他节点发起的RequestVote的投票和预投票的消息。即阻止正常工作的节点给其他节点进行投票。\nLeader Lease机制需要和Check Quorum配合使用，一种可能的局部网络分区如下：\n‍\n此时理想状态为3 4 发起选举，产生新的集群[2 3 4]，如果只引入了Leader Lease机制，此时只有2可以与Leader1进行正常通信，因此此时的2不会开启选举也不会为其他人投票，3 4 最多只能得到两票从而无法选举出Leader，而如果引入Check Quorum机制之后，Leader最多只能检测到一个节点，因此就会退位，从而不会因为Leader Lease干扰正常的选举流程，2 3 4形成新的可用集群。\n优化引入的问题 在引入Leader Lease并顺带开启Check Quorum之后，无论是否开启PreVote，都会引入新的问题，假设不开启PreVote，在发生网络分区之后，节点3就会不断的自增自己的Term，而如果开启了Leader Lease，节点1 2就会忽略节点3的投票信息，或节点1 2写入了新的日志，由于日志的安全性问题忽略3的RequestVote，而节点1 2向节点3发送的消息也会因为Term过低而导致被忽略。\n而如果引入了PreVote之后也会产生类似的情况，网络分区恢复之后节点3发起一轮PreVote，并得到了节点1 2 的回应，又产生了节点3的Term比1 2高的问题\n在原本的Raft当中，节点1 2收到了3的RequestVote，由于3的Term更高，从而会令1退位并更新Term，2更新Term，之后即可在相同的Term下进行新一轮的选举，从而避免了上述的情况。\n处理方法也很简单，按照和原本差不多的思路进行处理，原本会自动处理跟随高Term，此处就手动发送一次高Term强制跟随：如果收到了term比当前节点term低的leader的消息，且集群开启了Check Quorum / Leader Lease或Pre-Vote，那么发送一条term为当前term的消息，令term低的节点成为follower\n配置更新\n在更复杂的情况中，比如，在变更配置时，开启了原本没有开启的Pre-Vote机制。此时可能会出现与上一条类似的情况，即可能因term更高但是log更旧的节点的存在导致整个集群的死锁，所有节点都无法预投票成功。这种情况比上一种情况更危险，上一种情况只有之前分区的节点无法加入集群，在这种情况下，整个集群都会不可用\n处理方法为对于term比当前节点低的Term的PreVote，无论是否开启了Check Quorum Leader Lease，都通过发送一条为当前Term的信息，迫使其转换为Follower并更新Term。\n选举流程 RaftNode/Campaign 在Node接口当中，对外提供了一个Campaign函数，调用该函数即可以MsgHup为参数调用step(此处调用的为stepFollower)，从而开启选举的过程\n1 2 3 func (n *node) Campaign(ctx context.Context) error { return n.step(ctx, pb.Message{Type: pb.MsgHup}) } 但是更为常见的为通过Tick()函数进行触发，经过在上一章当中分析的Tick流程，最终调用到了follower或者candidate的tickEelction,之后在调用Step，在其中触发选举\n1 2 3 4 5 6 7 8 9 // tickElection is run by followers and candidates after r.electionTimeout. func (r *raft) tickElection() { r.electionElapsed++ if r.promotable() \u0026amp;\u0026amp; r.pastElectionTimeout() { r.electionElapsed = 0 r.Step(pb.Message{From: r.id, Type: pb.MsgHup}) } } hup/campaign 最终的选举逻辑是在hup/campaign函数当中进行处理的，先看一看当中的逻辑，之后再研究这两个函数的调用时机\nhup hup函数经过一定的检查之后开启选举过程：\n当前如果为Leader，那么则不能够进行选举 通过promotable​查看当前节点是否能够提升为Leader 查看当前是否有还有提交的日志。 当前的节点已提交的日志中，是否有还未被应用的集群配置变更ConfChange​消息，如果有，依旧不能进行选举 如果上述检查全部通过，那么即可尝试进行选举 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 func (r *raft) hup(t CampaignType) { if r.state == StateLeader { r.logger.Debugf(\u0026#34;%x ignoring MsgHup because already leader\u0026#34;, r.id) return } if !r.promotable() { r.logger.Warningf(\u0026#34;%x is unpromotable and can not campaign\u0026#34;, r.id) return } ents, err := r.raftLog.slice(r.raftLog.applied+1, r.raftLog.committed+1, noLimit) if err != nil { r.logger.Panicf(\u0026#34;unexpected error getting unapplied entries (%v)\u0026#34;, err) } if n := numOfPendingConf(ents); n != 0 \u0026amp;\u0026amp; r.raftLog.committed \u0026gt; r.raftLog.applied { r.logger.Warningf(\u0026#34;%x cannot campaign at term %d since there are still %d pending configuration changes to apply\u0026#34;, r.id, r.Term, n) return } r.logger.Infof(\u0026#34;%x is starting a new election at term %d\u0026#34;, r.id, r.Term) r.campaign(t) } promoptable​：\n在该函数当中主要检查三条：\n当前节点是否为新加入集群当中追赶进度的Learner 当前节点是否属于当前的集群 是否还有未应用的Snapshot 1 2 3 4 5 6 // promotable indicates whether state machine can be promoted to leader, // which is true when its own id is in progress list. func (r *raft) promotable() bool { pr := r.prs.Progress[r.id] return pr != nil \u0026amp;\u0026amp; !pr.IsLearner \u0026amp;\u0026amp; !r.raftLog.hasPendingSnapshot() } 之后，即可调用campaign尝试进行选举\ncampaign 由于调用campagin的位置并不只hup当中，因此最初首先也需要检查promotable。\n调用campaign时，传入一个参数表示当前为预选举还是正式选举。之后根据该参数对当前节点的状态进行更新\n1 2 3 4 5 6 7 8 9 10 if t == campaignPreElection { r.becomePreCandidate() voteMsg = pb.MsgPreVote // PreVote RPCs are sent for the next term before we\u0026#39;ve incremented r.Term. term = r.Term + 1 } else { r.becomeCandidate() voteMsg = pb.MsgVote term = r.Term } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 func (r *raft) becomeCandidate() { // TODO(xiangli) remove the panic when the raft implementation is stable if r.state == StateLeader { panic(\u0026#34;invalid transition [leader -\u0026gt; candidate]\u0026#34;) } r.step = stepCandidate r.reset(r.Term + 1) r.tick = r.tickElection r.Vote = r.id r.state = StateCandidate r.logger.Infof(\u0026#34;%x became candidate at term %d\u0026#34;, r.id, r.Term) } func (r *raft) becomePreCandidate() { // TODO(xiangli) remove the panic when the raft implementation is stable if r.state == StateLeader { panic(\u0026#34;invalid transition [leader -\u0026gt; pre-candidate]\u0026#34;) } // Becoming a pre-candidate changes our step functions and state, // but doesn\u0026#39;t change anything else. In particular it does not increase // r.Term or change r.Vote. r.step = stepCandidate r.prs.ResetVotes() r.tick = r.tickElection r.lead = None r.state = StatePreCandidate r.logger.Infof(\u0026#34;%x became pre-candidate at term %d\u0026#34;, r.id, r.Term) } 在完成了状态更新之后，当前节点首先投自己一票，这个操作是通过poll函数完成的，这个过程并不涉及到任何的rpc，只是单纯的投自己一票并进行记录，之后在根据配置文件来统计是否赢得选票，如果能够赢得选票，则证明当前的raft是以单节点的状态启动的，集群当中只有这一个节点。直接结束当前的选举流程，成为Leader\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func (r *raft) poll(id uint64, t pb.MessageType, v bool) (granted int, rejected int, result quorum.VoteResult) { if v { r.logger.Infof(\u0026#34;%x received %s from %x at term %d\u0026#34;, r.id, t, id, r.Term) } else { r.logger.Infof(\u0026#34;%x received %s rejection from %x at term %d\u0026#34;, r.id, t, id, r.Term) } r.prs.RecordVote(id, v) return r.prs.TallyVotes() } func (p *ProgressTracker) RecordVote(id uint64, v bool) { _, ok := p.Votes[id] if !ok { p.Votes[id] = v } } 1 2 3 4 5 6 7 8 9 10 if _, _, res := r.poll(r.id, voteRespMsgType(voteMsg), true); res == quorum.VoteWon { // We won the election after voting for ourselves (which must mean that // this is a single-node cluster). Advance to the next state. if t == campaignPreElection { r.campaign(campaignElection) } else { r.becomeLeader() } return } 而如果只靠自己给自己投票无法赢得选举的话，就需要借助发送rpc来请求其他的节点给自己投一票，这个发送的过程通过send添加到自身的[]Messages当中，交给上层的raft server去完成网络通信，等待对方投票完成后调用Step并向其中传入一个MsgHupResp的消息类型进行处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 var ids []uint64 { idMap := r.prs.Voters.IDs() ids = make([]uint64, 0, len(idMap)) for id := range idMap { ids = append(ids, id) } sort.Slice(ids, func(i, j int) bool { return ids[i] \u0026lt; ids[j] }) } for _, id := range ids { if id == r.id { continue } r.logger.Infof(\u0026#34;%x [logterm: %d, index: %d] sent %s request to %x at term %d\u0026#34;, r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), voteMsg, id, r.Term) var ctx []byte if t == campaignTransfer { ctx = []byte(t) } r.send(pb.Message{Term: term, To: id, Type: voteMsg, Index: r.raftLog.lastIndex(), LogTerm: r.raftLog.lastTerm(), Context: ctx}) } 只是简单的将msg添加到[]Messages当中，之后通过Ready()交给上层进行发送\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 func (r *raft) send(m pb.Message) { if m.From == None { m.From = r.id } if m.Type == pb.MsgVote || m.Type == pb.MsgVoteResp || m.Type == pb.MsgPreVote || m.Type == pb.MsgPreVoteResp { if m.Term == 0 { // All {pre-,}campaign messages need to have the term set when // sending. // - MsgVote: m.Term is the term the node is campaigning for, // non-zero as we increment the term when campaigning. // - MsgVoteResp: m.Term is the new r.Term if the MsgVote was // granted, non-zero for the same reason MsgVote is // - MsgPreVote: m.Term is the term the node will campaign, // non-zero as we use m.Term to indicate the next term we\u0026#39;ll be // campaigning for // - MsgPreVoteResp: m.Term is the term received in the original // MsgPreVote if the pre-vote was granted, non-zero for the // same reasons MsgPreVote is panic(fmt.Sprintf(\u0026#34;term should be set when sending %s\u0026#34;, m.Type)) } } else { if m.Term != 0 { panic(fmt.Sprintf(\u0026#34;term should not be set when sending %s (was %d)\u0026#34;, m.Type, m.Term)) } // do not attach term to MsgProp, MsgReadIndex // proposals are a way to forward to the leader and // should be treated as local message. // MsgReadIndex is also forwarded to leader. if m.Type != pb.MsgProp \u0026amp;\u0026amp; m.Type != pb.MsgReadIndex { m.Term = r.Term } } r.msgs = append(r.msgs, m) } Step预处理 Step函数包含了所有对状态机进行更改的操作，但是通过stepxxx + 通用处理的拆分之后逻辑上相当的清晰，主要负责一下四种操作，其他的全部交给stepxxx去单独处理：\n处理比当前节点Term高的消息 处理比当前节点Term低的消息 通过hup进行选举 处理Vote PreVote授予投票 比当前Term高的消息： 如果收到了一个更高的Term，并且如果为PreVote或者Vote类型的消息，如果Check Quorum通过，并且当前节点持有（暂时这么形容）Leader Lease，那么就对其忽略，直接返回。\n此外还有一个force变量，用于表明针对优化当中的场景一二的解决方案，如果Context当中的内容为\u0026quot;campaignTransfer\u0026quot;的话，即为通过强制令转移Leader来解决节点无法加入到集群当中的问题。\n1 2 // campaignTransfer represents the type of leader transfer campaignTransfer CampaignType = \u0026#34;CampaignTransfer\u0026#34; 之后，对于MsgPreVote和对方不拒绝的MsgPreVoteResp，忽略即可，不需要进行任何操作。除此之外，对于其他高于自身的消息类型当中，MsgApp、MsgHeartBeat、MsgSnap，这三种信息只能由Leader发送，均代表当前存在一个明确的Leader，对其进行跟随。\n而对于其他情况，比如当前节点为Leader，对一个follower发送了一条Heartbeat，但是收到了一个比自己Term要高的HeartbeatResp，则证明当前的集群当中存在某个未知的Leader，因此同样将自身转换为follower，但是Leader的id却未知。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 case m.Term \u0026gt; r.Term: // 接收到一个更高的Term，但是当前checkQuorum校验通过，即在租期内，因此没必要对其进行投票，不处理 if m.Type == pb.MsgVote || m.Type == pb.MsgPreVote { force := bytes.Equal(m.Context, []byte(campaignTransfer)) inLease := r.checkQuorum \u0026amp;\u0026amp; r.lead != None \u0026amp;\u0026amp; r.electionElapsed \u0026lt; r.electionTimeout if !force \u0026amp;\u0026amp; inLease { // If a server receives a RequestVote request within the minimum election timeout // of hearing from a current leader, it does not update its term or grant its vote r.logger.Infof(\u0026#34;%x [logterm: %d, index: %d, vote: %x] ignored %s from %x [logterm: %d, index: %d] at term %d: lease is not expired (remaining ticks: %d)\u0026#34;, r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), r.Vote, m.Type, m.From, m.LogTerm, m.Index, r.Term, r.electionTimeout-r.electionElapsed) return nil } } switch { case m.Type == pb.MsgPreVote: // Never change our term in response to a PreVote case m.Type == pb.MsgPreVoteResp \u0026amp;\u0026amp; !m.Reject: // We send pre-vote requests with a term in our future. If the // pre-vote is granted, we will increment our term when we get a // quorum. If it is not, the term comes from the node that // rejected our vote so we should become a follower at the new // term. default: // 除此之外按照raft的正常逻辑进行处理，遇到更高Term的对其进行跟随 r.logger.Infof(\u0026#34;%x [term: %d] received a %s message with higher term from %x [term: %d]\u0026#34;, r.id, r.Term, m.Type, m.From, m.Term) if m.Type == pb.MsgApp || m.Type == pb.MsgHeartbeat || m.Type == pb.MsgSnap { // 这三种消息只能由leader发送，且发送者等term更高，因此收到消息后变为follower r.becomeFollower(m.Term, m.From) } else { // 不知道leader是谁，但是收到了更高term的消息依旧需要转变为follower r.becomeFollower(m.Term, None) } } 比当前节点Term更低的消息 最后，如果消息的Term​比当前Term​小，因存在优化引入的额外问题，除了忽略消息外，还要做额外的处理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 case m.Term \u0026lt; r.Term: // 接收到了一个来自更小的Term的心跳信息或者AppendEntry，如果不进行处理，假设产生分区， // 如果没开启preVote，被分区的节点的Term就会大于主分区，而由于checkQuorum，导致主分区的节点不会给 // 次分区的节点投票，次分区的无法成功选举，而次分区的Term大于主分区，主分区发送的会被起统统拒绝，因此就无法再 // 重新加入到集群当中 // 而对于开启preVote时，当一个节点进行preVote之后准备发起正式投票时被分区至分区2，脱离主分区1，其term会高于主分区 // 会发生和上述同样的情况，即该节点再也无法加入到集群当中 // 通过发送一个自身的Term让对方下台，就等于没采取checkQuorum 和Leader lease优化时的处理方法 if (r.checkQuorum || r.preVote) \u0026amp;\u0026amp; (m.Type == pb.MsgHeartbeat || m.Type == pb.MsgApp) { // We have received messages from a leader at a lower term. It is possible // that these messages were simply delayed in the network, but this could // also mean that this node has advanced its term number during a network // partition, and it is now unable to either win an election or to rejoin // the majority on the old term. If checkQuorum is false, this will be // handled by incrementing term numbers in response to MsgVote with a // higher term, but if checkQuorum is true we may not advance the term on // MsgVote and must generate other messages to advance the term. The net // result of these two features is to minimize the disruption caused by // nodes that have been removed from the cluster\u0026#39;s configuration: a // removed node will send MsgVotes (or MsgPreVotes) which will be ignored, // but it will not receive MsgApp or MsgHeartbeat, so it will not create // disruptive term increases, by notifying leader of this node\u0026#39;s activeness. // The above comments also true for Pre-Vote // // When follower gets isolated, it soon starts an election ending // up with a higher term than leader, although it won\u0026#39;t receive enough // votes to win the election. When it regains connectivity, this response // with \u0026#34;pb.MsgAppResp\u0026#34; of higher term would force leader to step down. // However, this disruption is inevitable to free this stuck node with // fresh election. This can be prevented with Pre-Vote phase. r.send(pb.Message{To: m.From, Type: pb.MsgAppResp}) } else if m.Type == pb.MsgPreVote { // Before Pre-Vote enable, there may have candidate with higher term, // but less log. After update to Pre-Vote, the cluster may deadlock if // we drop messages with a lower term. //在更复杂的情况中，比如，在变更配置时，开启了原本没有开启的Pre-Vote机制。 // 此时可能会出现与上一条类似的情况，即可能因Term更高但是Log更旧的节点的存在导致整个集群的死锁， // 所有节点都无法预投票成功 // 对于Term比当前节点Term低的预投票请求，无论是否开启了Check Quorum / Leader Lease或Pre-Vote， // 都要通过一条Term为当前Term的消息，迫使其转为Follower并更新Term r.logger.Infof(\u0026#34;%x [logterm: %d, index: %d, vote: %x] rejected %s from %x [logterm: %d, index: %d] at term %d\u0026#34;, r.id, r.raftLog.lastTerm(), r.raftLog.lastIndex(), r.Vote, m.Type, m.From, m.LogTerm, m.Index, r.Term) r.send(pb.Message{To: m.From, Term: r.Term, Type: pb.MsgPreVoteResp, Reject: true}) } else { // ignore other cases r.logger.Infof(\u0026#34;%x [term: %d] ignored a %s message with lower term from %x [term: %d]\u0026#34;, r.id, r.Term, m.Type, m.From, m.Term) } return nil } 选举 最终，终于轮到进行选举了，根据是否为preVote向其中传入不同的选举类型即可\n1 2 3 4 5 6 7 switch m.Type { case pb.MsgHup: if r.preVote { r.hup(campaignPreElection) } else { r.hup(campaignElection) } 投票 在投票上进行安全性校验：\n如果当前Term之前对该节点投过票，允许投票 如果当前Term未投过票，并且自身也没有Leader，允许投票 接收到一个Term高于自己的PreVote，允许投票 此外还需要校验自身的Log，防止Log覆盖。\n否则，投出拒绝票。\n1 2 3 4 5 6 7 8 9 10 11 case pb.MsgVote, pb.MsgPreVote: // We can vote if this is a repeat of a vote we\u0026#39;ve already cast... canVote := r.Vote == m.From || // ...we haven\u0026#39;t voted and we don\u0026#39;t think there\u0026#39;s a leader yet in this term... (r.Vote == None \u0026amp;\u0026amp; r.lead == None) || // ...or this is a PreVote for a future term... (m.Type == pb.MsgPreVote \u0026amp;\u0026amp; m.Term \u0026gt; r.Term) // ...and we believe the candidate is up to date. // raft当中通过日志Term进行的最基础的安全性校验 if canVote \u0026amp;\u0026amp; r.raftLog.isUpToDate(m.Index, m.LogTerm) { // Note: it turns out that that learners must be allowed 1 2 3 func (l *raftLog) isUpToDate(lasti, term uint64) bool { return term \u0026gt; l.lastTerm() || (term == l.lastTerm() \u0026amp;\u0026amp; lasti \u0026gt;= l.lastIndex()) } 角色相关step step 当所有的预处理均处理完之后，如果还有后续工作，就交给stepxxx来完成，分别为stepLeader、stepFollower、stepCandidate\n1 2 3 4 5 6 type stepFunc func(r *raft, m pb.Message) error type raft struct { // ... step stepFunc // ... } 1 2 3 4 5 default: err := r.step(r, m) if err != nil { return err } stepLeader stepLeader当中涉及到的逻辑较为复杂，包含了选举、日志复制、快照处理等多方面，这里先仅分析选举相关的内容。\n首先如果Step传来的消息为MsgCheckQuorum​类型，那么就执行Check Quorum操作，检查当前集群当中活跃的节点数量，如果活跃数量达不到Quorum的要求，当前的Leader就退位成为Follower。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 case pb.MsgCheckQuorum: // The leader should always see itself as active. As a precaution, handle // the case in which the leader isn\u0026#39;t in the configuration any more (for // example if it just removed itself). // // TODO(tbg): I added a TODO in removeNode, it doesn\u0026#39;t seem that the // leader steps down when removing itself. I might be missing something. if pr := r.prs.Progress[r.id]; pr != nil { pr.RecentActive = true } if !r.prs.QuorumActive() { r.logger.Warningf(\u0026#34;%x stepped down to follower since quorum is not active\u0026#34;, r.id) r.becomeFollower(r.Term, None) } // Mark everyone (but ourselves) as inactive in preparation for the next // CheckQuorum. r.prs.Visit(func(id uint64, pr *tracker.Progress) { if id != r.id { pr.RecentActive = false } }) return nil stepCandidate 在stepCandidate当中，如果接收到了Leader发送来的消息，如MsgApp​ MsgHeartbeat​ MsgSnap​，那么就放弃选举，自动转换为follower，而如果接收到了MsgProp​，propose消息只有Leader才能够处理，因此返回一个Err，拒绝处理。\nstepCandidate当中的选举的相关逻辑主要为处理选举的结果，同样通过poll函数进行检查，但是此时已经有其他的节点通过票了，会记录在Tracker.progress当中，如果能够拿到足够多的选票，在根据当前选举的状态判断，如果是预选举，那么就开启正式选举，而如果是正式选举，那么就成为Leader，并且上线之后立刻广播一次AppendEntries。\n‍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 // stepCandidate is shared by StateCandidate and StatePreCandidate; the difference is // whether they respond to MsgVoteResp or MsgPreVoteResp. func stepCandidate(r *raft, m pb.Message) error { // Only handle vote responses corresponding to our candidacy (while in // StateCandidate, we may get stale MsgPreVoteResp messages in this term from // our pre-candidate state). var myVoteRespType pb.MessageType if r.state == StatePreCandidate { myVoteRespType = pb.MsgPreVoteResp } else { myVoteRespType = pb.MsgVoteResp } switch m.Type { case pb.MsgProp: r.logger.Infof(\u0026#34;%x no leader at term %d; dropping proposal\u0026#34;, r.id, r.Term) return ErrProposalDropped case pb.MsgApp: r.becomeFollower(m.Term, m.From) // always m.Term == r.Term r.handleAppendEntries(m) case pb.MsgHeartbeat: r.becomeFollower(m.Term, m.From) // always m.Term == r.Term r.handleHeartbeat(m) case pb.MsgSnap: r.becomeFollower(m.Term, m.From) // always m.Term == r.Term r.handleSnapshot(m) case myVoteRespType: gr, rj, res := r.poll(m.From, m.Type, !m.Reject) r.logger.Infof(\u0026#34;%x has received %d %s votes and %d vote rejections\u0026#34;, r.id, gr, m.Type, rj) switch res { case quorum.VoteWon: if r.state == StatePreCandidate { r.campaign(campaignElection) } else { r.becomeLeader() r.bcastAppend() } case quorum.VoteLost: // pb.MsgPreVoteResp contains future term of pre-candidate // m.Term \u0026gt; r.Term; reuse r.Term r.becomeFollower(r.Term, None) } } return nil } stepFollower follower当中和选举相关的内容不是很多，只有一条超时进行选举，其他的都是对Leader状态的跟随，即处理AppendEntries、Heartbeat、Snapshot。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 func stepFollower(r *raft, m pb.Message) error { switch m.Type { case pb.MsgProp: if r.lead == None { r.logger.Infof(\u0026#34;%x no leader at term %d; dropping proposal\u0026#34;, r.id, r.Term) return ErrProposalDropped } else if r.disableProposalForwarding { r.logger.Infof(\u0026#34;%x not forwarding to leader %x at term %d; dropping proposal\u0026#34;, r.id, r.lead, r.Term) return ErrProposalDropped } m.To = r.lead r.send(m) case pb.MsgApp: r.electionElapsed = 0 r.lead = m.From r.handleAppendEntries(m) case pb.MsgHeartbeat: r.electionElapsed = 0 r.lead = m.From r.handleHeartbeat(m) case pb.MsgSnap: r.electionElapsed = 0 r.lead = m.From r.handleSnapshot(m) case pb.MsgTransferLeader: if r.lead == None { r.logger.Infof(\u0026#34;%x no leader at term %d; dropping leader transfer msg\u0026#34;, r.id, r.Term) return nil } m.To = r.lead r.send(m) case pb.MsgTimeoutNow: r.logger.Infof(\u0026#34;%x [term %d] received MsgTimeoutNow from %x and starts an election to get leadership.\u0026#34;, r.id, r.Term, m.From) // Leadership transfers never use pre-vote even if r.preVote is true; we // know we are not recovering from a partition so there is no need for the // extra round trip. r.hup(campaignTransfer) case pb.MsgReadIndex: if r.lead == None { r.logger.Infof(\u0026#34;%x no leader at term %d; dropping index reading msg\u0026#34;, r.id, r.Term) return nil } m.To = r.lead r.send(m) case pb.MsgReadIndexResp: if len(m.Entries) != 1 { r.logger.Errorf(\u0026#34;%x invalid format of MsgReadIndexResp from %x, entries count: %d\u0026#34;, r.id, m.From, len(m.Entries)) return nil } r.readStates = append(r.readStates, ReadState{Index: m.Index, RequestCtx: m.Entries[0].Data}) } return nil } 状态切换 当进行状态切换时， 均会调用一个reset​函数，对自身的状态进行重置：\n将自己的term设置为预期的Term 清除自身的Leader 重置定时器 清除选票 遍历tracker.Progress重置状态 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func (r *raft) reset(term uint64) { if r.Term != term { r.Term = term r.Vote = None } r.lead = None r.electionElapsed = 0 r.heartbeatElapsed = 0 r.resetRandomizedElectionTimeout() r.abortLeaderTransfer() r.prs.ResetVotes() r.prs.Visit(func(id uint64, pr *tracker.Progress) { *pr = tracker.Progress{ Match: 0, Next: r.raftLog.lastIndex() + 1, Inflights: tracker.NewInflights(r.prs.MaxInflight), IsLearner: pr.IsLearner, } if id == r.id { pr.Match = r.raftLog.lastIndex() } }) r.pendingConfIndex = 0 r.uncommittedSize = 0 r.readOnly = newReadOnly(r.readOnly.option) } follower 1 2 3 4 5 6 7 8 func (r *raft) becomeFollower(term uint64, lead uint64) { r.step = stepFollower r.reset(term) r.tick = r.tickElection r.lead = lead r.state = StateFollower r.logger.Infof(\u0026#34;%x became follower at term %d\u0026#34;, r.id, r.Term) } 字段 作用 step 角色对应的step，在上面已经分析过 tick 角色对应的tick函数，对于follower为tickElection，用于进行超时选举 lead 当前的Leader state 角色状态 candidate 1 2 3 4 5 6 7 8 9 10 11 12 func (r *raft) becomeCandidate() { // TODO(xiangli) remove the panic when the raft implementation is stable if r.state == StateLeader { panic(\u0026#34;invalid transition [leader -\u0026gt; candidate]\u0026#34;) } r.step = stepCandidate r.reset(r.Term + 1) r.tick = r.tickElection r.Vote = r.id r.state = StateCandidate r.logger.Infof(\u0026#34;%x became candidate at term %d\u0026#34;, r.id, r.Term) } 字段 作用 step 角色对应的step，在上面已经分析过 tick 角色对应的tick函数，对于candidate为tickElection，用于进行超时选举 Vote 当前Term下投出的票，candidate会为自己投票 state 角色状态 PreCandidate 对于PreCVandidate，严格意义上其并非状态切换，即本质上还是为follower，只不过为了对外发起PreVote设置了一个单独的StatePreCandidate的状态，因此在becomePreCandidate当中不会调用reset进行状态重置，只有当其确认会进行正式投票时，才会转换为Candidate\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (r *raft) becomePreCandidate() { // TODO(xiangli) remove the panic when the raft implementation is stable if r.state == StateLeader { panic(\u0026#34;invalid transition [leader -\u0026gt; pre-candidate]\u0026#34;) } // Becoming a pre-candidate changes our step functions and state, // but doesn\u0026#39;t change anything else. In particular it does not increase // r.Term or change r.Vote. r.step = stepCandidate r.prs.ResetVotes() r.tick = r.tickElection r.lead = None r.state = StatePreCandidate r.logger.Infof(\u0026#34;%x became pre-candidate at term %d\u0026#34;, r.id, r.Term) } 字段 作用 step 角色对应的step，和Candidate公用相同的逻辑 tick 角色对应的tick函数，对于candidate为tickElection，用于进行超时选举 lead 进行预投票时即为认定当前集群当中无Leader，因此设置为None Vote 当前Term下投出的票，candidate会为自己投票 state 角色状态 Leader 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 func (r *raft) becomeLeader() { // TODO(xiangli) remove the panic when the raft implementation is stable if r.state == StateFollower { panic(\u0026#34;invalid transition [follower -\u0026gt; leader]\u0026#34;) } r.step = stepLeader r.reset(r.Term) r.tick = r.tickHeartbeat r.lead = r.id r.state = StateLeader // Followers enter replicate mode when they\u0026#39;ve been successfully probed // (perhaps after having received a snapshot as a result). The leader is // trivially in this state. Note that r.reset() has initialized this // progress with the last index already. r.prs.Progress[r.id].BecomeReplicate() // Conservatively set the pendingConfIndex to the last index in the // log. There may or may not be a pending config change, but it\u0026#39;s // safe to delay any future proposals until we commit all our // pending log entries, and scanning the entire tail of the log // could be expensive. r.pendingConfIndex = r.raftLog.lastIndex() emptyEnt := pb.Entry{Data: nil} if !r.appendEntry(emptyEnt) { // This won\u0026#39;t happen because we just called reset() above. r.logger.Panic(\u0026#34;empty entry was dropped\u0026#34;) } // As a special case, don\u0026#39;t count the initial empty entry towards the // uncommitted log quota. This is because we want to preserve the // behavior of allowing one entry larger than quota if the current // usage is zero. r.reduceUncommittedSize([]pb.Entry{emptyEnt}) r.logger.Infof(\u0026#34;%x became leader at term %d\u0026#34;, r.id, r.Term) } 字段 作用 step 角色对应的step，stepLeader tick 角色对应的tick函数，对于Leader为tickHeartbeat，发送心跳信息 lead lead设置为自身 state 角色状态 ‍\n","permalink":"http://itfischer.space/en/posts/tech/etcd/raft%E9%80%89%E4%B8%BE%E6%B5%81%E7%A8%8B/","summary":"Raft选举流程 引用：http://blog.mrcroxx.com/posts/code-reading/etcdraft-made-si","title":"etcd/raft选举流程"},{"content":"日志与恢复 数据库当中存在哪些故障？ 简单来说可以分为三种故障：\n事务故障 系统故障 存储介质故障 而事务故障也可分为两种：\n逻辑错误 (Logical Errors)：由于一些内部约束，如数据一致性约束，导致事务无法正常完成 内部错误 (Internal State Errors)：由于数据库内部调度、并发控制，如死锁，导致事务无法正常提交 系统故障可以分为两种：\n软件故障 (Software Failure)：如 DBMS 本身的实现问题 (NPE, Divide-by-zero) 硬件故障 (Hardware Failure)：DBMS 所在的宿主机发生崩溃，如断电。且一般假设非易失性的存储数据在宿主机崩溃后不会丢失 如果存储介质发生故障，通常这样的故障就是无法修复的，如发生撞击导致磁盘部分或全部受损，磁盘上的磁性介质被刮伤。在 L1 - RAID​ 就有介绍过这种情况，通常每个磁盘都会有对应的备份数据、备份磁盘，那么就保证了数据不会丢失。我们需要从备份记录中恢复数据\n对于故障数据库的解决方案？ 首先对于存储介质的故障，在数据库层面无能为力，只能通过硬件层面的备份与恢复进行解决。而对于上层的事务故障和系统故障，可以db层面尽可能的进行解决。\n对于支持事务的数据库系统，在故障恢复上就要考虑合理编排或者恢复数据，以保证事务的特性不被违反。如不能出现不一致的中间态，或者说违反了原子性，存在部分指令成功执行而部分指令未执行的情况。\n故障恢复机制包括两部分：\n在事务执行过程中采取的行动来确保在出现故障时能够恢复 在故障发生后的恢复机制，如确保原子性、一致性和持久性 对于两方面总结的话，就是在事务执行过程中采用shadow page或者WAL的方式，再加上合适的 Buffer Pool Policy，维护相关数据，之后在发生故障时，对事务进行回滚或者重做\nBuffer Pool Policies 修改数据时，DBMS 需要先把对应的数据页从持久化存储中读到内存中，然后在内存中根据写请求修改数据，最后将修改后的数据写回到持久化存储。在整个过程中，DBMS 需要保证两点：\n1DBMS 告知用户事务已经提交成功前，相应的数据必须已经持久化 2.如果事务中止，任何数据修改都不应该持久化 如果真的遇上事务故障或者系统故障，DBMS 有两种基本思路来恢复数据一致性，向用户提供上述两方面保证：\nUndo：将中止或未完成的事务中已经执行的操作回退 Redo：将提交的事务执行的操作重做 DBMS 如何支持 undo/redo 取决于它如何管理 buffer pool。我们可以从两个角度来分析 buffer pool 的管理策略：Steal Policy 和 Force Policy\nSTEAL：\nsteal所决定的是DBMS是否允许一个未提交的事务是否修改持久化存储当中的数据，如果允许，则为STEAL，如果不允许，则为NO-STEAL\n如果允许(Steal)，当 T1 回滚时，需要把已经持久化的数据读进来，回滚数据，再保存回去。但是在不发生回滚时，DBMS 的 I/O 较低。\n如果不允许(No-steal)，当 T1 回滚时，由于所有数据都只保存在内存中没有进入磁盘中，所以只需要丢弃这些内存中的 page 即可。但是假如 T1 是一个长事务，内存可能无法完全容纳进 T1 的所有 page，就需要将中间数据存储在额外的磁盘空间中，那么 DBMS 会带来额外的空间浪费。\nFORCE：\nforce所决定DBMS是否要求在一个事务允许commit之前，该事务所作的所有更改应当被映射到数据库当中，即进行脏页回写，如果要求，则为FORCE，反之则为NO-FORCE\nforce所要求的是将该事务本身做出的更改映射到数据库当中，而对于T1所做的更改，应当将其剔除或避开，采用的解决方案为生成一个副本，在其中保存只由T2做出的更改，之后在commit时将这个副本映射到数据库当中，就只会将T2做出的更改映射到磁盘当中。\n如果强制(Force)，每次事务提交之后，都必须要保证数据落盘，保证数据一致性。\n如果不强制(No-Force)，DBMS 则可以延迟批量地将数据落盘，数据一致性可能存在问题，但 I/O 效率较高。\n最终的buffer pool 策略为STEAL指标和FORCE指标的组合。\nsteal no-steal force Steal + Force No-Steal + Force no-force Steal + No-Force No-Steal + No-Force 通常实践中应用的为No-Steal + Force与Steal + No-Force\nShadow Page No-Steal + Force 在使用No-Steal + Force的策略组合下：\n当事务中止时，无需进行回滚，因为事务作出的修改不会主动进行持久化，也不允许被其他的事务捎带落盘 当事务提交时，也无需进行重做，因为提交的数据已经全部进行持久化 最简单的实现方式为Shadow Page，其会维护两份数据：\nmaster：包含所有已经提交的事务的数据 shadow：在Master之上增加为提交的数据变动 最开始，在内存当中维护一个Master的Page Table和指向该Table的DB Root，当有事务要进行写操作时，在内存当汇总给你创建一个Shadow Page Table，全部指向硬盘当中的Master，之后硬盘上也从Master 复制出来一个Shadow Page，并且令硬盘当中的对应的page table改变指向。\n之后，所有的写操作都在Shadow Page上进行。在事务提交前，DB Root依旧指向Master Page Table，所有的Shadow Page 对其他的事务都不可见。\n在事务提交时，则改变DB Root的指向，令其指向Shadow page Table。从而该事务作出的更改对于其他的事务全部可见\n在 shadow paging​ 下回滚、恢复数据都很容易：\nUndo/Rollback：删除 shadow pages (table)​，啥都不用做 Redo：不需要 redo​，因为每次写事务都会将数据落盘 Shadow Page的缺陷？ 复制整个 page table 代价较大，尽管可以通过以下措施来降低这种代价：\n需要使用类似 B+ 树的数据结构来组织 page table​ 无需复制整个树状架构，只需要复制到达有变动的叶子节点的路径即可 事务提交的代价较大：\n需要将所有发生更新的 data page、page table 以及根节点都同时落盘。 容易产生磁盘碎片，使得原先距离近的数据渐行渐远，IO 就会变成磁盘 IO ，拖缓速度。 需要做垃圾收集 只支持一个写事务或一批写事务一次性持久化，需要基于不同的并发策略分析。 Write-Ahead Log Steal + No-Force WAL基于一个假设，每个修改操作都有一条对应的日志文件。\n设立一个单独的日志文件，要求在进行持久化刷盘之前，必须要证对应的日志文件首先被持久化道磁盘当中，从字面意思上体现了write-ahead log 。当日志完成写入之后，即可认为此次事务已经提交，可以给予响应。\n整个过程如下：\n首先将事务进行的操作放在内存当中，即为redo buffer 在对内存中的数据进行修改(data page对应的buffer pool中的数据） 如果该data page要刷盘，需要把该事务对应的所有的日志强制全部刷盘 当一个事务进行提交时，无需将data page进行刷盘，只需要保证data page对应的日志全部刷盘，只要日志全部刷盘完成，则可以对外宣布此次事务成功commit 一个事务终止时，需要将其的所有日志全部刷盘，包括abort记录 WAL当中的日志格式？ 每个事务开始时，先日志上写上一个\u0026lt;BEGIN\u0026gt;​标签\n事务commit时，日志上写上一个\u0026lt;COMMIT\u0026gt;​​标签\n日志上的每一条记录包含：\nTransaction Id (事务 id) Object Id (数据记录 id)：filename​​ 和 pageid/blocknum​​ Before Value (修改前的值)，用于 undo 操作 After Value (修改后的值)，用于 redo 操作 Log Sequence Numbers\nDBMS为每条日志生成的全局唯一的序列号，一般LSN单调递增，并且为了进行crash recovery，需要在各个部分进行记录相关信息\n每个page保存着一个pageLSN，DBMS本身维护者flushedLSN，表明上一次落盘的日志的LSN，而在一个$page_x$落盘之前，首先应当保证 $pageLSN_x \u0026lt;= flushedLSN$，这意味着在该page落盘时，该page的所有的日志都已经落盘。来保证 WAL的思想\n存储和管理的位置如图：\nLogging Scheme 就像SQL执行器那样可以分为物理计划和逻辑计划，在Logging Scheme(日志方案？)上同样可以分为物理和逻辑两种方案\nPhysical Logging：记录在数据库中的真实记录，在哪个page的什么位置的进行了修改 Logical Logging：逻辑日志只记录做的逻辑操作，如更新所有age=20的tuple，记录内容较少，但是相对的难以弄清楚具体做了什么变化，恢复的时候代价更为昂贵 还有一种混合策略，称为 physiological logging​，这种方案不会像 physical logging​ 一样记录 xx page xx 偏移量上的数据发生 xx 改动，而是记录 xx page 上的 id 为 xx 的数据发生 xx 改动，前者需要关心 data page 在磁盘上的布局，后者则无需关心。physiological logging​ 也是当下最流行的方案。\nWAL如何解决了Shadow Page的缺陷 在Shadow Page策略当中，通过Page Table访问Shadow Page访问属于随机 IO ，会影响事务提交时的效率，影响并发事务的数量。而顺序写入永远比随机写入快，Write-Ahead Log即践行了这种思想，就好比那个粉板的例子，顺序写粉板永远必随机写到账本上的速度要快。\n此外，WAL也不涉及到像是Page复制等操作，只需要维护一个日志文件，之后一致向其中追加写入即可。\n说一说WAL当中的Checkpoint WAL会一直进行记录，如果发生崩溃想要进行redo来恢复数据库状态，那么就要执行整个日志文件来进行恢复，耗时较长。\n当缓存中的数据（data page）已经持久化到硬盘当中，此后便无需再担心这些数据的持久化问题，便可以打上一个checkpoint，通过checkpoint的方式，就像游戏中的检查点，后续的恢复只需要从checkpoint的位置开始恢复即可，之前的日志文件无需在意，甚至可以直接删除。\n通过\u0026lt;CHECKPOINT\u0026gt;​标签来表明在此处打上了一个checkpoint\nCheckpoint有什么相关优化？ 目前描述的CheckPoint为 Non-Fuzzy Checkpoint，创建时：\n停止创建任何的新事务 等待目前所有的活跃(active)事务执行完毕 将所有的脏页进行刷盘 由此可见，Non-Fuzzy Checkpoint的创建会严重影响DBMS的性能，因此在此基础上可以进行改进：\nSlighly Better Checkpoint​\n在进行checkpoint创建后，根据锁的暂停写事务，写事务可以向checkpiont开始前就已经加锁的page进行继续写入，但是不能获取新的锁，此时系统当中由于已经存在已经开始但被暂停的写事务，因此在之后创建checkpointj扫描整个buffer pool刷盘时，会将脏页也顺带写入磁盘当中，可能会出现不一致的问题，因此需要额外记录：\nActive Transaction Table（ATT）：记录当前所有的活跃事务 Dirty Page Table（DPT）：记录目前buffer pool中的脏页 ATT当中记录着事务id、事务状态以及lastLSN，当之后事务中止或者提交时，响应的记录才会被删除\n‍\nFuzzy Checkpoint fuzzy的意思为checkpoint的创建为一个动态的模糊的过程，并不是在某一个事务的之前之后进行的创建，而是在事务的执行过程中进行创建。\nfuzzy checkpoint的创建不中断任何一个事务的执行，在此期间允许事务继续执行和脏页的刷盘\n由于创建为一个时间段，并且允许事务执行，因此需要标记开始的和结束的状态：\nCHECKPOINT-BEGIN：表明开始创建检查点 CHECKPOINT-END：结束创建，包含ATT+DPT 当checkpoint创建成功后，checkpoint的LSN被写入到 DB的MasterRecord中\n任何一个在CHECKPOINT-BEGIN之后开始的事务都不被记录到CHECKPOINT-END中的ATT当中，记录在CHECKPOINT-BEGIN之前开始但是还未执行完的事务\nARIES Database Recovery Algorithms for Recovery and Isoation Exploiting Semantics\n对于ARIES，其基础是日志策略必须使用WAL，即：\n任何更改在写入到数据库之前都要先记录在日志当中并且持久化到硬盘之上 buffer pool的管理策略必须使用 STEAL+NO-FORCE 并且通过redo 和 undo 来完成崩溃后的恢复：\n在crash后还是进行恢复时，最开始先重新执行日志中记录的操作，将数据库的状态恢复到崩溃之前 在redo执行完之后，再进行undo操作，撤销未完成的事务，并且将undo操作记录到日志当中，来确保重复失败时不会重复操作 事务提交时的日志行为？ 当事务提交时，首先写一条COMMIT记录当WAL当中，之后将COMMIT以及之前的日志进行落盘，之后将flushedLSN进行更新，之后即可清除掉内存当中COMMIT以及之前的日志。之后再写入一条TXN-END记录到日志当中，表示事务真正结束\n事务中止时的日志行为？ 要进行事务中止，就要在当前的状态的基础之上，根据先前写入的WAL日志，进行一条条的回退进行undo，恢复到事务开始时的状态。\n因此在原本日志的基础上，需要引入一个额外的数据preLSN，表明当前事务日志的前一条日志，用于形成一个链表的形式将整个事务的相关日志串联起来，便于回退执行。\n为了防止在回滚过程中再次故障导致部分操作被执行多次，回滚操作也需要写入日志中，等待所有操作回滚完毕后，DBMS 再往 WAL 中写入 TXN-END 记录，意味着所有与这个事务有关的日志都已经写完，不会再出现相关信息\n因此引入了CLR，CLR作为一种操作，与COMMIT UPDATE等一样需要通过日志进行记录，CLR当中记录undo的具体操作，并且通过一个undoNext指针，指向下一条进行undo的日志。\n永远不会对CLR本身进行undo\n完整过程为：\n首先在日志前添加一个ABORT记录，表示开始进行中止操作 找到日志当中该事务的最后一条日志，根据preLSN逆序进行操作 对于之前的每个UPDATE RECORD，添加一条CLR，之后将数据库当中的值恢复为原本的值 最终回滚结束时，在日志的末尾添加一条TXN-END，表明回滚结束 ‍\nARIES -Recovery Phases Crash Recovery主要分为三个过程：\nAnalysis：通过日志解析出在crash之前的数据库状态，找到上一个检查点 通过ATT和DPT获取崩溃时的数据库的状态，找到当时的脏页和活跃事务的情况，来确定那些需要redo，那些需要undo Redo：从一个合适的点开始重复执行日志中记录的所有操作，即便该操作对应的事务在后续被abort Undo：对于crash前的所有未commit的事务（包括没来得及进行commit，执行了一半的事务和最终需要abort的事务）反向执行，撤销其进行的操作和对磁盘的影响 Analysis phase 从最近的begin checkpoint进行扫描，如果扫描到了一个TXN-END​那么就把对应的事务移除ATT，而对于其他类型的记录：\n就将其加入到ATT当中，并设置status为UNDO 如果扫描到了一个COMMIT​就将事务的状态改为COMMIT​ 对于UPDATE​ records，如果更新的page p不在 DPT当中，那么就将其加入到DPT当中，并且设置：recLSN = LSN​（表明这是第一次将该page加载到buffer pool当中并且进行更改） 当执行完Analysis过程，构建出ATT和DPT之后：\nATT表明在crash时有哪些事务处于活跃状态 DPT表明当时有哪些脏页还没来得及落盘 Redo Phase 重建起crash时db的状态，重新执行所有的update和CLRs\n从 DPT 中找到最小的 recLSN，从那开始向后 redo​ 日志和 CLR日志，除了一下的两种情况，其他均应当进行redo操作：\n如果不在DPT中，则证明之前在某时间已经将其刷盘持久化到硬盘当中 如果记录的LSN小于 page的recLSN，则证明，在此次操作将其刷盘之后，又有其他的事务读取了该页到内存当中进行了重新的修改，因此此次操作被后续的操作覆盖，因此可以忽略（参考 recLSN的定义） 最小的recLSN表明为数据库中最早的脏页状态，再此之前的日志均为产生脏页，因此不需要进行重新构建。\nredo时，需要：\n重新执行日志中的操作 将 pageLSN 修改成日志记录的 LSN 不再新增操作日志，也不强制刷盘 在 Redo Phase 结束时，会为所有状态为 COMMIT 的事务写入 TXN-END 日志，同时将它们从 ATT 中移除。\nUndo Phase 将所有 Analysis Phase 判定为 U (candidate for undo) 状态的事务的所有操作按执行顺序倒序 Undo​，并且为每个 undo 操作写一条 CLR 更新到日志文件中。\n","permalink":"http://itfischer.space/en/posts/tech/%E6%97%A5%E5%BF%97%E4%B8%8E%E6%81%A2%E5%A4%8D/","summary":"日志与恢复 数据库当中存在哪些故障？ 简单来说可以分为三种故障： 事务故障 系统故障 存储介质故障 而事务故障也可分为两种： 逻辑错误 (Logical Errors)：由","title":"日志与恢复"},{"content":"事务与并发控制 简单介绍一下事务？ 事务作为数据库执行过程中的一个逻辑单位，可以表明一组操作，这一组操作要么一起成功，要么全部回滚，并且事务之间一定程度上相互隔离，互不影响。事务具有事务的四大特性 ACID，分别为原子性 一致性 隔离性与持久性\n介绍一下事务的四大特性 事务的四大特性分别为原子性、一致性、隔离性与持久性\n对于原子性，则事务本身作为一个整体，要么全部成功执行，要么全部失败回滚，不存在执行成功一半的中间态。\n一致性表明数据库在事务执行前和执行后，数据库的状态必须满足预定的一致性规则，即数据库只能从一种状态转换为另外一种一致性状态，最简单的例子即为银行转账，只有转账前与转账后的两种一致性状态，不存在一人转出而另一人未收到的中间状态。此外一致性分为数据库一致性和事务一致性，数据库只保证数据库一致性，就像上面所说的那样，而事务一致性则更是一种逻辑关系，需要操作人员去手动约束\n此外 数据库的一致性应当与分布式系统当中的一致性的概念相区分，分布式当中的一致性指的是在多个节点上数据保持一致的状态，比如其中最严格的线性一致性就要求对外需要表现成单机节点，如果一次写入完成那么后续就一定要能够读取到数据\n隔离型：并发执行的多个事务之间应当隔离，使每个事务感觉在独立的操作数据库，防止事务之间的相互干扰与数据污染\n持久性：持久性表明一旦事务提交，那么其做出的更改应当为永久性的\n说一下事务的隔离级别 四种隔离级别的差别主要聚焦于读操作上，对于写操作，为了保证数据库一致性，只有写入的锁只有数据提交时才能够释放，如果中途释放锁，写入的操作结果可能会被其他的覆盖，从而违反了数据库一致性。单独看写锁的话，都是采用了严格两阶段锁的策略，保证一致性，同时避免级联中止。\nREAD UNCOMMITTED\n最宽松的隔离级别，根据上表可以看到脏读 不可重复读 幻读都会发生\n在实现上读操作不需要加锁，写操作和其他的一致，因此所有读相关的锁都不支持，如果想尝试加[S,IS,SIX]，都会抛出异常。由于只支持写锁，在shrinking阶段，任何加锁操作都是不允许的，会抛出异常。\n因此并不保证读取到的数据一定是提交的数据，由于并没有加读锁和其他的事务进行互斥，其他的事务一旦写入之后就可以被读取，因此当读取到之后，如果写入数据的事务中止回滚，那么读取到的数据就变成了脏数据产生脏读。\nREAD COMMITTED\n相比于读未提交解决了脏读的问题，但是依旧会出现不可重复读和幻读\n在实现上读操作需要在Table上加IS，在Row上加S，并且当完成一次读取之后即可将锁释放，无需等待事务的提交。释放S锁并不会导致事务从growing向shrinking转变。\n由于通过S锁进行互斥，从而保证那些正在写入的事务在提交前不应被读取到，能够读取到的数据一定是提交之后稳定写入的数据。\nREPEATABLE READ\n在读提交的基础上又解决了不可重复读的问题，存在幻读。\n实现上加锁策略和READ COMMITTED并没有区别，但是在释放锁上读写锁全部采用严格两阶段锁的策略，此次事务会一直持有读锁，从而中途不可能被其他的事务重新写入更新，在一次事务当中读取的数据全部为一致的。\nshrinking阶段不允许添加任何的锁，释放S X锁就会向shrinking进行转变。\n可串行化\n最严格的隔离级别，简单来说就是在该隔离级别下，事务的执行可以等价成一次不存在并发的串行执行。最简单的判断是否为可串行化的方法就是交换一系列不冲突的指令，从而看原本的事务调度是否可以被分为两个在时间上完全错开的串行调度，简单用读写来表示，一次通过交换或重拍转换成串行结果如下：\n在15-445当中没有实现该隔离级别，不过在6.830当中实现过，就简单说一下6.830的实现方式\n其相比于REPEATABLE READ，避免的幻读的发生，最简单的实现手段是加表级别的锁，从而保证在读的时候没有其他的事务能够插入新的数据，这样虽然可以保证各个方面的安全，脏读 不可重复读 幻读全部解决，但是带来的问题就是并发度很低，如果一张表上存在写入的事务，那么就没有并发度了。\n如何判断事务之间为可串行化 最简单的方式即为像上面那样去交换两条不冲突的指令，如果能够通过交换的形式把两个事务的指令在时间顺序上完全隔开，那么就是满足可串行化的。\n此外还有一种方法是，对于存在冲突的两个指令，建立一条边，从时间较早的指向时间较晚的，如果在之间出现了环，那么就是不可串行化调度的\n并发控制手段都有哪些 并发控制协议是DBMS如何决定在多事务时的正确交叉执行\n悲观锁：悲观锁：对于同一个数据的并发操作，悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java中，synchronized关键字和Lock的实现类都是悲观锁。而在MySQL当中，排他锁即为悲观锁 乐观锁：乐观锁认为自己在使用数据时不会有别的线程修改数据，假设并发冲突很罕见，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据（具体方法可以使用版本号机制和CAS算法）。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作：重试或者报异常。 锁(Lock)和锁存器(Latch)当中有什么区别 锁的概念更加属于数据库的抽象逻辑层面的，意在保护数据库的逻辑结构 ，如tuple或者 表等\n锁存器意在保护数据库的物理结构 如真实的数据结构等，如使用锁存器保证buffer pool的线程安全，来解决一个B+树的并发问题，所讨论的就是使用锁存器来对那个树的节点来进行加锁保护。对于锁存器来说，线程为其的竞争或者持有者，而锁则是以事务为单位\n介绍一下两阶段锁协议 考虑这样一种情况，一个简单的银行转账的例子，事务T1负责完成转账，其获取锁之后修改了A的余额，释放锁，正准备获取B的锁修改B的余额时，事务T2获取到了AB的锁，读取数据，结果读取到了一种中间态，违反了事务的一致性。\n这种问题可以通过引入两阶段锁协议进行解决，即将加锁和释放锁过程划分为两个阶段，第一个阶段称为growing，只进行加锁，第二阶段称为shrinking，只进行解锁操作。\n引入两阶段锁协议之后，在看上述问题，T1修改完A之后不释放A的锁，之后尝试对B上锁，此时出现两种情况，如果T2还未对B上锁，那么顺利执行，不会违反一致性，而如果B上锁，那么就会造成死锁，需要通过死锁检测来进行解决\n两阶段锁协议会引入什么问题？ 还是以银行转账为例，T1首先完成对A和B的更新，更新后的数据为A1B1之后进入shrinking阶段，释放了AB的锁，此时事务T2在A1B1的基础上进行修改，产生副本A2B2，但是之后，事务T1由于某些问题需要中止进行回滚，其产生的数据A1B1就成了脏数据，从而在基础上进行修改的A2B2也同样无效，应当回滚，这就产生了级联中止，即一个事务的中止导致了其他事务的中止\n此外，就像上面的例子那样，两阶段锁协议同样还会引入死锁的问题。\n级联中止有什么解决方案？ 一种解决方案为将两阶段锁协议升级为严格两阶段锁协议，即growing阶段不变，而shrinking阶段改变为只有事务提交时才能够释放锁，而在上述的例子当中，事务T2就无法在T1的基础上进行修改得到A2B2，只会读取到T1提交或者中止后的结果，从而避免的级联中止\n死锁的产生条件 移步OS\n死锁的解决条件 主要有破除和预防两个方案来解决死锁问题\n破除：最简单的方式即为超时等待，在获取锁是通过一个自旋锁的方式，在自旋当中进行检查，如果超出了规定的时间就放弃获取锁，并将事务中止。\n此外，还可以通过构建等待图 + DFS的形式来进行解决，即如果一个事务在等待另外一个事务上的锁时，就添加一条边，从等待者指向资源持有者，之后通过DFS的方式进行搜索，而如果在等待图当中出现了环，那么就证明出现了死锁，此时需要挑选一个受害者(victim)，将其持有的锁释放，并且对事务进行回滚\n对于死锁的处理，可以通过锁的合理分配，来达到提前避免，首先假设时间较久的时间戳的事务拥有较高的优先级，则通常有以下两种策略：\nWait-Die（old waits for young）\n如果申请锁的比现在持有锁的拥有更高的优先级，那么申请锁的继续等待，直到持有锁的释放锁 其他情况申请的事务放弃（ aborts） Wound-Wait(young wait for old)\n如果申请锁的比现在持有锁的拥有更高的优先级，那么持有锁的放弃（aborts），然后释放锁 其他情况申请锁的等待 在死锁破除当中如何选择Victim 牺牲者的选取通常由一下几个因素进行综合考虑：\n年龄，即事务创建的时间戳 任务，比如如果一个事务需要执行大量的查询，那么最好把他当作牺牲者（最好不要把他当作牺牲者）具体是否选取取决于不同的数据库 持有的锁的数量 回滚时关联的其他的事务的多少（级联终止）强限制两阶段锁并不是必须的，有的系统采用普通的两阶段锁协议，锁定然后级联中止的处理方式 数据库当中常见的锁级别 在基础的读写锁模型上，为了提高并发度，在数据库当中通常还支持意向锁来提高并发度和便于锁的管理\n意向锁是一种不与行级锁冲突表级锁，因此可以不去检查底层的节点（tuple）是否存在冲突就去对更高一级别的节点去加锁（Table）即表明我对这张表的某些行有读取或者写入的意向\n共享意向排他锁（Shared Intention Exclusive Lock）：= S-LOCK + IX-LOCK，S-LOCK这一属性表明要去读取表的所有的记录，其他的事务就不会并发的更新任意的记录，IX-LOCK让其他事务知道我们要更新目标表的一部分记录，其他事务就不会并发的读取所有的记录，这里只会阻止对表的整体的操作，如SeqScan(表级S锁)，和全表更新(表级X锁)，对于部分更新，则在表级别上添加IX锁，不会和SIX发生冲突，一定程度提高并发度\n而在加锁时，首先需要对表级别加意向锁，如IS IX SIX，之后再对行级别加读写锁 S X，在加锁时首先需要检查在Table上是否发生冲突，如果发生冲突则直接阻塞，当获取到Table上的锁之后，再判断row是否存在冲突，而在释放锁时，行锁可以按需释放，表锁需要保证在行锁全部释放完毕后才能释放\n说一说时间戳排序并发控制 先介绍一下基本时间戳顺序协议\n首先通过时间戳来对事务进行标记，同时对于一个tuple 需要记录其 W-TS和R-TS，即最后一次对其进行读写操作的事务的时间戳，用于在一个事务要对一个tuple进行读写操作时判断是否有其他事务会和它产生冲突\n在基本时间戳顺序协议中，时间戳记录的是该事务开始时的时间，即通过BEGIN关键字声明开启事务时旧分配一个时间戳，但是DBMS本身还是并发的，因此可能会出现后续的事务在较早的事务之前执行，此时就可能存在问题，可能要考虑将较早的事务给撤销，并重新分配时间戳重新执行\n当一个事务访问到一个tuple时，发现TS \u0026lt; W-TS ，则证明未来的一次写入覆盖了目前正在读的数值，即出现了脏读问题，读取到的是旧值，需要终止\n对于读写，有以下的规则：\n读：\n如果TS($T_i$) \u0026lt; W-TS(X),则违反了时间戳顺序，读到了旧的值，该事务需要重启并分配新的时间戳\n否则：\n允许事务T去读取tuple X 更新R-TS（X）为$max(R-TS(X),TS(T_i))$ 生成一个本地副本用于该事务后续的重复读（如果再去读最新的数据则会产生两次读取不一致的不可重复读的问题） 写：\n如果$TS(T_i)\u0026lt;R-TS(X) or TS(T_i) \u0026lt; W-TS(X)$ 放弃$T_i$并重启$T_i$\n$TS(T_i)\u0026lt;R-TS(X)$会导致后续读取操作读取不到此次的写入 $TS(T_i) \u0026lt; W-TS(X)$会导致此次的写入无意义 否则：\n允许事务$T_i$去写入，并且更新W-TS(X) 生成一个本地副本用于可重复读 对于写操作，还可以通过Thomas Write Rule进行优化：\n$TS(T_i) \u0026lt;R-TS(X)$:放弃并重启$T_i$（与原本一致） $TS(T_i) \u0026lt;W-TS(X)$:忽略此次写入，并允许事务继续执行 其他情况则未违反协议，允许$T_i$写入，并更新$W-TS(X)$ 虽然本质上违反了协议，但是当前此次写后续会被覆盖掉，因此可以直接进行忽略\n总的来说，基本时间戳顺序协议的精髓为当一个事务创建时分配一个时间戳，由于事务的原子性，将事务当中所有的指令的发生时间全部集中于begin的这一时间戳上，之后如果有违反时间顺序的，事务则会中止回滚，并尝试重新开启事务，如读取时发现有未来的事务已经对其进行了写入。\n说一说乐观并发控制 基本思想也为将事务的所有指令全部视为在一个时间点上发生\nDBMS给每个事务均分配一个私有的工作空间：\n进行读操作时将读的结果copy到私有的工作空间当中 修改操作则针对于工作空间来进行修改 当一个事务准备提交时，再将工作空间和全局Database进行比较，查看是否存在冲突，这个过程称为Validation\n如果不存在冲突，则将工作空间的结果应用到全局Database当中\n阶段\nRead Phase：当事务进行读写操作时，将所有访问到的数据（tuple）读取到私有的工作空间后，再工作空间上进行操作 Validation Phase：在事务commit前验证是否和其他事务存在冲突 Write Phase：如果不存在冲突，则将私有工作空间的做的更改应用到全局数据库上，否则放弃并重启事务 在还未进行validate时，副本的时间戳设置为∞，如果进行了写操作，那么此时私有空间中W-TS即为∞\n在验证节点才会获取一个时间戳，因此将验证的时间操作视为事务真正的开始时间，A先于B进行验证，则认为A先于B开启事务\n悲观并发控制和乐观并发控制各自的应用场景 来自GPT\n悲观并发控制（Pessimistic Concurrency Control，简称 PCC）和乐观并发控制（Optimistic Concurrency Control，简称 OCC）是两种不同的并发控制策略，它们在不同的应用场景下有各自的优势。悲观并发控制采用一种悲观的思想，即认为在多用户并发环境下，数据冲突的可能性较高。因此，在读写数据时会对数据加锁，以确保数据的独占访问。悲观并发控制适用于数据竞争激烈的场景，以及发生并发冲突时使用锁保护数据的成本要低于回滚事务的成本的环境中在这种场景下，悲观锁可以有效地保护数据的一致性和完整性。\n乐观并发控制则采用一种乐观的思想，即认为在多用户并发环境下，数据冲突的可能性较低。因此，在读写数据时不会对数据加锁，而是在提交数据更新之前检查是否有其他事务修改了数据。如果发现冲突，则回滚事务并通知用户，乐观并发控制适用于数据争用不大，数据冲突较少的场景下在这种场景下，乐观锁可以提高系统的整体吞吐量，因为它避免了锁的开销。总的来说，乐观锁适用于写操作较少的情况下（多读场景），即冲突真的很少发生的时候而悲观锁适用于多写的场景，因为在这种情况下，冲突较容易发生，使用悲观锁可以避免不断的重试操作，从而提高性能。\n说一说幻读 幻读是在事务并发执行时可能出现的一种现象，它指的是在一个事务内部，多次执行相同的查询操作，但在不同的查询中返回的行数却不一致。通常是由于其他事务并发地插入或删除了符合查询条件的数据所导致的。\n为了解决幻读问题，可以采取以下几种方案：\n使用间隙锁：在某些数据库系统中，可以使用间隙锁（Gap Lock）来锁定一个范围内的空间，以阻止其他事务在该范围内插入新的数据。这可以有效地解决幻读问题。 版本控制：使用版本号或时间戳来跟踪数据的变化。在读取操作时，根据事务开始时的版本号或时间戳来确定读取的数据快照，以保证一致性。可以通过MVCC（多版本并发控制）等机制来实现。 提升隔离级别：将隔离级别提升到可串行化（Serializable）级别。可串行化级别能够解决幻读问题，但也会牺牲一定的并发性能，因为它会对并发操作进行严格的序列化。 MVCC 来自 GPT\nMVCC（Multi-Version Concurrency Control，多版本并发控制）是一种并发控制方法，主要用于数据库管理系统中实现对数据库的并发访问，在编程语言中实现事务内存MVCC通过保存数据的历史版本，根据比较数据的版本号来决定数据的是否显示，在不需要加读锁的情况就能达到事务的隔离效果，最终可以在读取数据的时候可以同时进行修改，修改数据时候可以同时读取，极大的提升了事务的并发性能。在InnoDB存储引擎中，MVCC是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存了行的过期时间（或删除时间），当然存储的并不是实际的时间值，而是系统版本号。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。MVCC的优点包括：1. 支持并发读取：多个事务可以同时读取数据库中的数据，提高了并发性能。\n避免了锁竞争：在读取过程中，不需要对记录进行加锁，因此避免了锁竞争的情况，提高了并发性能。 保证数据一致性：MVCC通过版本控制，可以保证在并发读取的情况下，数据的一致性。 然而，MVCC也存在一些局限性：1. 增加存储空间：为了实现版本控制，MVCC在每条记录上都需要添加时间戳，因此会增加存储空间。\n无法解决幻读问题：虽然MVCC通过版本控制可以避免锁竞争，但是在一些情况下，仍然会出现幻读的问题 无法实现一些锁的功能：例如 SELECT \u0026hellip; FOR UPDATE、SELECT \u0026hellip; LOCK IN SHARE MODE 等锁的功能，无法在 MVCC 中实现。 总之，MVCC是一种基于版本控制的并发控制机制，通过版本控制和时间戳比较来保证数据的一致性和并发性能，但也存在一些局限性。在数据库中，可以采用MVCC与悲观锁或乐观锁的组合方式来最大程度地提高数据库并发性能，并解决读写冲突和写写冲突导致的问题。\nMVCC与 PCC和 OCC之间有什么联系 MVCC主要解决了以下问题：1. 读写之间的阻塞问题：通过MVCC可以让读写互相不阻塞，即读不阻塞写，写不阻塞读，从而提升事务并发处理能力\n降低死锁的概率：因为InnoDB的MVCC采用了乐观锁的方式，读取数据时并不需要加锁，对于写操作，也只锁定必要的行 MVCC可以解决读-写冲突，但不能解决写-写冲突为了解决写-写冲突，可以将MVCC与其他并发控制方法结合使用，例如：\nMVCC + 悲观锁：MVCC解决读写冲突，悲观锁解决写写冲突\nMVCC + 乐观锁：MVCC解决读写冲突，乐观锁解决写写冲突\n‍\n","permalink":"http://itfischer.space/en/posts/tech/%E4%BA%8B%E5%8A%A1%E4%B8%8E%E5%B9%B6%E5%8F%91%E6%8E%A7%E5%88%B6/","summary":"事务与并发控制 简单介绍一下事务？ 事务作为数据库执行过程中的一个逻辑单位，可以表明一组操作，这一组操作要么一起成功，要么全部回滚，并且事务之间","title":"事务与并发控制"},{"content":"数据库基础结构 数据库结构 为什么需要DBMS？ 一个数据库在最基础的层次上需要完成两件事情：当你把数据交给数据库时，它应当把数据存储起来；而后当你向数据库要数据时，它应当把数据返回给你。\n因此，最简单的数据库可以使用两条bash脚本来完成，在ddia当中也给出了例子：\n1 2 3 4 5 6 7 8 #!/bin/bash db_set () { echo \u0026#34;$1,$2\u0026#34; \u0026gt;\u0026gt; database } db_get () { grep \u0026#34;^$1,\u0026#34; database | sed -e \u0026#34;s/^$1,//\u0026#34; | tail -n 1 } 存储时，则对其进行追加写入，将新数据添加到文件的末尾，因此对于重复的key，永远在末尾为最新的数据，因此在读取时，则读取指定key的最后一个数据作为结果。\n不妨思考一下这个最简单的数据库有什么又存在什么问题，可以得到最终的答案\n首先该数据库采用的为基于日志结构的，可以提供持久化存储，作为数据库的最基本功能\n之后由于所有数据全部存储于硬盘之上，每次读取都涉及到较高的磁盘IO，因此需要通过buffer pool manager来将数据存储于内存当中进行缓存以提高性能 每次读取都需要遍历所有数据，可以通过构建索引进行优化，如bitcask在内存当中构建一个hashmap或者构建B+树 在末尾追加会在磁盘当中产生大量冗余，因此需要优化存储，最简单的方式为采用bitcask进行定期压缩，或者直接采用LSM-Tree结构进行存储。以上两种为采用基于日志的结构的，此外，还可以采用基于Page的HeapPage进行存储 对于某些业务，需要提供事务的支持，即事务的ACID 为防止数据丢失并且优化写入速度，即将随机写入转换为顺序写入，引入日志系统 对多线程的访问提供支持，相关数据结构需要保证线程安全，并且对于事务也需要提供Lock Manager的支持 以上为KV类型的，如果需要实现关系型数据库，那么则需要实现关系模型，和SQL的执行引擎和优化器 为什么需要Buffer Pool？ 说明了数据从硬盘到内存当中最后为CPU所用的过程，而操作系统作为之上的最底层，为了防止过分频繁的访问，OS本身也会对操作系统的Page进行缓存（OS的Page区分于DBMS的Page），即page Cache\n当通过系统调用read()​进行读取时，内核首先会从page cache当中去寻找是否存在缓存，如果不存在缓存，则申请页帧空间，进行IO操作，之后将其存放到page cache当中，进行缓存，之后再将其拷贝到程序的缓存当中，读取结束\n这样存在的问题是一份数据在page cache当中进行了一次缓存的同时，还在程序当中进行一次缓存，缓存两次导致CPU cache命中率不高。因此提出的内存映射机制（linux当中为 mmap）\n在使用mmap调用时，系统并不马上为其分配内存空间，而仅仅是添加一个VMA(Virtual Memory Area)到该进程中，当程序访问到目标空间时，产生缺页中断。在缺页中断中，从page cache中查找要访问的文件块，若未命中，则启动磁盘I/O从磁盘中加载到page cache，然后将文件块在page cache中的物理页映射到进程mmap地址空间。数据在内存当中只会存在一份，并且之后当程序退出或者关闭文件时，page cache当中的数据并不会清除，如果物理内存足够的情况下，会一直保存在内存当中，供之后的进行去读取。\n由此可见，OS对于数据已经提供了一定的缓存能力，但是我们仍需要在DBMS当中提供一个Buffer Pool去手动管理内存，原因如下：\nOS对于数据库当中的内容一无所知，即OS并不知道当前的那些Page对于数据库来说是较为重要，不能被淘汰的，想象这样一个场景，遍历table1从中获取到age最大的那一行数据，而在遍历读取page的过程当中，OS判定系统的物理内存不足，需要进行淘汰，但是把之前加载到内存当中的page进行淘汰。导致整个SQL执行出现问题。 使用Buffer Pool进行手动管理，可以明确pin​住某些page，在Buffer Pool进行淘汰时则不会选择其进行淘汰。 同时Buffer Pool高度自定义性，整个Buffer Pool的大小，以及单个page的大小，不再受限于OS 如何管理Buffer pool当中的Page？ OS相关的page淘汰策略都可以拿来直接使用，如FIFO、Random、LRU、RR等，此外还可以使用LRU-K的淘汰策略，\n简单介绍一下LRU-K？ LRU-K将缓冲区分为历史队列和缓冲队列，一个page首先加入到历史队列当中，在淘汰前访问了k次，那么就将其移动到缓冲队列当中。并且引入了K- distance的概念，即前K次的距离，可以简单使用逻辑时间戳来表示，在历史队列当中的K- distance为正无穷，而在缓冲队列当中的Page的K- distance即为前K次访问的距离（即时间戳的差值）在淘汰时选择K- distance最大的进行淘汰\n通过LRU-K，可以避免由于全表遍历而导致的缓存污染问题\nLRU-K的缺点？ ChatGPT：\nLRU-K（Least Recently Used K）是一种缓存淘汰策略，它根据最近使用的频率来确定要淘汰的缓存项。虽然 LRU-K 在某些情况下表现良好，但也存在一些缺点：\n内存消耗：LRU-K 需要维护额外的数据结构（如计数器或时间戳数组）来跟踪最近使用的频率。这会增加额外的内存消耗，并可能影响缓存系统的整体性能。 算法复杂性：相比于简单的 LRU 算法，LRU-K 需要更多的逻辑和计算来跟踪和更新每个缓存项的使用频率。这增加了算法的复杂性，可能导致实现和维护的困难。 参数选择：选择适当的 K 值是 LRU-K 的一个挑战。K 值的选择需要综合考虑系统的特性和工作负载。选择过小的 K 值可能导致频繁的缓存项淘汰，而选择过大的 K 值可能导致缓存命中率下降。 不适用于突发访问模式：当出现突发访问模式时，LRU-K 的性能可能下降。如果有一组热点数据在短时间内被频繁访问，而其他数据很少被访问，LRU-K 可能无法准确识别这种热点数据，并导致它们被提前淘汰。 综上所述，虽然 LRU-K 在某些场景下可以改善缓存命中率，但它也存在一些不足之处。在选择缓存淘汰策略时，需要考虑系统的特性、工作负载以及对内存消耗和算法复杂性的容忍度。\nBuffer Pool当中的Page Table有什么作用？ 此处可以对比OS的内存映射和读取进行理解，在OS当中，首先数据是真实存储于物理内存当中，将物理内存划分为多个Page大小的槽位，每个槽位可以存储一个Page，而该槽位通过PFN（Phyical frame number）来表示。\n当程序访问虚拟内存当中的数据时，虚拟内存当中的数据会属于某一个Page，用VPN（virtual page number）进行表示。\n因此在读取数据时，就需要一个从VPN到PFN到一个映射，这个工作就交给Page Table来完成，其最简单的组织方式为一个数组，当中存储所有可能的映射（有效无效都存在），而如果采用多级页表的方式，其组织形式就是一个哈希表，key为VPN，val为PFN\n在buffer pool 当中同理，首先创建一个Page*[]用于存储内存当中所有的Page，对应OS当中的物理内存，该数组的下标即为一个frame_id（对应PFN），对于上层获取数据而言，要拿到一个Page则需要通过page_id（对应VPN），此处的page table同样完成page_id -\u0026gt; frame_id的映射关系。\n数据在磁盘上有什么组织形式？ 主要分为两种组织形式，分别为基于Page的和基于日志的\nPage\n基于Page的主要方式以Page为基本数据单位，当中的数据主体为Tuple，此外还有一些元数据用于辅助读取，之后多个Page组成一个File，一个File可对应一个一张表。基于Page的最常见的为HeapFile-Page的组织形式。HeapPage承载Tuple，多个HeapPage构成一个HeapFIle。\n首先，在HeapFile-Page的情况下为乱序存储的，即按顺序插入tuple a 和b，并不保证b存储于a之前。\n因此在一个Page当中，只要能够找到空余空间，就可以将数据存储于其中，在这种规则下，HeapPage的结构为一个byte[]数组的header，用于表示该slot上是否为空余，之后则是一个Tuple[]数组，用于存储具体的tuple，在6.830当中，header[]的长度设置为8，即一个HeapPage当中可以存储256个tuple\n之后HeapPage即可组成HeapFile，对于HeapPage的管理方式，最简单的方式即为不管理，当需要一个page时就根据pageId和pageSize计算出偏移量去偏移读取，也可通过一个Page Directory进行管理，本质上为一个page_id-\u0026gt;offset的偏移量，有点类似于Bitcask\n基于日志\nBitcask\n基于日志最经典的就是Bitcask模型，硬盘上为只进行顺序写入的文件，在内存当中再维护一个哈希表记录偏移，简单的以k-v类型为例，在写入时只进行追加写入，insert和update delete合并为一个put操作，insert update只需要向其中写入最新的数据，并更新索引，delete操作则写入一个墓碑标志，用于进行压缩，并且删除索引。在读取时只需要根据索引进行读取即可。\n由于一直顺序写入会导致当中存储很多陈旧数据，此时就涉及到一个压缩操作，即将顺序扫描原本的db文件，并只保存最新的值\n存储单元通常如下\nLSM-Tree​\n首先对于一个段或者level，内部key需要按照顺序维护，这也是与Bitcask存储模型的最大的不同。\n在存储上分为内存和磁盘两部分，所有的请求首先会写入到内存当中，在内存中进行管理，为了维护有序结构，在内存中可以使用跳表或者B+树等方式管理key，当内存当中的key到达上限时，在将其顺序写入到磁盘上，即可保证磁盘当中内容的有序性。\n在写入之后，内存当中只需要保存稀疏的一个索引结构即可，不必保存所有的key。\n在磁盘上即为SSTable，其中分为两种数据块，一种为存储数据，另一种为索引块，标明各个块的key的范围。\n读写操作\n写：就像上面所说的那样，先写入到内存当中，之后再刷盘进行持久化，形成一个有序数据块。 读：由于是追加写入，为了获取到最新的key，先尝试从内存中进行获取，如果没有则去读取最新的对应范围的数据块，如果获取不到再去寻找更陈旧的数据块，在第一次获取时则完成搜索，否则继续直至完成搜索。而对于范围查询，可以并行的查询多个符合范围的component 合并\n为了解决磁盘当中component积累导致的读性能下降的问题，需要将多个component合并，以去除重复的key，从而减少查询的次数。主要有两种方案：\nlevel：在L层级的一个component会接受从L-1层级来的合并，直至达到该层的上限，之后向L+1层去进行合并 tiers：一层当中会有T个component，当满了需要合并时，T个component合并成一个并放入到下一层。 L+1层能够容纳的key的数量为L层的T倍，并且对于插入删除数量相等的均衡情况，层数维持不变。\nlevel策略下会通过频繁的合并以减少component的数量，对读请求较为友好，tiers策略下合并较少(一层满了T个之后才会合并)，因此对于写请求较为友好。\n索引 谈一谈索引和常见的索引形式 数据库索引是一种用于提高数据库查询性能的数据结构。它类似于书籍的目录，通过按照特定字段或字段组合对数据库表中的数据进行排序和组织，从而加快对数据的检索速度。\n较为常见的索引类型为hashtable 和 B+ Tree，在形式上分为聚簇索引和非聚簇索引\n对于聚簇索引索引，则存在一个主键，默认为主键创建索引，之后主键的索引上存储真实的数据，通过在主键索引上搜索即可找到对应的Tuple，这样的好处是当需要通过主键进行搜索时，可以通过对索引的一次访问就找到数据，而对于二级索引，其中存储的为key -\u0026gt; 主键，即通过二级索引只能找到一个主键索引的标志，之后再通过这个标识去主键索引当中读取，这个过程称为回表。 对于非聚簇索引，不存在主键、二级索引的概念，所有的索引所存储的都是一个key -\u0026gt; offset，这个offset通常为一个组合，当中可能包含table_id page_id offset，拿到这个offset之后再去HeapFile-Page当中偏移读取。 说一说B+树 不想说了。。。移步Bustub Lab2\n简单介绍一下哈希表 这里不针对哈希索引，只是说一下哈希表这种数据结构，主要分为静态哈希和动态哈希\n静态哈希\n静态哈希的本质上为一个数组，通过哈希函数 % len的方式找到一个数组下标，该下标就对应着真实的存储位置。\n对于哈希冲突问题，根据不同的解决方式可以继续细分：\n最基础的即为线性探测，即向下一个哈希槽移动，在删除时将由于冲突向下移动的均向原来的位置移动一格，或者设置一个墓碑，令该哈希槽不可用\n罗宾汉哈希\n罗宾汉劫富济贫，缩小贫富差距，因此引入一个权重，来标识距离原本计算出来的槽的距离，最终的目标是令所有的所有的key的权重（距离）较为平均\n下图蓝色标识原本计算出的槽的位置，val[i]标识最终的位置具体最初计算出的位置的距离\nD[1]\u0026lt;E[2]，因此为了保证均衡，E进入当前D所在的槽，将D挤到下一个槽处，这样D E的权重均为2，而不是一个1一个3\nCUCKOO 哈希\n创建两个哈希表，但加入一个键是，使用两个不同的哈希函数进行运算，在两个哈希表中挑选一个没有冲突的槽放入 如果两个均冲突了，选择一个槽，假设选择了1号哈希表，将其中原本的元素取出，使用2号哈希函数计算，试图放入到2号哈希表中，如果还冲突，取出2号的去1号尝试 这个过程中可能存在无限循环的问题，需要进行检测，并对哈希表扩容 动态哈希\n链哈希：在一个哈希槽上存在多个哈希桶，各个哈希桶之间用链表连接，哈希桶中存在多个Key\nEXTENDIBLE HASHING：\n维护一个全局计数器，标识一个哈希在寻找哈希槽是需要比较的位数 每个哈希槽维护一个local计数器，标识在这个哈希槽内需要比较的位数 全局计数器用于找到对应的哈希槽，局部计数器用于在扩容时分配元素\n‍\n‍\n","permalink":"http://itfischer.space/en/posts/tech/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80%E7%BB%93%E6%9E%84/","summary":"数据库基础结构 数据库结构 为什么需要DBMS？ 一个数据库在最基础的层次上需要完成两件事情：当你把数据交给数据库时，它应当把数据存储起来；而后当","title":"数据库基础结构"},{"content":"raft-example 在etcd当中，提供了一个raft-example，该程序并非构建了一个完整的Raft模块，而是对Raft模块的的基本使用。并在此基础上构建了一个简单的KV存储结构。而Raft模块是作为一个包的形式存在的，Raft模块只提供如Leader Election Log Replication Snapshot等Raft的基本逻辑功能，而在此之上的存储、网络通信等都交给了使用Raft包的开发者来自行决定。\n因此最终的结构就分为了三层：\n最底层的Raft Package，只提供Raft最基本的逻辑功能 Raft服务器，调用底层的Raft Package的相关API，自定义日志和快照的存储以及网络通信等 内存数据库，和Raft服务器之间通过channel进行通信，向下提交日志，并且接受Raft服务器等Commit信息，之后将其应用于自身的存储模块当中。 kvstore 此处的设计基本上与MIT6.824一致，首先看一下kvstore的结构体\n1 2 3 4 5 6 type kvstore struct { proposeC chan\u0026lt;- string // channel for proposing updates mu sync.RWMutex kvStore map[string]string // current committed key-value pairs snapshotter *snap.Snapshotter } proposeC用于向下层的Raft服务器提交新的请求，之后下层的Raft服务器拿到之后调用Raft Package当中的相关API封装为日志，之后形成共识 kvStore即为内存数据库，但是可以通过快照 + WAL日志来提供持久化稳定存储 snapshotter用于处理快照等相关消息，如Load Save等，在etcdserver/api/snap当中实现 创建​\nkvStore的创建过程也比较简单，首先尝试从快照当中恢复数据，之后就单独开启一个goroutine，监听从下层Raft传来的日志提交信息，如果提交，就将其应用到自身\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func newKVStore(snapshotter *snap.Snapshotter, proposeC chan\u0026lt;- string, commitC \u0026lt;-chan *commit, errorC \u0026lt;-chan error) *kvstore { s := \u0026amp;kvstore{proposeC: proposeC, kvStore: make(map[string]string), snapshotter: snapshotter} snapshot, err := s.loadSnapshot() if err != nil { log.Panic(err) } if snapshot != nil { log.Printf(\u0026#34;loading snapshot at term %d and index %d\u0026#34;, snapshot.Metadata.Term, snapshot.Metadata.Index) if err := s.recoverFromSnapshot(snapshot.Data); err != nil { log.Panic(err) } } // read commits from raft into kvStore map until error go s.readCommits(commitC, errorC) return s } readCommit的实现和MIT6.824当中的实现稍有不同，由于ectd的快照是真实存储的，因此下层的Raft只需要通过一个nil​来告知一下上层有了新的快照，之后上层就可以去进行读取，之后就是读取commit信息应用于自身，最后通过close chan的形式通知下层Raft上层应用完毕。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 func (s *kvstore) readCommits(commitC \u0026lt;-chan *commit, errorC \u0026lt;-chan error) { for commit := range commitC { // 相比于MIT 6.824 此处的snapshot为真正持久化的，因此无需通过channel传输 // 因此在chan当中只需要发送一个nil用于通知即可 if commit == nil { // signaled to load snapshot snapshot, err := s.loadSnapshot() if err != nil { log.Panic(err) } if snapshot != nil { log.Printf(\u0026#34;loading snapshot at term %d and index %d\u0026#34;, snapshot.Metadata.Term, snapshot.Metadata.Index) if err := s.recoverFromSnapshot(snapshot.Data); err != nil { log.Panic(err) } } continue } // handle the commit data // .... close(commit.applyDoneC) } if err, ok := \u0026lt;-errorC; ok { log.Fatal(err) } } 除了几个从Snapshot当中加载数据的函数之外，就只剩写入和读取了。写入操作和MIT6.824一样，当提交了一个写入请求之后，向下提交一个日志，此时kvstore会一直阻塞，直至下层的Raft形成了共识并向上传递了commit的信息，之后才会通知客户端写入成功。\n但是对于读取操作，在raft-example当中并没有通过Raft日志来保证强一致性，而是直接在Leader处进行本地读的操作,可以提高读操作的qps，但是相对的，强一致性就无法得到保证。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 func (s *kvstore) Lookup(key string) (string, bool) { s.mu.RLock() defer s.mu.RUnlock() v, ok := s.kvStore[key] return v, ok } func (s *kvstore) Propose(k string, v string) { var buf bytes.Buffer if err := gob.NewEncoder(\u0026amp;buf).Encode(kv{k, v}); err != nil { log.Fatal(err) } // 将kv写入到下层，等待raft完成共识 s.proposeC \u0026lt;- buf.String() } 在kvstore之上还有一层httpApi，用于对外提供网络服务，比较简单就放一下代码\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 func (h *httpKVAPI) ServeHTTP(w http.ResponseWriter, r *http.Request) { key := r.RequestURI defer r.Body.Close() switch { case r.Method == \u0026#34;PUT\u0026#34;: v, err := ioutil.ReadAll(r.Body) if err != nil { log.Printf(\u0026#34;Failed to read on PUT (%v)\\n\u0026#34;, err) http.Error(w, \u0026#34;Failed on PUT\u0026#34;, http.StatusBadRequest) return } h.store.Propose(key, string(v)) // Optimistic-- no waiting for ack from raft. Value is not yet // committed so a subsequent GET on the key may return old value w.WriteHeader(http.StatusNoContent) case r.Method == \u0026#34;GET\u0026#34;: if v, ok := h.store.Lookup(key); ok { w.Write([]byte(v)) } else { http.Error(w, \u0026#34;Failed to GET\u0026#34;, http.StatusNotFound) } case r.Method == \u0026#34;POST\u0026#34;: url, err := ioutil.ReadAll(r.Body) if err != nil { log.Printf(\u0026#34;Failed to read on POST (%v)\\n\u0026#34;, err) http.Error(w, \u0026#34;Failed on POST\u0026#34;, http.StatusBadRequest) return } nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil { log.Printf(\u0026#34;Failed to convert ID for conf change (%v)\\n\u0026#34;, err) http.Error(w, \u0026#34;Failed on POST\u0026#34;, http.StatusBadRequest) return } cc := raftpb.ConfChange{ Type: raftpb.ConfChangeAddNode, NodeID: nodeId, Context: url, } h.confChangeC \u0026lt;- cc // As above, optimistic that raft will apply the conf change w.WriteHeader(http.StatusNoContent) case r.Method == \u0026#34;DELETE\u0026#34;: nodeId, err := strconv.ParseUint(key[1:], 0, 64) if err != nil { log.Printf(\u0026#34;Failed to convert ID for conf change (%v)\\n\u0026#34;, err) http.Error(w, \u0026#34;Failed on DELETE\u0026#34;, http.StatusBadRequest) return } cc := raftpb.ConfChange{ Type: raftpb.ConfChangeRemoveNode, NodeID: nodeId, } h.confChangeC \u0026lt;- cc // As above, optimistic that raft will apply the conf change w.WriteHeader(http.StatusNoContent) default: w.Header().Set(\u0026#34;Allow\u0026#34;, \u0026#34;PUT\u0026#34;) w.Header().Add(\u0026#34;Allow\u0026#34;, \u0026#34;GET\u0026#34;) w.Header().Add(\u0026#34;Allow\u0026#34;, \u0026#34;POST\u0026#34;) w.Header().Add(\u0026#34;Allow\u0026#34;, \u0026#34;DELETE\u0026#34;) http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) } } Raft Raft模块，或者说Raft服务器，构建于Raft的包之上，Raft包提供的Leader Eelction Log Replication功能之上，再提供日志和快照的存储形式、节点之间的通信方式等功能，构建出一个完整的Raft。\n首先还是看一下Raft的结构体\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 type raftNode struct { proposeC \u0026lt;-chan string // proposed messages (k,v) confChangeC \u0026lt;-chan raftpb.ConfChange // proposed cluster config changes commitC chan\u0026lt;- *commit // entries committed to log (k,v) errorC chan\u0026lt;- error // errors from raft session id int // client ID for raft session peers []string // raft peer URLs join bool // node is joining an existing cluster waldir string // path to WAL directory snapdir string // path to snapshot directory getSnapshot func() ([]byte, error) confState raftpb.ConfState snapshotIndex uint64 appliedIndex uint64 // raft backing for the commit/error channel node raft.Node raftStorage *raft.MemoryStorage wal *wal.WAL snapshotter *snap.Snapshotter snapshotterReady chan *snap.Snapshotter // signals when snapshotter is ready snapCount uint64 transport *rafthttp.Transport stopc chan struct{} // signals proposal channel closed httpstopc chan struct{} // signals http server to shutdown httpdonec chan struct{} // signals http server shutdown complete logger *zap.Logger } proposeC 从 kvstore当中接受新的请求，创建为日志 confChangeC Raft集群配置更新的相关消息 commitC 向kvstore发送日志提交的信息 errorC 传递错误信息 snapshotIndex 快照化的最后一条日志的Index，用于在故障恢复之后找到Index appliedIndex 应用于kvstorte的最后一条日志Index，用于故障恢复 node Raft Package对外提供的API接口 raftStorage 稳定存储日志，由于使用了WAL，因此可以使用内存的形式稳定存储 wal 对WAL日志操作的封装 transport raft节点之间的通信 此外还有三个struct{}的 chan用于进行消息通知，接收方使用select阻塞，发送方向其中发送一个空结构体即可使其从阻塞当中恢复。\nRaft的创建 定义一个newRaftNode供上层的kvstore进行调用，传入一个propose的channel用于接受kvstore的新的请求，之后初始化raftNode的部分值，并将commitC与errorC返回交给kvstore。之后再单独开启一个goroutine调用startRaft来启动一些raft的服务。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 func newRaftNode(id int, peers []string, join bool, getSnapshot func() ([]byte, error), proposeC \u0026lt;-chan string, confChangeC \u0026lt;-chan raftpb.ConfChange) (\u0026lt;-chan *commit, \u0026lt;-chan error, \u0026lt;-chan *snap.Snapshotter) { commitC := make(chan *commit) errorC := make(chan error) rc := \u0026amp;raftNode{ proposeC: proposeC, confChangeC: confChangeC, commitC: commitC, errorC: errorC, id: id, peers: peers, join: join, waldir: fmt.Sprintf(\u0026#34;raftexample-%d\u0026#34;, id), snapdir: fmt.Sprintf(\u0026#34;raftexample-%d-snap\u0026#34;, id), getSnapshot: getSnapshot, snapCount: defaultSnapshotCount, stopc: make(chan struct{}), httpstopc: make(chan struct{}), httpdonec: make(chan struct{}), logger: zap.NewExample(), snapshotterReady: make(chan *snap.Snapshotter, 1), // rest of structure populated after WAL replay } go rc.startRaft() return commitC, errorC, rc.snapshotterReady } 初始化 startRaft​\n在startRaft当中继续完成一部分初始化的工作，当前的节点有可能是之前宕机之后重启，因此需要首先检查快照与写入的WAL，从中读取快照并通知上层的kvstore去应用快照恢复内存数据库，和恢复Raft日志到memoryStorage当中。\n之后再对底层的Raft Package进行一些相关的配置，如设置心跳信息等\netcd当中使用的为逻辑时钟，即通过Tick来推进时钟，如将heartbeat的间隔设置为1次tick，选举的时间间隔设置为10次tick\n之后再定义raft节点之间的传输协议之后，初始化基本完成，之后分别开启一个协程去负责节点之间的通信和一个用于处理和kvstore的channel和raft package的channel\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 func (rc *raftNode) startRaft() { // handle snapshot and wal // ... c := \u0026amp;raft.Config{ ID: uint64(rc.id), ElectionTick: 10, HeartbeatTick: 1, Storage: rc.raftStorage, MaxSizePerMsg: 1024 * 1024, MaxInflightMsgs: 256, MaxUncommittedEntriesSize: 1 \u0026lt;\u0026lt; 30, } // 在创建节点时，如果通过原本的WAL日志进行log和snapshot的加载 // 或者为中途加入到集群当中的节点相比于新节点少了通过bootstrap进行初始化加载 if oldwal || rc.join { rc.node = raft.RestartNode(c) } else { rc.node = raft.StartNode(c, rpeers) } rc.transport = \u0026amp;rafthttp.Transport{ Logger: rc.logger, ID: types.ID(rc.id), ClusterID: 0x1000, Raft: rc, ServerStats: stats.NewServerStats(\u0026#34;\u0026#34;, \u0026#34;\u0026#34;), LeaderStats: stats.NewLeaderStats(zap.NewExample(), strconv.Itoa(rc.id)), ErrorC: make(chan error), } rc.transport.Start() for i := range rc.peers { if i+1 != rc.id { rc.transport.AddPeer(types.ID(i+1), []string{rc.peers[i]}) } } // serveRaft对外监听端口提供服务 // serveChannels处理存储层发起的请求，和raftNode所传来的相关信息 go rc.serveRaft() go rc.serveChannels() } channels处理 重点看一下serveChannels\n在serverChannels当中，又单独开了一个goroutine，其中通过select 监听kvstore的propose channel和集群配置的channel，调用Raft Package的API交给下层的Raft Package进行处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 go func() { confChangeCount := uint64(0) for rc.proposeC != nil \u0026amp;\u0026amp; rc.confChangeC != nil { select { case prop, ok := \u0026lt;-rc.proposeC: if !ok { rc.proposeC = nil } else { // blocks until accepted by raft state machine rc.node.Propose(context.TODO(), []byte(prop)) } case cc, ok := \u0026lt;-rc.confChangeC: if !ok { rc.confChangeC = nil } else { confChangeCount++ cc.ID = confChangeCount rc.node.ProposeConfChange(context.TODO(), cc) } } } // client closed channel; shutdown raft if not already close(rc.stopc) }() 而serverChannels自身的goroutine用于处理Raft Package处理完毕的消息，共select 两个channel，一个用于处理定时器，如果通过这个channel收到了消息那么就调用Tick函数推进逻辑时钟。\n另外一个Channel是通过raftnode.Ready()函数返回的一个 chan Ready​\nReady的结构体定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 type Ready struct { // The current volatile state of a Node. // SoftState will be nil if there is no update. // It is not required to consume or store SoftState. *SoftState // The current state of a Node to be saved to stable storage BEFORE // Messages are sent. // HardState will be equal to empty state if there is no update. pb.HardState // ReadStates can be used for node to serve linearizable read requests locally // when its applied index is greater than the index in ReadState. // Note that the readState will be returned when raft receives msgReadIndex. // The returned is only valid for the request that requested to read. ReadStates []ReadState // Entries specifies entries to be saved to stable storage BEFORE // Messages are sent. Entries []pb.Entry // Snapshot specifies the snapshot to be saved to stable storage. Snapshot pb.Snapshot // CommittedEntries specifies entries to be committed to a // store/state-machine. These have previously been committed to stable // store. CommittedEntries []pb.Entry // Messages specifies outbound messages to be sent AFTER Entries are // committed to stable storage. // If it contains a MsgSnap message, the application MUST report back to raft // when the snapshot has been received or has failed by calling ReportSnapshot. Messages []pb.Message // MustSync indicates whether the HardState and Entries must be synchronously // written to disk or if an asynchronous write is permissible. MustSync bool } 此处我们只需要HardState、Entries、Snapshot、Messages、CommitedEntries\n就像上文所说的那样，Raft Package并不负责通信和存储，这两部分都要交给RaftServer处理，因此将HardState、Entries写入到WAL当中，Snapshot同样进行保存。之后再把Entries添加到raftStorage当中。\n而Message即为当前节点产生的所有的需要发送给其他节点的消息，都需要在此处进行发送，通过之前定义的transport模块进行发送。\n而对于底层Raft Package已经达成共识认定为committed 的Entries，通过publishEntries​处理后通过commitC通知上层的kvstore。\n","permalink":"http://itfischer.space/en/posts/tech/etcd/raft-example/","summary":"raft-example 在etcd当中，提供了一个raft-example，该程序并非构建了一个完整的Raft模块，而是对Raft模块的的基本使用。并在此基础上构","title":"raft-example"},{"content":"内存虚拟化 简单谈一谈对内存虚拟化的理解？ 计算机当中的内存条成为物理内存，虚拟化内存即为对物理内存进行抽象，抽象成为地址空间的一个概念。在其中划分了程序代码，栈、堆三个空间，栈用于保存函数的调用信息，分配空间给局部变量，用于传递参数的函数返回值，堆用于管理动态分配的内存。\n每个进程使用的内存为成为地址空间的虚拟内存，而不是直接使用物理内存， 首先可以保证易用性，每个进程对地址的使用可以从0KB开始，之后操作系统再将地址空间映射到物理内存的某个位置上。同时可以保证安全性和隔离性。每个进程的地址空间都会被映射到不同的物理内存范围上，除非使用特殊的手段，一个进程在运行期间无法访问到其他进程在物理内存上的映射。\n说一说分段内存管理 对于地址空间，如果不进行分段，则需要通过一个基址寄存器和一个范围寄存器将整个地址空间映射到物理内存的某个位置上，存在的问题是不灵活并且容易产生碎片，影响之后内存的分配。\n而分段的形式，则可以将整个地址空间分为多个段，可以是代码 + 栈 + 堆，也可以采用更加细致的分法，对于每一段，单独进行映射，从而减少内存碎片的问题。\n分段在技术上如何实现 原本地址空间作为整体进行映射只需要一对基址寄存器 + 范围寄存器，分段时可以采用多对寄存器，来对不同的段进行映射。\n在寻址时，首先根据地址的前几位找到对应的段，对于后面的几位，需要首先与该段的虚拟内存的段首进行计算，得出一个偏移量，之后再与基址寄存器当中的地址进行偏运算，得到真实的物理地址。\n此外，对于栈这种反向增长的段，需要通过一个标志位来表明其增长方向，和基址寄存器偏移运算时偏移量为负\n分段形式如何进行空闲内存管理 在分段内存管理下，可以将物理内存视为一个连续的大型的数组，每次分配就会从该数组当中分配走一部分连续的，大小不固定的一部分内存。而空闲空间是通过空闲链表(free_list_)进行管理的。最初链表当中只有一个元素，即完整的数组，如果从物理内存的开头进行分配，则最后空闲链表当中为一个大小变小了的元素，而如果在物理内存的中间进行分配，则会导致物理地址的空闲部分被分割成两份，空闲链表当中即存在两个元素，这样带来的问题，新的较大的内存无法得到分配，如原本30的内存地址在中间分配10的内存，空闲地址被分割成了10 10两个部分，对于15的分配需求则无法满足。\n对于上例当中中间的部分进行释放，则会得到10 10 10三段内存空间，此时则需要进行合并，将其合并成一个完整的空闲内存空间。\n如果因为当中存在大量的碎片而导致无法进行分配时，可以暂停所有进程的执行，进行一次重新的映射，将内存碎片进行合并。只不过暂停所有进程的执行的代价相当大。\n如何对已经分配出去内存进行回收 在通过malloc进行分配内存时，在寻找到一块空闲的物理内存之后，可以创建一个header，在其中保存一些相关的信息，其中至少包含分配的内存空间的大小，也有可能有一些额外的信息来帮助内存空间的释放。之后返回的指针指向header之后的地址。\n在通过free进行释放时，可以通过指针运算找到header的位置，之后将header和分配的内存一并释放\n段式内存分配的基本策略 固定分配（Fixed Allocation）：固定分配是最简单的内存分配策略，将内存按照固定大小的段进行划分。每个进程被分配一个或多个固定大小的段，这些段在进程的整个生命周期中保持不变。固定分配适用于不需要动态内存管理的环境。\n等分分配（Equal Allocation）：等分分配将可用内存均匀地划分为若干等大小的段，每个进程被分配一个等大小的段。这种分配策略适用于进程数目固定且相对均匀的场景。\n动态分配（Dynamic Allocation）：动态分配是一种灵活的分配策略，根据进程的需求动态分配内存段。动态分配可以采用以下几种策略：\n首次适应（First Fit）：在空闲分区列表中找到第一个大小足够的空闲分区。 最佳适应（Best Fit）：在空闲分区列表中找到最小的足够大小的空闲分区。 最差适应（Worst Fit）：在空闲分区列表中找到最大的足够大小的空闲分区。 下次适应：多维护一个指针，指向上一次查找结束的位置 快速适应（Quick Fit）：将内存划分为几个不同大小的分区，每个分区维护一个空闲链表。根据进程的大小选择相应的空闲链表进行分配。 伙伴分配（Buddy Allocation）：伙伴分配将可用内存划分为大小为2的幂次方的块，每个块的大小是上一个块大小的两倍。进程申请内存时，分配器会查找大小合适的空闲块，如果找到了比所需大小稍大的块，则会进行分割。如果没有合适的块，将进行内存合并以创建更大的块。\n说一下分页内存管理 相比于分段动态的确定分配内存的大小，分页采取的方式是先将物理内存和虚拟内存划分为多个基本单位，一个单位成为一个页(Page)。以页为单位进行虚拟内存和物理内存之间的映射，在地址空间当中，每个Page都有一个id，成为VPN(vitural page number)，而在物理空间当中，存在一个PFN(Physical Frame Number)，对于地址空间当中的一个Page，通过地址转换即可通过VPN得到PFN，从而找到物理内存的地址。\n寻址时，地址的前几位作为VPN，经过地址转换得到PFN，后几位作为偏移量找到对应的地址\n而VPN到PFN的映射的集合称为页表，由于每个进程的地址空间均为从0开始，因此每个进程都拥有一个独立的页表。\n页表的数据结构和存储？ 页表本身存储在内存当中，其本质即为一个数组，数组的下标即为VPN，数组的每个元素为一个Entry，称为PTE(Page Table Entry)其中除了存储物理页号PPN之外，还有一些标志位，如有效位，表明该映射是否有效，保护位，存在位，参考位等\n如何加速内存的访问过程？ 一次完整的内存访问过程需要：\n对地址右移运算得到前几位，求出VPN 计算出对应PTE在内存当中的位置 读取出对应的PTE，判断是否有效，如果有效则在其中获取出PPN 根据PPN + 偏移量计算出真实的物理地址，读取内容 整个过程中涉及到多次内存访问和计算，速度较慢，因此引入缓存机制，称为TLB(Transaction-loolaside-buffer) 对频繁发生虚拟内存到物理地址转换的硬件缓存。\n其采取全相联的方式，即只有一个组，一条地址映射可能存在TLB中的任意位置，而一条TLB的结构可以简单的表示为：VPN | PFN | 其他位​，其他位用于表示其是否为一个有效的转换以及读写等保护位\n在引入了TLB之后，整个内存访问的过程为：\n检查TLB当中是否存在一个相关映射 如果存在相关映射并且有效，那么直接获取到对应的PFN，去物理内存当中读取 如果不存在，则称为TLB未命中，按照之前的方法进行地址转换，并且将获取到的PFN生成一个Entry，添加到TLB当中，而当TLB已满时，可以采用随机或者LRU的方式从中淘汰掉某些Entry，继续添加 谁来处理TLB未命中？ 在之前的操作系统当中，通常通过硬件来处理TLB未命中，发生未命中时，硬件遍历页表找到正确的PTE，取出想要的地址映射，用其更新TLB\n更加现代的体系结构当中，通常通过软件来处理TLB未命中，发生TLB未命中时，硬件系统跑出一个异常，暂停当前的指令流，并且将特权级别提升至内核模式，跳转到对应的陷阱处理程序，该陷阱处理程序由操作系统提供，在其中通过特权指令更新TLB，从陷阱中返回，之后硬件重试指令则会TLB命中\n上下文切换时TLB的处理 由于每个进程均有一个页表，如果TLB也为当前进程所独占的话，每次上下文切换后，其中的映射则全部失效，又需要重新开始建立缓存，因此对应的解决方案为添加一个额外的标志位，称为地址空间标识符(ASID)，可以视为进程标识符(PID)，之后即可多个进程共享TLB。\n‍\n如何解决页表过大的问题？ 由于单级页表采用的管理方式为类似数组的形式，即一开始就确定好VPN和PFN的映射关系，如果存在了改映射，再单独使用有效位进行标识。再加上每个进程均会拥有一个页表，对于32位的地址空间，Page为4KB大小，假设一个Page Table Entry的大小为4个字节，则一个页表的大小为4M，假设当前有100个进程，页表则会占用400MB大小的内存。\n最简单的方式即为用更大的Page，Page的大小扩大N倍，即可使Page Table Entry的数量减少到1/N，占用空间缩减为原本的1/N 分段分页混合：分段是将进程的地址空间划分为多个段，每个段具有不同的大小和属性，如代码段、数据段、堆段和栈段等。每个段都有一个段表，用于映射段的逻辑地址到物理地址。由于段的大小可以根据需求进行调整，因此可以避免对整个进程地址空间的映射，节省了内存空间 多级页表：将Page Table分为Page大小的单元，如果该单元内存在有效的映射，则分配该页的Page Table，否则不分配，可以避免为无效的映射额外分配内存进行存储 展开说说多级页表 先以简单的二级多级页表为例，首先将Page Table按照Page的大小进行划分为一个个单元，之后确定一个二级索引，称为Page Directory，其中的每一个条目称为Page Directory Entry（PDE），PDE当中存储有效位和Page Table当中的某一个单元所在的物理内存的位置的PFN。\n当Page Table当中某一个单元其中存在有效的映射时，为其建立二级索引，即在Page Directory当中添加一个Entry，包含该单元的物理Page Frame Number，通过这样的形式，对于一个单元中全为无效映射的单元，不为其建立索引，也不为其在物理内存上分配空间，从而简约了内存。\n同时，页表也不需要连续存储，可以以Page大小为单位存储在内存的任意位置，之后通过二级索引即可找到。\n在进行寻址时，将虚拟地址划分为三段，第一段作为Page Directory索引找到对应的PDE，之后根据PDE当中的PFN去物理内存中读取到对应的Page Table单元，之后第二段作为Page Table的索引，找到对应的PTE，在其中获取到最终的PFN，在使用最后一段作为偏移量，和PFN一起确定最终的物理地址。\n而当页表过多时，即Page Directory也无法放入到一个Page当中，可以建立三级索引，此时将虚拟地址划分为4段，按照同样的方式进行索引寻找\n同时，多级页表也可以和TLB进行结合，TLB当中为单级形式，记录VPN和最终的PFN的映射，多级索引搜索过程中Page Table单元的PFN不存储于TLB当中，如果TLB未命中，再按照多级索引的方式进行搜索。\n介绍一下Swap 之前所假设的都为所有Page全部存储于内存当中，而当内存满了的时候，则需要将一部分没有使用到的内存交换到磁盘上进行存储，此时即可给进程提供一个假象，即当前的计算机上有无穷无尽的内存可供使用，永远不会因为内存不足而导致失败。\n在原本的基础上首先在磁盘上开辟一块空间，作为交换空间(Swap Space)，之后对于原本的原本的查找过程，此时则会存在当前的Page不存在于内存当中，已经被交换到了硬盘上，此时需要在Page Table Entry当中添加一位存在位，如果存在，则直接使用PTE当中的PFN在物理内存当中进行读取，如果不存在，此时则触发了一个Page Fault，需要首先将Page从硬盘当中交换到内存当中，之后在进行读取。\n说一说Page Fault Page Fault即为当前读取的Page被交换到了磁盘上，可以通过PTE当中的存在位进行判断，并且在PTE当中存储硬盘地址，用于进行读取，当磁盘完成IO之后，操作系统更新Page Table，将该页标记为存在，更新PTE的PFN字段，用于之后直接从物理内存当中进行读取，并且此时还会更新TLB进行缓存。\n当触发了Page Fault之后，操作系统通过某些算法淘汰一个Page，之后从磁盘当中进行读取，之后设置PFN和存在位。\n介绍一下内存缓冲淘汰策略 首先缓冲淘汰策略的衡量指标为缓存命中与未命中的处理时间求期望。\n提出的最理想的淘汰策略为：当缓存满时，淘汰未来最远的将来才会访问的Page，但是由于对于未来的情况无法估计，因此其只作为最优的情况用于性能评估，看其他的算法在性能上与其性能上的差距。\n真实可以实现的有FIFO与Random，以及LRU。其中LRU基于局部性原理（时间局部性与空间局部性）可以取得较好的性能。\n在LRU的基础上还有用于解决缓存污染问题的LRU-K和用于减少计算开销的近似LRU\n近似LRU：\n时钟算法：时钟算法使用一个环形缓冲区，每个页面有一个访问位，表示是否被访问过。算法维护一个指针，指向当前位置。当页面被访问时，访问位设置为1。当需要淘汰页面时，算法从当前位置开始查找，如果访问位为0，则选择淘汰该页面，并将指针移到下一个位置；如果访问位为1，则将访问位设为0，并将指针移到下一个位置，继续查找。这样，较长时间未被访问的页面会被淘汰，近似实现了LRU算法的效果。 二次机会算法：二次机会算法是对时钟算法的改进，它在每个页面的基础上添加了一个修改位，表示页面是否被修改过。算法维护一个指针，指向当前位置。当页面被访问时，访问位设置为1，修改位保持不变。当需要淘汰页面时，算法从当前位置开始查找，如果访问位和修改位都为0，则选择淘汰该页面，并将指针移到下一个位置；如果访问位为1，则将访问位设为0，并将指针移到下一个位置，继续查找。如果访问位为0而修改位为1，则将访问位设为1，并将指针移到下一个位置。这样，较长时间未被访问且未被修改的页面会被优先淘汰 ","permalink":"http://itfischer.space/en/posts/tech/%E5%86%85%E5%AD%98%E8%99%9A%E6%8B%9F%E5%8C%96/","summary":"内存虚拟化 简单谈一谈对内存虚拟化的理解？ 计算机当中的内存条成为物理内存，虚拟化内存即为对物理内存进行抽象，抽象成为地址空间的一个概念。在其中","title":"内存虚拟化"},{"content":"如何理解CPU虚拟化？ CPU虚拟化，用一句话简单的概括就是，通过CPU虚拟化的手段，可以让多个程序在同一时间段内在一台机器上运行，共享CPU，同时对此毫无感知，每个程序认为只有自己在使用CPU。\n为了达到此目的，操作系统首先将运行的程序抽象成进程。要运行一个程序即启动一个进程，为其分配相应的资源和内存空间。之后将其交给操作系统管理。而为了实现多个进程都毫无感知的同时运行使用CPU，此时就需要涉及到进程调度算法，如FIFO、多级反馈队列等。并且通过上下文切换的来实现进程之间的切换。此时就可以将单一的CPU交给不同的进程去使用，此时在宏观的时间段上来看，就有多个程序在同时执行，使用同一个CPU。\n进程相关API fork():通过fork()​可以创建一个子进程，fork的调用和返回都比较奇怪，调用当一个进程调用fork之后，当前进程会从fork返回，同时也会创建一个子进程，也从fork当中返回。如果不加以区分的话，二者之后会执行相同代码。\n子进程与父进程之间的区分方式采用的为进程描述符 pid​，对于父进程，返回时会得到子进程的pid，而对于子进程，得到的pid为0，之后可以通过if的方式区分，另父进程和子进程执行不同的代码。但是此时父进程与子进程执行的为同一个程序，只不过是不同的部分\n代码的格式如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 #include\u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;iostream\u0026gt; int main() { std::cout \u0026lt;\u0026lt; \u0026#34;hello world pid:\u0026#34; \u0026lt;\u0026lt; getpid() \u0026lt;\u0026lt; std::endl; int rc = fork(); if (rc \u0026lt; 0) { std::cout \u0026lt;\u0026lt; \u0026#34;fork failed\u0026#34; \u0026lt;\u0026lt; std::endl; } else if (rc == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;hello world in child\u0026#34; \u0026lt;\u0026lt; std::endl; } else { int rc = wait(NULL); std::cout \u0026lt;\u0026lt; \u0026#34;hello world in parent\u0026#34; \u0026lt;\u0026lt; std::endl; } return 0; } 最终的执行结果如下：\nwait()​父进程可以通过调用wait​函数来等待子进程执行完成，如上面的结果一样，父进程打印的消息在子进程之后，如果不加wait则会乱序打印。\nwait只应当在父进程中调用，如果在子进程当中调用，则会产生一个错误，并返回-1\nexec():通过exec()​可以令父子进程执行完全不同的程序。当子进程返回之后，可以调用exec令其执行不同的程序，exec不会创建新的进程，调用之后会将当前的子进程给覆盖掉，就像原本的进程没有执行过一样，包括代码段、堆、栈及其他内存空间都会被初始化，因此，exec之后的代码也都不会执行，对exec的调用也永远不会返回。\n首先设置一个简单的打印数组的函数，用于之后给exec调用\n1 2 3 4 5 6 7 8 9 10 #include\u0026lt;iostream\u0026gt; #include\u0026lt;vector\u0026gt; int main() { std::vector\u0026lt;int\u0026gt; v1 = {1,2,3}; for (auto num : v1) { std :: cout \u0026lt;\u0026lt; num \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } std::cout \u0026lt;\u0026lt; std::endl; return 0; } 程序主体结构如下，创建出子进程之后调用exec，用于不需要参数，只需要传递一个程序名即可\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #include\u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;unistd.h\u0026gt; #include \u0026lt;sys/wait.h\u0026gt; #include \u0026lt;iostream\u0026gt; int main() { std::cout \u0026lt;\u0026lt; \u0026#34;hello world pid:\u0026#34; \u0026lt;\u0026lt; getpid() \u0026lt;\u0026lt; std::endl; int rc = fork(); if (rc \u0026lt; 0) { std::cout \u0026lt;\u0026lt; \u0026#34;fork failed\u0026#34; \u0026lt;\u0026lt; std::endl; } else if (rc == 0) { std::cout \u0026lt;\u0026lt; \u0026#34;hello world in child\u0026#34; \u0026lt;\u0026lt; std::endl; auto arg = strdup(\u0026#34;./print_vec\u0026#34;); char * myargs[2]; myargs[0] = strdup(\u0026#34;print_vec\u0026#34;); myargs[1] = NULL; execvp(arg, myargs); std::cout \u0026lt;\u0026lt; \u0026#34;shouldn\u0026#39;t print\u0026#34; \u0026lt;\u0026lt; std::endl; } else { int rc = wait(NULL); std::cout \u0026lt;\u0026lt; \u0026#34;hello world in parent\u0026#34; \u0026lt;\u0026lt; std::endl; } return 0; } 最终的执行结果如下：\n当前程序创建子进程之后，子进程调用exec执行其他的程序，exec之后的那一条输出也没有被执行，父进程通过wait等待新的子进程执行结束。\n‍\n通过这三种看起来较为诡异的API，可以很方便的实现shell，shell本身也为一个程序，比如输入一个可执行程序时，shell就通过fork创建一个新的子进程，并调用exec将输入的程序名传递给exec，执行新的程序，之后shell通过wait等待新程序的执行结束。结束之后，再输出一个提示符，等待用户的下一次输入。\n‍\n用户态与内核态 为了保证安全性，操作系统不能令进程不受任何限制的执行，就像早期的操作系统那样，仅仅是一个库。\n为了对进程加以限制，操作系统有了用户态和内核态的概念，在用户态的模式下，进行只能执行一些基础的操作，不能发起IO访问硬件资源，而在内核态下，操作系统可以访问机器的全部资源，如发起IO等。用户态和内核态的之间通过陷阱指令和从从陷阱返回的指令来实现用户态和内核态的切换。\n陷阱表是什么 陷阱表由一系列条目组成，每个条目与特定的系统调用、异常或中断相关联。每个条目包含处理程序的地址或处理程序的指针，当相应的事件发生时，操作系统会根据陷阱表中的条目找到对应的处理程序来处理事件，而执行了陷阱表上的相关指令就会从用户态切换到内核态。提升权限，完成一些操作。之后操作系统可以将控制权返回给用户态，使进程继续执行。\n对于一个发起IO的进程，整个的执行流程是什么？ 当要创建一个进程时，操作系统首先在内核态为该进程分配内存，之后将程序加载到内存当中，根据argv来设置程序栈，用寄存器/pc填充内核栈，之后从陷阱当中返回 此时操作系统转换为用户态，跳转到main函数，进行执行，直至执行到发起IO的系统调用 此时将寄存器保存到内核栈，转向内核模式，根据陷阱表来跳转到对应的处理程序 在内核态下处理陷阱，完成系统调用的工作，从陷阱返回 从内核栈恢复寄存器，切换到用户模式，pc进行相应的跳转 继续执行，直至从main函数中返回，通过exit(0)系统调用重新切换到内核态 在内核态下释放进程的内存，将进程从进程列表当中清除。 进程间如何进行切换？ 在单CPU单核的前提下。首先操作系统本身也是一个特殊的应用程序，如果此时CPU在运行某个应用进程，那么他就不能运行操作系统，此时操作系统失去了对计算机的控制。为解决此问题，采用的是时钟中断的方式，即每隔一段时间CPU会发出一次中断。产生中断时，正在运行的进程会终止，此时操作系统即可重获对计算机的控制，之后可以决定是让之前的进程继续运行还是做其他的工作。\n此时确定了操作系统能够间隔性的重新获取计算机的控制权。当操作系统获取到控制权之后，即可通过上下文切换的方式完成进行的切换\n如何理解上下文？ 上下文这个概念在计算机当中出现的较为频繁，如操作系统当中和spring当中的Application Context。简单来说就是保存了某一个时刻的全部状态，所需的信息都可以从上下文当中获取，对于操作系统来说，进程上下文即指描述进程或线程当前状态的所有信息的集合。它包含了使进程或线程能够继续执行的所有必要信息，包括寄存器值、程序计数器、栈指针、内存映射、打开的文件描述符、信号处理程序、权限等等。\n因此在上下文切换时，则将当前正在运行的进程的上下文保存到内存的某个位置，之后再将另外一个进程的上下文进行加载，从内核态返回之后再执行的就是另外一个进程了。\n这个过程有点类似于函数调用，在进行函数调用时，将当前函数的在寄存器当中的一些信息压入到栈中进行保存，之后便可以进行跳转，执行另外一个函数。只不过上下文切换所需保存和恢复到信息更广，而且进程之间的程序栈不共享，因此信息保存与内存当中另外的某个区域。\n常见的进程调度算法有哪些 先来先服务（First-Come, First-Served，FCFS）：按照进程到达的先后顺序进行调度，即先到先服务。 最短作业优先（Shortest Job Next，SJN）：选择估计运行时间最短的进程进行调度，以最小化平均等待时间。 最短剩余时间优先（Shortest Remaining Time First，SRTF）：选择剩余执行时间最短的进程进行调度，以最小化等待时间。 优先级调度（Priority Scheduling）：每个进程都分配了一个优先级，优先级高的进程优先被调度。 轮转调度（Round Robin，RR）：按照轮转的方式分配处理器时间片给每个进程，每个进程依次执行一个时间片。 最高响应比优先（Highest Response Ratio Next，HRRN）：根据等待时间和服务时间的比率来选择下一个被调度的进程，以提高系统响应性。 多级反馈队列调度（Multilevel Feedback Queue，MLFQ）：将进程划分为多个队列，每个队列有不同的优先级和时间片大小，进程根据行为在队列之间切换 介绍一下多级反馈队列调度 共有五条规则：\n如果A的优先级 \u0026gt; B的优先级，则运行A 如果A和B的优先级相同，则轮转运行A和B 当一个进程进入系统时，先将其加入到最高的优先级当中 一旦一个进程在某一层用完了其的时间配额（无论中间是否有主动放弃过CPU，放弃了多少次）都将其移动到下一层当中，降低优先级 每经过一段时间S，就将系统中所有的工作全部重新添加到最高一级的队列当中 多级反馈队列基于优先级调度，即规则1 2 3。在此基础之上，为了解决底层计算密集型的进程饥饿的问题，提出了规则4，可以使新加入系统的优先级逐渐下降，同时规则4也可以防止某些进程通过主动放弃CPU的方式长期占有高优先级，规则5则可以使底层的长期得不到执行的进行重新到最高层得以执行。\n","permalink":"http://itfischer.space/en/posts/tech/cpu%E8%99%9A%E6%8B%9F%E5%8C%96/","summary":"如何理解CPU虚拟化？ CPU虚拟化，用一句话简单的概括就是，通过CPU虚拟化的手段，可以让多个程序在同一时间段内在一台机器上运行，共享CPU","title":"CPU虚拟化"},{"content":"Lab4 由于五一临近考试，再加上后续还需要准备各种机试以及408等，整个Lab4做的比较的草率，基本上只实现了最基础的功能，Leaderboard Bonus的各种优化都没去实现，确实是没时间了，在这里也只能简单记录一下\nTask1 - Lock Manager 当事务尝试读取或者修改Tuple时，TableHeap和Executor类都会使用 Lock Manager去尝试在一个Tuple上获取锁（通过RID）\n支持表级别和Tuple级别的锁，以及读未提交，读提交，可重复读三种隔离级别，Lock Manager通过隔离级别进行加锁\nHints​\n通过LockRequestQueue来保存等待获取的锁，对于Tuple和对于Table 考虑何时进行锁升级，进行锁升级时，需要对LockRequestQueue做什么 当获取到锁之后，通过条件变量来通知尝试获取锁的事务 需要对事务的状态进行维护，如GROWING，SHRINKING，当进行了unlock时，就涉及到状态的切换（具体取决于隔离级别） 通过*lock_set_​来记录一个事物获取的所有的锁，当事务进行提交时，lock_manager释放其相关的锁 将一个事务的状态设置为ABORTED隐含地中止了该事务，但直到TransactionManager::Abort被调用时才会显式地中止。你应该通读这个函数和提供的测试，以了解它的作用，以及你的锁管理器在中止过程中是如何使用的。 实验手册当中并没有提供太多有用的信息，在lock_manager.h的注释当中，涉及到了具体应当如何实现，这里挑有用的翻译一下，按照整个注释提供的信息，捋顺一下基本就能够完成功能了。\nLOCK NOTE​\nLockTable()和LockRow()均为阻塞的方式，应当等到锁授予之后再return，对于中止的事务应当拒绝授予，并返回false\n支持的锁模型：\nTable级的支持所有类型的锁 Tuple级别的不支持意向锁 隔离级别：只有符合当前隔离级别的锁，并且根据当前情况允许，才能授予锁，例如，READ_UNCOMMITTED下不支持S/IS/SIX锁，相似的X/IX锁在SHRINKING阶段同样不支持\nREPEATABLE_READ：可以获取所有级别的锁，只有在GROWING阶段才能够获取锁 READ_COMMITTED：可以获取所有级别的锁，在GROWING阶段可以获取所有的锁，在SHRINKING阶段只能获取IS，S锁 READ_UNCOMMITED：只支持IX,X锁，并且只在GROWING阶段可以获取 在在Tuple上获取锁的时候，需要保证首先在Tuple所属的Table上获取到对应的锁，\n锁升级：对于一个当前事务已经上锁过的资源：\n要获取的和之前上锁的类型一致，直接返回\n不允许多个事务同时进行锁升级，通过一个标志位来进行判断\n如果锁模型不一样，考虑对其进行升级，允许的类型如下：\n1 2 3 4 * IS -\u0026gt; [S, X, IX, SIX] * S -\u0026gt; [X, SIX] * IX -\u0026gt; [X, SIX] * SIX -\u0026gt; [X] 如果对一个事务授予了一个锁，那么需要对事务的锁集合进行适当的更新\nUNLOCK NOTE​\nUnlockTable() UnlockRow()尝试在某一资源上释放锁，需要确保当前事务持有该锁，如果没有持有该锁就释放则抛出异常\n只有在没有持有该Table所下属的Tuple时，才能释放Table上的锁，否则抛出异常\n事务状态更新：根据隔离级别，释放锁会导致事务从GROWING想SHRINKING转变，只有释放S/X锁才会改变事务状态。\nREPEATABLE_READ：释放S/X导致转变为SHRINKING READ_COMMITTED：释放X锁导致向SHRINKING转变，释放S锁无影响 READ_UNCOMMITED：释放X锁导致向SHRINKING转变，READ_UNCOMMITTED不支持S锁 相关数据结构 LockRequest​\n根据当中的granted​字段，可以代表一次申请请求，或者代表当前Table或者Tuple正持有的锁\nLockRequestQueue​\n维护LockRequest的队列，代表一个Table或者Tuple锁的申请和持有情况，按照FIFO的顺序来对锁进行申请，释放锁时将其从队列当中移除。\n**table_lock_map/row_lock_map_**​\n维护所有Table，Row的锁的情况，非并发安全，需要通过锁来保护\nc++当中并没有类似Java的ConcurrentHashMap，因此大多数数据结构都需要手动加锁解锁来维护并发安全的问题，无论是queue还是map\nLock 根据上述，基本可以捋清楚整个LockTable的过程，主要分为几个阶段，前几个阶段一直尝试抛出异常，中止掉一些特殊情况，之后才尝试对锁进行获取。\n1.异常处理\n可能有的异常为：\n锁类型不支持：READ_UNCOMMITED下尝试获取S/IS/SIX， 阶段错误，即在Shrinking阶段尝试获取锁： REPEATBLE_READ条件下尝试获取任意类型的锁 READ_COMMITTED条件下获取X IX SIX READ_UNCOMMITTED条件下获取任意类型的锁 对于LockRow，还有还未在Table上加锁就在Row上加锁 2.获取到Table对应的LockRequestQueue​\n通过锁保护并发安全，如果存在则直接获取，然后释放锁，如果不存在则创建一个。\nLockRequestQueue所记录的是对于一个Table上相关的锁的持有情况，即只有确定需要要申请锁，才将锁加入到其中，之后只有锁释放的时候，才将其从队列当中移除，因此主要起到两个作用：\n记录当前Table当前Table上锁的申请顺序，根据申请顺序来确定是否需要授予锁或者进行锁升级 维护已经申请下来的锁，控制并发，直至释放锁时才将其从队列当中移除 而如果当中只有一个且为当前事务新创建的Lock_Reqeust的话，则证明当前没有任何一个事务在当前资源上申请锁或者持有锁，那么自己就是优先级最高的，即不需要等待其他的锁授予完，也不需要锁升级，也不需要检查是否冲突，直接授予锁即可。\n3.检查锁升级\n如上面所说，该Table的queue当中同时存在申请锁和持有锁的记录，根据granted字段来判断\n首先判断是否需要进行锁升级，即当前的事务之前是否有在该资源上持有锁，如果有，则考虑进行锁升级，如果没有则跳过这一步，直接尝试加锁\n锁升级的前提条件为当前没有任何其他的事务在进行锁升级，通过一个标志位来判断，如果有则中止事务，抛出异常。该标志位在进行锁升级时置为当前事务的tid，在完成锁升级，即获取到了锁之后，置为一个空的tid。\n到目前已经满足的锁升级的外部条件，即当前事务之前在该资源上持有了一个锁，并且没有其他的事务正在进行锁升级，此时需要判断内部条件，即锁的类型是否满足升级条件：\n1 2 3 4 * IS -\u0026gt; [S, X, IX, SIX] * S -\u0026gt; [X, SIX] * IX -\u0026gt; [X, SIX] * SIX -\u0026gt; [X] 如果之前持有的锁的等级高于当前持有的锁，那么清空标志位，结束锁升级，也不需要授予新的锁，直接返回。 如果当前的锁满足升级条件，那么释放之前的锁，将锁升级的过程视为一次新的锁的获取，设置升级的标志位，在第一个还未授予的Lock_Reqeust之前添加一个新的LockRequest从而可以保证可以立刻对其进行锁升级，如果不满足升级条件，则中止事务，抛出异常 4.获取锁​\n我目前初步的想法是如果能执行到这一步，则证明无需进行锁升级，常规加锁，即获取到资源对应的队列，新建一个LockRequest请求，将其添加到队列当中。之后根据队列当中的锁情况来进行锁的获取。\n最开始我的想法是单独创建一个线程，不断的去轮询所有队列，如果发现可以授予锁，那么则授予锁，并通过条件变量来通知调用Lock的线程，告知其已经授予锁，可以从阻塞当中苏醒返回，而调用Unlock的线程只需要释放锁，不需要传递或者发出任何信号。整体实现上有点像6.824当中日志提交向上层告知Apply信息的过程，但是后来想了想，也看了一下别人的博客的思路，感觉没必要那么麻烦。信号发送的工作交给调用Unlock的线程即可，大致思路如下：\n调用Lock将新创建的LockRequest加入到队列当中 之后通过条件变量的形式，反复判断是否满足获取锁的条件，如果不满足则通过条件变量阻塞 如果有其他的线程释放锁了，通过notify_all进行通知，那么此时就会从阻塞当中恢复，重新判断一次是否能够加锁，如果能则加锁并返回一个false，跳过条件变量，后续直接返回 GrantLock\n单独封装成一个函数，在作为条件变量while的进入条件，遍历当前的队列，如果在此次申请的锁之前还有未授予的锁(granted == false)，那么无法授予，返回false，交给条件变量去等待。如果当前的锁为队列当中第一个未授予的，那么判断是否兼容，如果不兼容则返回false，等待发生冲突的锁释放，兼容则返回true，授予锁并跳过条件变量。\n在调用GrantLock时队列当中至少存在一个lock_request，即当前事务新创建的，如果不存在则证明出现了问题，返回false\nUnlock Unlock的思路比较简单，首先同样先处理异常情况，如果unlock时发现在对应的资源上未持有锁，抛出异常。对于UnlockTable还需要检查是否有该Table对应的Row上面的锁还未释放，如果有，抛出异常\n之后即可遍历队列，找到当前事务持有的锁，将其释放，通知其他线程，让其重新进行一次竞争，尝试获取锁，最后根据锁的类型和隔离级别来判断是否转换为SHRINKING。\n当事务提交或者中止时，会将当前事务持有的所有的锁进行释放，因此需要集合来维护当前事务所持有的所有的锁，在事务类当中已经给出，因此在lock unlock的过程当中，一旦锁的持有情况有变化，就对对应的集合进行更新。\nTask2 - Deadlock Detection 通过一个后台线程去不断的检测是否存在死锁，建立一个相互等待的图，去发现是否存在循环，如果存在则证明有死锁，打破循环\n相关API：\nAddEdge(t1,t2)​：添加一条从t1到t2的边，如果存在的话就什么都不需要做 RemoveEdge(t1,t2)​：如果存在则移除一条边 HasCycle(txn_id)​：通过dfs去寻找是否存在循环，如果存在循环，则存储循环当中最新的事务id，并返回true，并且返回找到的第一个循环 GetEdgeList()​：返回所有存在的边，用于进行结果检测 RunCycleDetection()​：循环检测的主体 Hint​\n不需要维护一个图，每当后台线程唤醒时建立一个表，之后销毁 为保证dfs的确定性，每次选择最低的事务id为第一个，而下一个进行遍历的也选择最低的事务id 线程唤醒时，需要打破所有的循环 对于中止的事务，不加以考虑，不画边和节点 通过静态GetTransaction​方法来获取一个事务的指针 std::this_thread::sleep_for​进行休眠，时长为CYCLE_DETECTION_INTERVAL​ 当一个事务等待另外一个事务时，添加一条边，可能会有多个事务在同一个对象上持有锁，因此存在一等多的情况 事务中止时，需要将State设置为ABORTED​ 当一个事务中止时，需要通知该事务被中止 Task2的核心就是dfs，通过dfs在有向图当中检测是否有环，首先扫描当前的锁情况，通过一个集合记录当前所有的事务的tid，由于需要保证dfs的结果是确定的，因此每次从最小的txn_id，即最早创建的事务开始遍历，当遍历到一个txn_id之后，就将其加入到活跃集合当中，如果后续的搜索过程当中再次找到了该集合，那么就证明出现了环，此时挑选一个最新的事务，即txn_id最大的事务进行中止。dfs结束之后，就将其从活跃数组当中清除，并且可以将其添加到安全集合当中，之后在对其他的节点进行判断时，只要遍历到该节点，就证明后续并不存在环，可以直接返回。\n当发现了死锁，并将事务设置中止之后，通知正在等待锁的事务，此时苏醒之后需要检查当前事务的状态是否中止，如果中止，则结束加锁的过程，直接返回。\n该后台线程会和正在获取和释放锁的线程之间存在并发安全问题，访问相关数据结构时需要加锁保护。\nTask3 - Concurrent Query Execution 对之前的Next() 方法进行修改，当事务进行上锁和解锁失败时，应当抛出异常。\n当事务中止时，需要对该事务涉及到的table上的tuple 还有index进行undo操作，通过在事务当中维护一个写集合来实现，当事务终止时，将该集合传递给Abort\n获取锁失败时，应该抛出ExecutionException​异常，告知上层。\n在一个事务当中，应当考虑会对数据进行多次查询，针对不同的隔离级别进行处理\n在加锁上，由于支持了意向锁，因此在Init方法当中先通过意向锁锁住整个表，之后在Next方法当中在加写锁或者读锁锁住单个Tuple。根据隔离级别再考虑合适释放掉锁。\n隔离级别​\nREPEATABLE_READ：采用严格两阶段锁协议，因此只加锁不解锁，最终由commit或者abort统一释放锁，在Init当中加表级的意向锁，在Next当中对Tuple加读写锁 READ_COMMITTED：在Init当中加表级的意向锁，在Next当中添加行级别的锁，对于S锁，读取完就可以释放，无论是否为Growing。X锁为保证对外隔离，需要在事务提交时进行释放。可以保证读取到的都是已经提交的数据，但是无法提供可重复读，在一次事务当中的读取的数据并不一致。 READ_UNCOMMITTED：读操作无需加锁，因此会读取到写入但是还未提交的数据，写操作在Init当中加意向锁，Next当中加X锁，最后由事务统一释放。 在执行器当中，会接收到加锁\\解锁操作从下层抛出的异常，因此需要捕获下层的异常，并重新对外抛出一个统一的异常，并且加锁\\解锁操作也可能会失败，如果失败同样需要向上抛出一个加锁失败的异常，因此加锁的形式大致如下：\n1 2 3 4 5 6 7 8 9 10 11 12 try { if (exec_ctx_-\u0026gt;GetTransaction()-\u0026gt;GetIsolationLevel() != IsolationLevel::READ_UNCOMMITTED) { auto is_locked = exec_ctx_-\u0026gt;GetLockManager()-\u0026gt;LockTable( exec_ctx_-\u0026gt;GetTransaction(), LockManager::LockMode::INTENTION_SHARED, table_info_-\u0026gt;oid_); if (!is_locked) { throw ExecutionException(\u0026#34;SeqScan Executor Get Table Lock Failed\u0026#34;); } } // LOG_DEBUG(\u0026#34;SeqScan Init::table_name:%s table_id,%d\u0026#34;, table_info_-\u0026gt;name_.c_str(), table_info_-\u0026gt;oid_); } catch (TransactionAbortException e) { throw ExecutionException(\u0026#34;SeqScan Executor Get Table Lock Failed \u0026#34; + e.GetInfo()); } 隔离级别 在Task1和Task3上，在注释当中已经给出了非常详细的加锁解锁以及各种方向上的指导，只要将其翻译成代码级别上整个Lab就实现好了。基本上不怎么需要特别的动脑子，全程被喂饭，加上个人基础不太好，因此感觉还是有必要捋顺一下四种隔离级别在实现上的不同，以及各自针对的情况。\n四种隔离级别的差别主要聚焦于读操作上，对于写操作，为了保证数据库一致性，只有写入的锁只有数据提交时才能够释放，如果中途释放锁，写入的操作结果可能会被其他的覆盖，从而违反了数据库一致性。单独看写锁的话，都是采用了严格两阶段锁的策略，保证一致性，同时避免级联中止。\nREAD UNCOMMITTED​\n最宽松的隔离级别，根据上表可以看到脏读 不可重复读 幻读都会发生\n在实现上读操作不需要加锁，写操作和其他的一致，因此所有读相关的锁都不支持，如果想尝试加[S,IS,SIX]，都会抛出异常。由于只支持写锁，在shrinking阶段，任何加锁操作都是不允许的，会抛出异常。\n因此并不保证读取到的数据一定是提交的数据，由于并没有加读锁和其他的事务进行互斥，其他的事务一旦写入之后就可以被读取，因此当读取到之后，如果写入数据的事务中止回滚，那么读取到的数据就变成了脏数据产生脏读。\nREAD COMMITTED​\n相比于读未提交解决了脏读的问题，但是依旧会出现不可重复读和幻读\n在实现上读操作需要在Table上加IS，在Row上加S，并且当完成一次读取之后即可将锁释放，无需等待事务的提交。释放S锁并不会导致事务从growing向shrinking转变。\n由于通过S锁进行互斥，从而保证那些正在写入的事务在提交前不应被读取到，能够读取到的数据一定是提交之后稳定写入的数据。\nREPEATABLE READ​\n在读提交的基础上又解决了不可重复读的问题，存在幻读。\n实现上加锁策略和READ COMMITTED并没有区别，但是在释放锁上读写锁全部采用严格两阶段锁的策略，此次事务会一直持有读锁，从而中途不可能被其他的事务重新写入更新，在一次事务当中读取的数据全部为一致的。\nshrinking阶段不允许添加任何的锁，释放S X锁就会向shrinking进行转变。\n可串行化\n最严格的隔离级别，简单来说就是在该隔离级别下，事务的执行可以等价成一次不存在并发的串行执行。最简单的判断是否为可串行化的方法就是交换一系列不冲突的指令，从而看原本的事务调度是否可以被分为两个在时间上完全错开的串行调度，简单用读写来表示，一次通过交换或重拍转换成串行结果如下：\nSummary 整个Lab4大致就是这样，由于几天后就要考试因此整个Lab4做的比较粗糙，记录写的也很粗糙。只实现了最基本的功能，能够通过所有测试，期间还因为一个智能指针的问题卡了很久，不过未经优化的QPS几乎垫底，不过目前也没有时间做什么优化了，只能暂时搁置了。不过这也算是我用cpp写的头一个比较完整的项目，也算是克服了对cpp的恐惧。后续的话精力应该主要放在机试和408复习上了，928前应该不打算开新的Lab了，后续复习的额外时间打算看一看rocksdb和一些c++还有cmake相关的工程基础。\n","permalink":"http://itfischer.space/en/posts/tech/bustub/bustub-lab4/","summary":"Lab4 由于五一临近考试，再加上后续还需要准备各种机试以及408等，整个Lab4做的比较的草率，基本上只实现了最基础的功能，Leaderboard","title":"BusTub-Lab4"},{"content":"Lab3 Task1 火山模型 在Task1当中实现了SeqScan​ Insert​ Delete​ IndexScan​，由于均为火山模型，因此在实现上大同小异，就集中说明一下火山模型，和各自所需要注意的即可。\n火山模型，或者说迭代器模型，核心就是借助迭代器进行遍历。即对于一个算子来说，自身即为一个迭代器，对外提供一个Next方法，将数据处理后通过Next方法一条条给对外提供。以SeqScan为例，每次调用Next都从table当中获取一条tuple，之后交给上层。\n而多个迭代器可以组成迭代器链，上层通过下层的迭代器的Next函数来获取tuple，处理之后又通过自身的迭代器对外返回。就像下图这样，NestedLoopJoin以 MockTableScan作为数据来源，自身处理完的生成的tuple又一条条的通过Next交给Aggregation。\n‍\n此外，这里迭代器采用的是Top-to-Bottom的方式，即以上层的迭代器为驱动，不断的调用下层的Next来获取数据。\nSeqScan：实现一个全表遍历，比较简单，只需要通过table上的迭代器去一条条获取即可\nIndex\u0026amp;Delete：这两个实现上差不多,将子迭代器当中的一条条插入到目标表当中即可。需要注意的是，由于为火山模型，上层会不断的尝试调用Next直至获取到false，因此为了防止重复插入或删除，需要设置一个flag，在完成插入后置为false，之后再调用即可直接返回，中止迭代。\n此外，插入和删除涉及到更新索引，调用一下对应的函数就好\nIndex Scan：SeqScan为通过table的迭代器进行遍历，Index Scan即通过索引来进行遍历，即之前实现的B+树的迭代器，按照Lab知道上给的提示，转换之后获取其迭代器\n1 2 tree_{dynamic_cast\u0026lt;BPlusTreeIndexForOneIntegerColumn *\u0026gt;(index_info_-\u0026gt;index_.get())}, iter_{tree_-\u0026gt;GetBeginIterator()} 由于BusTub实现的是非聚簇索引，因此索引当中存储的为RID，即一个tuple的唯一标识，RID由page_num 和 slot_num组成，表示该tuple属于哪张表，存在于表的哪个位置。\n之后再去表当中偏移读取获取到所需的tuple\nSQL执行过程 这一部分强烈推荐看一下迟先生写的ButTub养成记。我这里就简单说一下代码当中需要用到的那部分。\n首先，根据SQL解析的结果，优化器会生成一个个的plan_node。代表的一个算子的执行策略，即需要从哪获取数据，获取数据的方式，tuple的结构等等。由于通过抽象语法树一步步的来，整个SQL执行的整体plan同样为树结构组织。\n之后每个算子，被抽象成一个Executor​，代表一个执行的动作。Executor​主要有三部分组成，分别为plan​, executor_context​,child_executor:\nplan即为上述通过优化器的来的具体执行方案，以及所需的数据，如Index_Scan当中会提供index的id，join当中会提供一个用于进行join key的Predicate，以及join的child plan\n根据情况，其会拥有一个或者多个子算子child_executor​ ，即火山模型的迭代器链，通过child_executor​来从底层获取数据。\nexecutor_context​：这个没什么好说的，就是用于保存整个数据库的上下文信息，可以通过其来获取到buffer_pool CataLog LockManager等，相比于6.830将其全部定义在Database类当中作为静态成员变量，个人感觉单独定义一个类作为上下文能够更优雅一点\n1 2 3 4 5 6 7 8 9 Transaction *transaction_; /** The datbase catalog associated with this executor context */ Catalog *catalog_; /** The buffer pool manager associated with this executor context */ BufferPoolManager *bpm_; /** The transaction manager associated with this executor context */ TransactionManager *txn_mgr_; /** The lock manager associated with this executor context */ LockManager *lock_mgr_; Task2 这里设计的有点绕，主要的相关类为：\n​AggregationPlanNode​、SimpleAggregationHashTable​、AggregationExecutor​，一个个来看\nAggregationPlanNode 包含了整个聚合操作的逻辑，通过对SQL的解析而来，包括进行哪些聚合计算，通过什么字段进行group by等。并且由于进行group by支持多个字段，以及一次可能传入多个算子，因此均为数组的形式：\n1 2 3 4 5 6 /** The GROUP BY expressions */ std::vector\u0026lt;AbstractExpressionRef\u0026gt; group_bys_; /** The aggregation expressions */ std::vector\u0026lt;AbstractExpressionRef\u0026gt; aggregates_; /** The aggregation types */ std::vector\u0026lt;AggregationType\u0026gt; agg_types_; 1 2 3 4 query select count(*), min(v1), max(v1), count(v1), sum(v1) from t1; ---- 6 -99999 99999 6 6 以上的一条sql解析完之后就是group_bys为空，arggegates,agg_types分别有5个参数。\n并且，只有参与Aggregation的列才会在生成plan等时候被加入到这几个数组当中，如下sql当中的v4,v5,v4+v5等均不会参与到Aggregation的过程，也不会被添加到数组当中，因此最终v4 v5会存在于group_bys当中，sum、min、count则会存在于aggregates和agg_types当中。\n1 select v4, v5, v4+v5, sum(v1+v2), min(v3+v4), count(*) from t1 group by v4, v5; SimpleAggregationHashTable SimpleAggregationHashTable在内部维护一个std::unordered_map，key为ArrgegateKey，val为一个ArrgegateVal，二者本质上分别为一个vector的简单封装，与6.830不同的是，这里的group by支持多个字段，对于多个字段，可以视为组合成一个联合索引那样。数组大小为算子的数量，存储在这个groupby字段下的所有算子的计算结果。定义分别如下:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 struct AggregateKey { /** The group-by values */ std::vector\u0026lt;Value\u0026gt; group_bys_; /** * Compares two aggregate keys for equality. * @param other the other aggregate key to be compared with * @return `true` if both aggregate keys have equivalent group-by expressions, `false` otherwise */ auto operator==(const AggregateKey \u0026amp;other) const -\u0026gt; bool { for (uint32_t i = 0; i \u0026lt; other.group_bys_.size(); i++) { if (group_bys_[i].CompareEquals(other.group_bys_[i]) != CmpBool::CmpTrue) { return false; } } return true; } }; /** AggregateValue represents a value for each of the running aggregates */ struct AggregateValue { /** The aggregate values */ std::vector\u0026lt;Value\u0026gt; aggregates_; }; 1 2 3 4 5 std::unordered_map\u0026lt;AggregateKey, AggregateValue\u0026gt; ht_{}; /** The aggregate expressions that we have */ const std::vector\u0026lt;AbstractExpressionRef\u0026gt; \u0026amp;agg_exprs_; /** The types of aggregations that we have */ const std::vector\u0026lt;AggregationType\u0026gt; \u0026amp;agg_types_; 插入​\n通过查询表得到的最终结果为一个Tuple，而HashTable当中为AggregateKey,AggregateVal，因此需要将tuple进行转换，\nkey为groupby的字段，如果不需要groupby，那么就使用一个空的数组作为key，之后所有的val都会在这个唯一的key上计算。否则则根据提供的group by字段去tuple当中进行提取，生成一个key。\n而对于Value则根据算子所需进行获取即可，如上面的那条Sql，最终vals数组当中所存储的为：[1,val(tuple1),val(tuple1),1(or null),val(tuple1)]​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 auto MakeAggregateKey(const Tuple *tuple) -\u0026gt; AggregateKey { std::vector\u0026lt;Value\u0026gt; keys; for (const auto \u0026amp;expr : plan_-\u0026gt;GetGroupBys()) { keys.emplace_back(expr-\u0026gt;Evaluate(tuple, child_-\u0026gt;GetOutputSchema())); } return {keys}; } /** @return The tuple as an AggregateValue */ auto MakeAggregateValue(const Tuple *tuple) -\u0026gt; AggregateValue { std::vector\u0026lt;Value\u0026gt; vals; for (const auto \u0026amp;expr : plan_-\u0026gt;GetAggregates()) { vals.emplace_back(expr-\u0026gt;Evaluate(tuple, child_-\u0026gt;GetOutputSchema())); } return {vals}; } 在生成了对应的key，val之后，通过InsertCombine以及CombineAggregateVlues进行插入。在其中找到对应的groupby字段之后，将所有的算子在原本的基础上进行计算。\n1 2 3 4 5 6 void InsertCombine(const AggregateKey \u0026amp;agg_key, const AggregateValue \u0026amp;agg_val) { if (ht_.count(agg_key) == 0) { ht_.insert({agg_key, GenerateInitialAggregateValue()}); } CombineAggregateValues(\u0026amp;ht_[agg_key], agg_val); } Iterator：为了将计算结果进行导出成Tuple，ht还提供了一个迭代器，可以用其对key和val分别进行迭代\nAggregateExecutor 在大致结构上和其他算子基本相同，都有一个由sql解析出来的plan，和一个用于读取数据的child_iterator，此外还有用于存储聚合结果的hashtable和用于导出聚合结果的迭代器.\n1 2 3 4 5 6 7 const AggregationPlanNode *plan_; /** The child executor that produces tuples over which the aggregation is computed */ std::unique_ptr\u0026lt;AbstractExecutor\u0026gt; child_; /** Simple aggregation hash table */ SimpleAggregationHashTable aht_; /** Simple aggregation hash table iterator */ SimpleAggregationHashTable::Iterator aht_iterator_; 与其他的算子不同的是，Aggregation是 pipeline breaker，即其他的火山模型的算子，通过next来获取到一条数据之后，立即向上返回，即自始至终该算子只会拥有或者处理一条tuple，而Aggregation不同的是，他通过child_terator的next获取到所有的数据，当计算完成之后，在通过自身的Next去将结果一条条的向上返回，但是这里依旧是火山模型，只是做了一次截断，并不会像物化模型那样一次性的返回多个tuple。\n因此就需要在Init当中迭代完child_iterator当中的所有的tuple，计算完成之后再通过Next将结果一条条向上返回。\nNext​\n正如上面所说，Next的作用就是通过迭代器，迭代上述存储聚合结果的hashtable，根据数据来一条条生成tuple。分别对key val进行迭代，即groupby和 aggregates，将其作为tuple的一列，插入到一个数组当中，之后根据该数组来构建tuple。\nNestedLoopJoin 这里实现上和6.830的差不多，主要有三个问题：\nduplicated key 需要注意的是不要漏匹配即可,如下表：如果直接双层while嵌套的话，在完成了T1的1 和T2的第一个1匹配完成并返回之后，上层再调用Next，就会调用T1.Next，就会令T1迭代到2，导致T1和T2的第二1匹配遗失，换句话说就是无法处理重复的key\nt1 join t2 on t1.colA = t2.colA​\nT1.colA T2.colA 0 0 1 1 2 1 因此为了防止每次调用Join的Next左表都会向下遍历一次，采用一个变量去暂存一下左表当前的tuple，之后当右表完全遍历完一次之后，左表才会通过Next移动至下一条tuple。\n大致结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 while (true) { if (!has_left_next_) { return false; } for (uint32_t flag = right_idx_ \u0026lt; 0 ? 0 : right_idx_; flag \u0026lt; right_tuple_vec_.size(); flag++) { // scan right tuple return true; } if (right_idx_ == -1 \u0026amp;\u0026amp; plan_-\u0026gt;GetJoinType() == JoinType::LEFT) { // generate a tuple with null to handle left join has_left_next_ = left_executor_-\u0026gt;Next(\u0026amp;last_left_tuple_, \u0026amp;left_rid); return true; } right_idx_ = -1; has_left_next_ = left_executor_-\u0026gt;Next(\u0026amp;last_left_tuple_, \u0026amp;left_rid); } 右迭代器 在NestedLoopJoin当中，左表当中的每一条数据都会遍历右表来寻找能够匹配的tuple，因此会导致右表的迭代器不断耗尽。最偷懒的方式为当右表的while迭代完成一次之后就调用其Init()​进行重置，不过这样后续又会去磁盘当中读取，造成不必要的io，因此更好的方式为通过一个数组进行暂存，之后遍历这个缓存下来的数组即可，可以减少多次IO。\nleft join BusTub分别支持Inner Join和Left Join，如果左表的一个tuple在右表当中完全没有匹配的话，通过在右表的位置填充null，生成一条数据。\nNestedIndexJoin 和NestedLoopJoin稍有不同，主要是数据的来源，由于驱动表一定要遍历，因此选择没有索引的表作为驱动表，右表的数据获取方式通过索引来实现。由于BusTub的索引不支持重复的key，因此也不需要一个变量来保存左表的tuple防止返回迭代了。如果通过索引能找到，那么就创建一条完整的记录，如果索引当中找不到并且当前的类型为left join，那么就创建一条记录，用null对右值进行填充。大致结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 while (child_executor_-\u0026gt;Next(\u0026amp;left_tuple, \u0026amp;useless_rid)) { std::vector\u0026lt;RID\u0026gt; index_rid; Value value = plan_-\u0026gt;KeyPredicate()-\u0026gt;Evaluate(\u0026amp;left_tuple, child_executor_-\u0026gt;GetOutputSchema()); tree_-\u0026gt;ScanKey(Tuple{{value}, index_info_-\u0026gt;index_-\u0026gt;GetKeySchema()}, \u0026amp;index_rid, exec_ctx_-\u0026gt;GetTransaction()); if (!index_rid.empty()) { // found by the index // generate a tuple return true; } // can not find by index,handle the left join case if (index_rid.empty() \u0026amp;\u0026amp; plan_-\u0026gt;GetJoinType() == JoinType::LEFT) { // generate tuple with null return true; } } HashJoin 在Lab当中并没有要求实现HashJoin，但是个人感觉比较有意思，再加上单纯在内存当中的HashJoin实现起来并不困难，就给实现了一下\n通过一个HashTable来存储右表当中的tuple，而由于join key可能重复，因此HashTable应当以hash为key，数组为value，数组当中存储有相同join key的Tuple。\n1 2 3 4 5 6 const HashJoinPlanNode *plan_; std::unordered_map\u0026lt;hash_t, std::vector\u0026lt;Tuple\u0026gt;\u0026gt; join_hash_; std::unique_ptr\u0026lt;AbstractExecutor\u0026gt; left_executor_; std::unique_ptr\u0026lt;AbstractExecutor\u0026gt; right_executor_; std::vector\u0026lt;Tuple\u0026gt; result_tuple_; std::vector\u0026lt;Tuple\u0026gt;::const_iterator tuple_iter_; 实现上的大致思路就是首先遍历右表，将tuple存储到HashTable当中。之后再遍历左表，去和右表进行匹配，此时相当于在右表上建立了一个HashIndex，如果匹配成功则将其记录到result_tuple数组当中，如果匹配失败，则去考虑是否为LeftJoin，如果是则添加一条右表为空的记录，先遍历右表的原因也是用于处理LeftJoin\n在进行匹配时，需要处理哈希冲突的问题，第一次忘考虑了，出现了9和8300被哈希到了同一个槽当中，之后误匹配上了，解决办法也比较简单，在通过哈希值找到对应的key之后在join前判断一次是否相等即可。\n1 if (right_join_key.CompareEquals(join_key) == CmpBool::CmpTrue) 之后在Next当中通过迭代器去访问result_tuple即可\n使用HashJoin进行优化之后，Leaderboard bonus的Q1执行会快上很多，如果不进行优化的话为30000，使用HashJoin可以到80多\nTask3 分别实现sort​ limit​ topN​三个算子以及topn对应的优化，前两个没什么好说，SeqScan修修补补即可。\nTop-N Top-N主要针对的为sort+limit的情况，即order by xxx limit n​，这种情况如果不进行优化则需要全表遍历并排序，之后再对排序完的结果进行一次计数截取。\n通过Top-N即可保证只有一次全表遍历，计算完之后逐条返回。\n根据lab的提示，很容易就可以确立思路，维护一个优先队列，并且保证当中只有K个数据，即通过Next不断获取并插入到其中，如果达到了K个就弹出一个，保证至多有K个\nThink of what data structure can be used to track the top n elements (Andy mentioned it in the lecture). The struct should hold at most k​ elements (where k​ is the number specified in LIMIT​ clause).\n不过由于std::priority_queue​当中并没有提供迭代器，也没有提供下标访问，而top访问的是排序最低的那个，因此可以使用一个栈，将整个逻辑进行翻转，从而保证从高向低的进行获取。\nTop-N Optimizer 1 2 3 4 5 6 7 8 9 10 11 auto Optimizer::Optimize(const AbstractPlanNodeRef \u0026amp;plan) -\u0026gt; AbstractPlanNodeRef { if (force_starter_rule_) { // Use starter rules when `force_starter_rule_` is set to true. auto p = plan; p = OptimizeMergeProjection(p); p = OptimizeMergeFilterNLJ(p); p = OptimizeNLJAsIndexJoin(p); p = OptimizeOrderByAsIndexScan(p); p = OptimizeSortLimitAsTopN(p); return p; } 将原本未经优化的plan树通过多条规则链式的进行优化，最后得到一个最终的优化结果。对于单个的优化方案，都是采用后续遍历的方式，自底向上的改写plan，将符合的类型进行优化。\n因此照猫画虎的实现一个，通过Top-N对sort+limit的情况进行优化，只有上层为limit，下层为sort的情况，才将其优化为Top-N。\nSummary 总的来说Lab3的难度并不大，并且在本地提供了全部的测试用例，而且不涉及并发，因此可以安心的单步调试，写起来也就比较的粗放一点，面向bug编程。\n整个Lab3做下来给我的感觉是相比于6.830，bustub更注重于深度，在很多地方都提供提供了一定的封装，如磁盘的读写，tuple的读取和写入。虽然少了自行构造table和tuple的过程，对于schema的理解可能差一点，但是在有限的时间内更注重于深度。此外，由于SQL执行这一部分设置在了索引之后，因此在功能的实现上也可以应用索引，如NestedIndexJoin、IndexScan等。在做6.830的基础之上写busTub的话个人感觉会更有收获一点，但是如果只写一个的话，BusTub可能更合适一点，不过需要去阅读一下其他部分的源码，或者像某些佬那样直接自己实现一个BusTub。\n最近手头事有点多，也快考试了，整个文章写的比较仓促，Leaderborad bonus暂时也没时间写了，只能先搁置在一边了。\n","permalink":"http://itfischer.space/en/posts/tech/bustub/bustub-lab3/","summary":"Lab3 Task1 火山模型 在Task1当中实现了SeqScan​ Insert​ Delete​ IndexScan​，由于均为火山模型，因此在实现上大同小异，","title":"BusTub Lab3 Query Execution"},{"content":"checkpoint2 Task3 在支持并发前没什么好说的，一个简单的迭代器。根据begin的条件找到一个起始页，之后在该页内遍历即可，当遍历完该页之后，根据nextPageId找到下一页继续遍历即可。\nUnpin\n最初通过FindLeaf找到一个page，对其进行了fetch，此时是对其pin住了的，当遍历完该page之后应该对该page进行unpin，当迭代器析构时，应当对当前页unpin。\n并发这里先不说，留在Task4当中进行实现。\nTask4 个人感觉相比于checkpoint1来说，其实并不难，在细节的处理上并没有checkpoint1当中复杂，尤其是delete操作，个人在delete上花费了大量时间进行理解和debug，并且在checkpoint1的测试数据并不完善，根据群友扒下来或者自己复刻的结果来看，对于delete的测试只有两个，并且一个单元测试当中只插入了五条数据进行测试，测试强度很低，这也导致很多问题在checkpoint1当中并没有暴露出来，最终混在了checkpoint2当中。\n并发 对B+树支持并发，使用所谓的螃蟹锁，只有在子节点安全的情况下才能够释放掉位于父节点上的锁，对于安全的情况根据操作而定：\nSearch​：从父节点上获取到R锁，之后遍历子节点，只要在子节点上获取到锁之后，就可以认为当前已经安全，之后便可以释放掉位于父节点上的锁。 Insert​：从根节点开始，尝试在子节点上获取W锁，一旦在孩子节点上获取锁成功，那么就检查是否安全，对于插入情况而言，如果插入后不产生分裂则视为安全，如果安全，那么就是释放所有祖先的锁 Delete​：从根节点开始，尝试在子节点上获取W锁，一旦锁定，检查是否安全，对于删除情况而言，为至少半满（对于根节点需要按照不同的标准的检查），一旦孩子节点安全，释放所有祖先的锁。 在并发的实现上，主要涉及两点,一个是通过Latch Crabbing在FindLeaf的途中一遍获取一遍释放锁，保证并发安全的同时提高并发度（此外还有delete时对兄弟节点加锁）。另一个则是在合适的位置释放掉和当前事务相关的锁。\n锁的形式 首先，所有的Page都是通过buffer pool进行获取的，只有通过buffer pool获取到了Page之后，才可以进行后续的操作，而从buffer_pool当中获取到的最初即为原始的Page，而不是对内存重新解释而来的BPlusTreePage，而在Page的头文件当中也给出了读写的latch，因此，对于单个Page的上锁，使用Page当中定义的ReaderWriterLatch​即可。\nroot_page\nroot_page同样是通过buffer_pool来获取并之后进行上锁的，正常情况下不需要进行额外的操作，但是如果一颗树根本不存在，那么这时就没有办法去获取root_page并且进行上锁，因为无法对一个不存在的page进行上锁，个人感觉这里的处理方式就有点类似于幻读的间隙锁，由于无法对一个不存在的page进行上锁，那么就对其间隙进行上锁，这里的间隙指的就是树的开头，在root_page的上面虚构一个节点，在获取root_page之前先获取该节点，当能够对root_page上锁之后就释放掉该锁。\ntransaction\n对Page访问的基本单位为transaction，因此transaction需要对于自己当前获取的锁进行保存和管理，这在transaction当中已经给了定义，通过GetPageSet()即可获取到管理当前相关page的队列。\n并且获取锁时为按照自上向下的顺序来获取锁，释放时同样按照自顶向下的顺序进行，并且由于顶上的竞争较为激烈，提前释放也有利于提高并发度。\n该队列只用于管理内部节点上的锁，对于进行插入或删除的叶子节点不进行管理，单独释放叶子节点上的锁。\nLatch Crabbing 所谓的螃蟹锁，通过提前释放锁的形式来增加并发度。大致含义上为从根节点开始，先获取一个节点的锁，之后再尝试获取其子节点的锁，当获取到了子节点的锁之后，如果此次操作对于子节点来说是安全的，那么就可以释放掉该节点上的锁。就像螃蟹那样，放下一只脚往前走（获取子节点的锁），在抬起另一只脚（释放当前节点上的锁）\n对于是否安全需要根据读写的形式进行判断：\n对于读操作，并不存在什么安全不安全的问题，只要能够获取到子节点的锁，那么就是安全的，此时就可以释放掉当前节点上的锁 对于插入操作：只有当执行到最后一步找到叶子节点之后才能够知道是否会引发分裂，以及分裂是否会向上作用。但是，在遍历到内部节点时，可以知道是，如果发生了分裂，当前节点是否会发生分裂。即在遍历到过程中，获取到子节点的锁之后，假设子节点发生了分裂，来判断子节点的分裂导致的InsertIntoParent​是否会引发自身的分裂，如果不会，那么就可以释放掉自身的锁，否则继续持有。 对于删除操作：逻辑上和插入操作差不多，只不过是否安全的判断标准为是否引起收缩。 对于Latch Crabbing，其虽然无法保证在持续占有锁的时候，后续一定会用到该锁，即对于一个五阶B+树，当前的内部节点，已经含有了4个key，5个value，此时根据条件应当继续持有锁，但是最终的子节点当中只有一对kv，因此不会发生分裂，此时的持有锁就属于白白持有了，后续不会用到，虽然这在情况对于并发度有一定影响，但是可以保证所有情况下的正确性。\n在实现上主要是在FindLeaf当中，首先，FindLeaf调用时是从根节点进行搜索的，因此首先需要获取到root_page上的锁，并且判断是否安全，从而释放掉root上的锁。\n之后，则循环找到叶子节点，反复的获取子节点的锁，然后判断自身是否安全，从而选择是否释放锁。\n这里主要需要注意一下删除操作的根节点，在Lab的页面上也给出了相关提示，根据定义，一个根节点最少有两个子节点，反过来的意思就是如果只有两个字节点的情况下，如果一个节点被删除掉，那么就可能触发AdjustRoot()，因此对于删除情况下的根节点的安全条件是子节点数大于3。\n下图中如果删除节点3则会引发AdjustRoot()​，因此不能释放锁。\n​\n​\n​\n锁的释放 无论是GetValue​,Insert​还是Remove​，都需要依赖FindLeaf来找到对应的leaf_page，之后进行具体的操作。因此调用完FindLeaf找到leaf_page之后，当前的事务是持有者该leaf_page的锁的，如果不安全的话，还有可能持有祖先节点的锁。因此就需要在合适的时机释放掉leaf_page上的锁和所有祖先节点上的锁，大致逻辑就是在Search、Insert、Remove三个函数FindLeaf之后、返回之前找一个合理的位置进行一次锁的释放，释放掉叶子节点和\nSearch 对于读操作，实现起来较为简单，直接使用螃蟹锁即可，其不存在什么不安全的情况，因此只需要当获取到孩子节点的锁之后，就可以释放掉父亲节点上的锁。锁的类型为读锁。\n因此只需要在GetValue返回前释放掉叶子节点上的锁即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::GetValue(const KeyType \u0026amp;key, std::vector\u0026lt;ValueType\u0026gt; *result, Transaction *transaction) -\u0026gt; bool { root_page_latch_.RLock(); auto leaf_page = FindLeaf(key, Operation::SEARCH, transaction); auto *node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(leaf_page-\u0026gt;GetData()); ValueType v; auto is_existed = node-\u0026gt;LookUp(key, \u0026amp;v, comparator_); buffer_pool_manager_-\u0026gt;UnpinPage(leaf_page-\u0026gt;GetPageId(), false); leaf_page-\u0026gt;RUnlatch(); if (is_existed) { result-\u0026gt;push_back(v); return true; } return false; } Insert Insert则涉及到一个安全问题，因此按照latch crabbing的策略，每次获取到锁之后都要尝试判断一次是否安全，即判断在子节点当中插入之后是否会产生分裂，如果不会产生，那么就可以释放掉之前所有的锁：\n对于内部节点，则判断size \u0026lt; maxSize 对于叶子节点，则判断size \u0026lt; maxSize - 1 在释放锁时需要释放当前节点以及当前节点所有的祖先节点的锁，即将transaction的page队列当中的所有的page上的锁全部释放，并且注意根节点的处理。\n相关函数：\nInsert：入口函数：锁住root_page_latch StartNewTree：创建一个新的root_page并向其中添加数据，已经在Insert当中上锁，无需操作 InsertIntoLeaf：已经存在一棵树，找到叶子节点并插入数据，调用FindLeaf找到叶子节点，此时还未释放叶子节点上的W锁，并且可能持有祖先节点上的锁，如果不涉及到分裂，W锁的释放和UnpinPage一同进行，在返回前释放。 InsertIntoParent：最初为LeafPage发生分裂而调用，调用时旧节点上的锁并未释放，整个过程中保持持有锁即可，等到返回之后再释放锁。 综上，对于Insert操作只需要才StartNewTree和InsertIntoLeaf执行结束后释放锁即可，当时为了将资源释放的逻辑进行统一，将释放锁的操作和InsertIntoLeaf当中和Unpin放在了一起。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::InsertIntoLeaf(const KeyType \u0026amp;key, const ValueType \u0026amp;val, Transaction *transaction) -\u0026gt; bool { auto leaf_page = FindLeaf(key, Operation::INSERT, transaction); // hold the latch of leaf_page and unsafe internal page auto *node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(leaf_page-\u0026gt;GetData()); auto size = node-\u0026gt;GetSize(); auto new_size = node-\u0026gt;Insert(key, val, comparator_); if (size == new_size) { // duplicate key leaf_page-\u0026gt;WUnlatch(); ReleaseLatchFromQueue(transaction); buffer_pool_manager_-\u0026gt;UnpinPage(leaf_page-\u0026gt;GetPageId(), false); return false; } if (new_size \u0026lt; leaf_max_size_) { leaf_page-\u0026gt;WUnlatch(); ReleaseLatchFromQueue(transaction); buffer_pool_manager_-\u0026gt;UnpinPage(leaf_page-\u0026gt;GetPageId(), true); return true; } // reach the maxsize after insert auto sibling_new_node = Split(node); sibling_new_node-\u0026gt;SetNextPageId(node-\u0026gt;GetNextPageId()); node-\u0026gt;SetNextPageId(sibling_new_node-\u0026gt;GetPageId()); auto first_key = sibling_new_node-\u0026gt;KeyAt(0); InsertIntoParent(node, first_key, sibling_new_node, transaction); ReleaseLatchFromQueue(transaction); leaf_page-\u0026gt;WUnlatch(); buffer_pool_manager_-\u0026gt;UnpinPage(sibling_new_node-\u0026gt;GetPageId(), true); buffer_pool_manager_-\u0026gt;UnpinPage(node-\u0026gt;GetPageId(), true); return true; } Remove 和Insert同理，安全条件为不产生合并或者窃取，即删除完至少为半满的状态，条件为 size \u0026gt; MinSize\n相关函数：\nRemove：入口函数，和Insert一样的处理方式，锁住root_page_latch\n调用FindLeaf找到对应的叶子节点，此时仍持有叶子节点的锁，\n如果尝试删除失败，则直接释放叶子节点上的锁，返回\nDeleteEntry：\nAdjustRoot：执行完Remove AdjustRoot函数之后，返回时释放锁即可 无论是Redistribute还是Coalesce，都需要获取到兄弟节点，因此需要对兄弟节点进行加锁 因此，我采用了一种比较无脑的实现方式。即当通过FindLeaf加锁之后，除了之后对兄弟节点进行加锁，其他情况一律不进行加锁，释放锁的操作全部统一到Remove当中，只需要在Remove函数的几次返回时释放锁，对于Delete函数当中，不进行任何解锁操作，Remove函数的大致结构如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 void BPLUSTREE_TYPE::Remove(const KeyType \u0026amp;key, Transaction *transaction) { root_page_latch_.WLock(); transaction-\u0026gt;AddIntoPageSet(nullptr); if (IsEmpty()) { ReleaseLatchFromQueue(transaction); return; } // hold the lock of leaf page and unsafe internal page auto leaf_page = FindLeaf(key, Operation::DELETE, transaction); auto *node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(leaf_page-\u0026gt;GetData()); if (node-\u0026gt;IsLeafPage()) { auto before_size = node-\u0026gt;GetSize(); auto size = node-\u0026gt;Delete(key, comparator_); if (before_size == size) { // cannot find the key; ReleaseLatchFromQueue(transaction); leaf_page-\u0026gt;WUnlatch(); buffer_pool_manager_-\u0026gt;UnpinPage(leaf_page-\u0026gt;GetPageId(), false); return; } } auto node_should_delete = DeleteEntry(node, transaction); ReleaseLatchFromQueue(transaction); leaf_page-\u0026gt;WUnlatch(); if (node_should_delete) { transaction-\u0026gt;AddIntoDeletedPageSet(node-\u0026gt;GetPageId()); } buffer_pool_manager_-\u0026gt;UnpinPage(leaf_page-\u0026gt;GetPageId(), true); std::for_each(transaction-\u0026gt;GetDeletedPageSet()-\u0026gt;begin(), transaction-\u0026gt;GetDeletedPageSet()-\u0026gt;end(), [\u0026amp;bpm = buffer_pool_manager_](const page_id_t page_id) { bpm-\u0026gt;DeletePage(page_id); }); transaction-\u0026gt;GetDeletedPageSet()-\u0026gt;clear(); } UnpinPage 原来在FindLeaf时，一旦遍历至子节点时，当前节点目前就不再被使用，此时就可以对其进行unpin，之所以说目前，是因为后续如果涉及到合并或者重分配到情况，会递归的向上重新获取节点，此时又会在此用到该Page。当时做checkpoint1的时候选择的策略是默认后续不会用到，允许进行淘汰，后续如果需要并且被淘汰了，大不了就再次从磁盘加载，也没什么大不了的。\n不过既然实现了并发之后，那么不如将逻辑进行一下统一，如果不安全，那么就既不释放锁，也不进行unpin，等待后续重新使用。封装成一个函数\nThink carefully about the order and relationship between UnpinPage(page_id, is_dirty)​ method from buffer pool manager class and UnLock()​ methods from page class. You have to release the latch on that page before you unpin the same page from the buffer pool. 并且，根据Lab的提示，需要注意释放锁的顺序和unpind的顺序，这里的锁是加在buffer pool的page上的，如果一个page还未释放掉锁就先unpin，那么之后就有可能被Evict掉，而之后如果需要用到该Page时，再通过FetchPage从磁盘加载，调用构造函数，锁的相关信息就会丢失。\n1 2 3 4 5 6 7 8 9 10 11 12 13 INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::ReleaseLatchFromQueue(Transaction *transaction) -\u0026gt; void { while (!transaction-\u0026gt;GetPageSet()-\u0026gt;empty()) { Page *page = transaction-\u0026gt;GetPageSet()-\u0026gt;front(); transaction-\u0026gt;GetPageSet()-\u0026gt;pop_front(); if (page == nullptr) { root_page_latch_.WUnlock(); } else { page-\u0026gt;WUnlatch(); buffer_pool_manager_-\u0026gt;UnpinPage(page-\u0026gt;GetPageId(), false); } } } 除了FindLeaf的FetchPage Unpin之外，其他的就按照checkpoint1当中的实现进行即可，即在Delete的过程当中，对于递归需要的parent page，和合并、重分配需要的sibling page，在哪里Fetch了就在该函数当中进行Unpin即可。\nFetchPage和UnpinPage有点像一种资源的申请和释放，可以按照RAII的那种思想，令申请和释放都在同一个作用域内。\nIterator 在Begin当中，通过Findleaf获取到一个leaf_page的锁，之后这个Iterator就一直持有该锁，等到通过++获取到下一个page时，再获取下一个page上的锁，并释放当前page上的锁。通过这种方案确实是能够通过Lab2的测试，但是个人感觉会引起死锁：一个线程正在处理叶子节点的重分配，当前已经获取到了右节点上的锁，正要尝试获取左节点上的锁，而另外一个线程正在执行从左向右的遍历操作，此时获取到了左节点的锁，尝试获取右节点的锁，此时就会形成死锁。这里的处理方式应当为令迭代器进行尝试，如果无法获取锁，那么就放弃获取，并释放掉自身的锁，从而破除死锁。\nDebug 多线程debug还是相当头疼的，不过由于我在锁的处理上实现的较为简单，通过FindLeaf获取到锁之后，在Insert和Remove当中只需要在几个地方释放掉锁就可以了，最开始也遇到了一些死锁问题，不过大多都是leaf_page最后忘在正确的地方释放了，通过单线程的单步调试就可以找到卡在了哪里，最终将死锁解决即可。后续再就没有遇见并发安全的问题了，也不存在死锁。个人比较幸运没有出现并发安全的问题，整个过程当中也就没有去一点点读log进行多线程debug。\n我个人主要的Bug是存在于checkpoint1当中的delelte的细节没有处理好，checkpoint1在gradescope上的测试并不完善，尤其是对于删除的测试，只进行过最基础的单节点的删除测试，因此在做checkpoint2时存在很多历史遗留问题，最直观的表现为ScaleTest无法通过，即对一个3阶的B+树，进行1w次插入、读取、删除。最后判断是否为空。对于3阶的B+树，插入过程中的分裂非常频繁，删除过程中的合并和重分配也同理。因此如果1w次存在问题，那么就说明B+树的逻辑处理一定存在问题，之后就可以将次数改为10次，运行几次基本上就可以复现1个bug，10次插入删除处理起来也比较方便，通过提供的画图工具把树画出来即可，分析哪里出现了问题。\n最后附一个通过记录和Leaderboard\n","permalink":"http://itfischer.space/en/posts/tech/bustub/busttub-lab2-c2/","summary":"checkpoint2 Task3 在支持并发前没什么好说的，一个简单的迭代器。根据begin的条件找到一个起始页，之后在该页内遍历即可，当遍历完该页之后，根据nextPa","title":"BusTub Lab2 B+Tree Index checkpoint2"},{"content":"Lab2 对于Lab的debug，由于并没有开放测试样例，因此最好的就是找一个合适的b+树的模拟动画，然后再使用官方的画图工具比较自己的B+树，一般3-4层没有问题了大多就是没有问题了，把分裂、合并、重分配等各种情况全部涉及到一次基本就可以了。\nB+树演示动画推荐官方的，其他的可能多少都会有点bug或者实现方式不太一样\nhttps://15445.courses.cs.cmu.edu/fall2022/bpt-printer/\nTask1 分别实现 Parent Page Internal Page Leaf Page，按照注释实现即可，较为简单\n注意一下arrry_[1]的使用方式即可\nflexible array​\n这里可以参考十一大佬的解释，描述的非常详细：flexible array\narray_[1]所采用的是一种flexible array的实现方式，即不指定一个数据的大小，将其封装到一个对象当中，对整个对象分配一个确定的大小，除去已经使用的空间，剩余的空间均可为array_[1]所使用。\n对于一个Internal Page，BUSTUB_PAGE_SIZE​定义的为4KB，即4096B，除去从父类继承而来的6个字段各占4B，其他剩余的所有内存均供flexible array来使用\nPage and BPlusTreePage​\n就像上面的图画的那样，这二者之间并不存在继承或者实现的关系，二者在类的定义上为并列的关系，并不像是在6.830的simpledb 当中通过BPlusTreePage去实现Page接口。\nB+树的一个Page作为索引而存在，而叶子节点即是索引又是真实的存储单元（即存储Tuple对应的RID，非聚簇索引实现）。\n即当要读取一个tuple时，首先根据保存的root_page_id去找到根节点，通过buffer_pool_manager将其加载到内存当中，而root_page是作为Page的数据维护在内存当中，Page当中的data_​为一个4KB的字节数组，通过reinterpret_cast​将其重新解释为BPlusTreePage，后续在其中搜索到对应的Key，在通过buffer_pool_manager去找到下一个节点，直至最后找到叶子节点，并通过RID再去读取到tuple。\n而对于HeapFile的实现方式，则是找到一个表的HeapFile，然后遍历不断读取其中的HeapPage，通过bufffer_pool_manager将其加载到内存当中，再遍历HeapPage当中的所有的tuple，直至找到对应的tuple。\n节点实现 节点大小 先说一个数据库系统概念当中对大小的定义：\n首先对于内部节点和叶子节点的大小都基于Max Degree n\n对于叶子节点max_size为n-1个kv，min_size为(n-1)/2向上取整 对于内部节点max_size为n个指针，n-1个k，min_size为n/2向上取整个指针​ 在Lab当中，对于一棵B+树，并没有制定Max Degree，而是分成leaf_max_size和internal_max_size来定义的，可以分开设置但是二者理论上应该是相等的：在是否需要分裂的判断时，Lab给出的逻辑是：internal_max_size在插入前进行判断，leaf_max_size在插入后进行判断，因此即可保证内部节点可以比叶子节点多容纳一个指针。\n因此对于min_size的求法：\nleaf_node: n / 2 internal_node (n + 1) / 2 不减一直接除可以与-1之后向上取整等价。\n实现方式 在一个Page内部通过array_[1]​来保存所有的KV，array_[1]的元素类型为一个KV键值对，对于叶子节点的实现较为便捷，kv的数量相等，直接从头开始填充即可，在bustub所实现的为非聚簇索引，因此key即为设置索引的键，value即为RecordId。在通过索引找到了RecordId之后再使用RecordId在一个Page当中去进行偏移读取。\n如果想MySQL那样的话主键索引锁存储的为整个tuple，二级索引存储的则是索引和主键，通过二级索引进行查询，需要通过一个回表的过程。\n‍\n而对于internal_page的实现稍微复杂一点，由于指针比索引多一个，因此可以选择将第一个key标记为空，不存储任何东西，之后再更新索引的时候记得跳过0号位，从1开始。\n对于上图的0003 0005 0007则为：\nTask2 只支持唯一索引，当插入重复的key时需要不执行并返回false。\n在插入时需要进行最大值的检测：\n对于内部节点为插入前子节点的数量（指针的数量）等于max_size 对于叶子节点为插入后kv键值对的数量等于max_size 写操作可能会导致根节点的更新，使用updateRootPageId​进行更新，只要root_page_id需要更新时，就对其进行调用\nGetValue 按照伪代码来严格实现，按照上图的划分方法进行封装，分别定义FindLeaf()，用于从根节点找到对应的叶子节点，在这过程当中需要在一个节点内部去搜索，因此分别定义一个LoopUp和KeyIndex：\nKeyIndex：调用std::lower_bound去寻找第一个 \u0026gt;= 的值 LookUp：调用KeyIndex，根据返回的下表去判断是否获取到。 并且叶子节点和内部节点的搜索逻辑并不相同，因此需要分别定义。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 INDEX_TEMPLATE_ARGUMENTS auto B_PLUS_TREE_LEAF_PAGE_TYPE ::KeyIndex(const KeyType \u0026amp;key, const KeyComparator \u0026amp;comparator) const -\u0026gt; int { auto target = std::lower_bound(array_,array_ + GetSize(),key, [\u0026amp;comparator](const auto \u0026amp;pair,auto k) { return comparator(pair.first,k) \u0026lt; 0; }) ; return std::distance(array_,target); } INDEX_TEMPLATE_ARGUMENTS auto B_PLUS_TREE_LEAF_PAGE_TYPE::LookUp(const KeyType \u0026amp;key, ValueType *valueType, const KeyComparator \u0026amp;keyComparator) const -\u0026gt; bool { int target_in_array = KeyIndex(key,keyComparator); if (target_in_array == GetSize() || keyComparator(array_[target_in_array].first,key) != 0) { return false; } *valueType = array_[target_in_array].second; return true; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 INDEX_TEMPLATE_ARGUMENTS auto B_PLUS_TREE_INTERNAL_PAGE_TYPE::Lookup(const KeyType \u0026amp;key, const KeyComparator \u0026amp;comparator) const -\u0026gt; ValueType { // skip the empty key auto target = std::lower_bound(array_ + 1, array_ + GetSize(), key, [\u0026amp;comparator](const auto \u0026amp;pair, auto k) { return comparator(pair.first, k) \u0026lt; 0; }); // larger than any key,return the last one if (target == array_ + GetSize()) { return ValueAt(GetSize() - 1); } // return the key == target_key if (comparator(target-\u0026gt;first, key) == 0) { return target-\u0026gt;second; } // return the key \u0026lt; target_key return std::prev(target)-\u0026gt;second; } Insert 主要分为四个函数，分别为：\n主体insert 当树为空时调用stratNewTree创建第一个节点 insert_in_leaf insert_in_parent Insert insert本身并不实现什么具体逻辑，对StartNewTree和insert_in_leaf进行分类即可\n1 2 3 4 5 6 7 8 INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::Insert(const KeyType \u0026amp;key, const ValueType \u0026amp;value, Transaction *transaction) -\u0026gt; bool { if (IsEmpty()) { StartNewTree(key,value); return true; } return InsertIntoLeaf(key,value,transaction); } StartNewTree 创建一个全新的树，创建一个叶子节点将其作为根节点，并向其中插入一条数据\n1 2 3 4 5 6 7 8 9 10 11 INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::StartNewTree(const KeyType \u0026amp;key, const ValueType \u0026amp;val) -\u0026gt; void { auto page = buffer_pool_manager_-\u0026gt;NewPage(\u0026amp;root_page_id_); if (page == nullptr) { throw Exception(ExceptionType::OUT_OF_MEMORY,\u0026#34;cannot allocate new page in StartNewPage\u0026#34;); } auto *leaf = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(page-\u0026gt;GetData()); leaf-\u0026gt;Init(root_page_id_,INVALID_PAGE_ID,leaf_max_size_); leaf-\u0026gt;Insert(key,val,comparator_); buffer_pool_manager_-\u0026gt;UnpinPage(page-\u0026gt;GetPageId(), true); } Insert封装在Leaf_page当中，找到一个合适的位置向其中插入，并且对于重复的key，拒绝插入。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 INDEX_TEMPLATE_ARGUMENTS auto B_PLUS_TREE_LEAF_PAGE_TYPE::Insert(const KeyType \u0026amp;key, const ValueType \u0026amp;val,const KeyComparator \u0026amp;keyComparator) -\u0026gt; int { auto distance = KeyIndex(key,keyComparator); // larger than any key if (distance == GetSize()) { *(array_ + distance) = std::make_pair(key,val); IncreaseSize(1); return GetSize(); } // duplicated key if (keyComparator(array_[distance].first,key) == 0) { return GetSize(); } std::move_backward(array_ + distance,array_ + GetSize(),array_ +GetSize() + 1); *(array_ + distance) = std::make_pair(key,val); IncreaseSize(1); return GetSize(); } Insert_in_leaf 对于不需要创建新树的情况，一律归类于向叶子节点当中插入数据\n先通过GetValue当中实现的FindLeaf找到对应的叶子节点，之后直接向其中插入，如果叶子节点的大小没有发生变化，那么则证明为重复的key，直接返回即可。\n如果大小发生变化但是没有超出最大值，无需进行分裂，也直接返回即可。\n剩余的情况则是需要进行分裂。\n叶子节点分裂​\n首先从buffer pool当中获取出一个新的Page用于承载分裂得来的新page 原本的节点保留minSize（maxSize / 2 )个数据，其他的全部拷贝至新的节点，最终的新节点当中的数据较多 当完成叶子节点的分裂之后，并将连接关系设置好之后，左右连接，parentId，之后再调用InsertIntoParent将右节点的第一个插入到父节点当中。\nInsertIntoParent 首先需要说明的是调用InsertIntoParent的时机一定是在叶子节点当中插入引发了分裂，之后将新分裂出的右节点的第一个key尝试插入到父亲节点当中\n对于伪代码当中的一条条来看：\n如果N为根节点：则创建新的根节点，insert_in_parent由叶子节点开始调用，而此时N为根节点则证明已经递归到对根节点进行分裂，N‘即为N的新兄弟节点，因此需要创建一个新的根节点，以N，N‘为子节点，\n对应以下的情况：K\u0026rsquo; = 0003\nP有不到n个指针，还未满，在父节点当中直接插入并结束，没什么好说的\n此时将6插入到父节点当中，父节点也无法容下，对父节点进行分裂，分裂完之后递归调用，在将5插入到上一级的父节点当中，此时可以容下5，因此不进行分裂，终止插入\n在整个插入过程当中一个比较重要的实现就是分裂，分裂的过程严格按照伪代码所给的进行实现，对于叶子节点和内部节点在分裂上的实现有所不同，因此定义一个Split来对叶子节点和内部节点的数据迁移和指针指向更改。\n创建一块内存块用于存放所有的之前的所有数据和要新插入的K\u0026rsquo; N\u0026rsquo;， 将旧节点的所有的数据都拷贝到其中，新插入的K‘N\u0026rsquo;也加入到其中 。 完成节点的分裂，前一个节点保存min_size个节点，后一个节点保存其他的所有节点 按照伪代码描述，前一个节点应当保存(n+1)/2向上取整个节点，因此如果是基数个的话，那么前一个节点会比后一个节点多一个，但是个人认为这个是无所谓的，因为两种实现方法都能够达到平衡，在网上看到的两个B+树的演示动画也采用了两种不同的实现方式。\n在分裂上的实现也不是很统一，由于对于叶子节点的分裂使用了先插入之后在判断的方式，因此直接分裂即可，对于内部节点需要在分裂前补上新的数据。伪代码当中的临时内存块其实不使用也可，直接在原节点上进行操作就行了。正确性上也可以得到保证(做完之后在网上看到了说直接插入会导致超出大小限制，但是我自己做下来按照直接插入的方式并没有出现什么问题)\n1 2 3 4 5 6 parent_node-\u0026gt;InsertAfterNode(old_node-\u0026gt;GetPageId(), key, new_node-\u0026gt;GetPageId()); auto parent_sibling_node = Split(parent_node); auto new_key = parent_sibling_node-\u0026gt;KeyAt(0); InsertIntoParent(parent_node, new_key, parent_sibling_node); buffer_pool_manager_-\u0026gt;UnpinPage(parent_page-\u0026gt;GetPageId(), true); buffer_pool_manager_-\u0026gt;UnpinPage(parent_sibling_node-\u0026gt;GetPageId(), true); Delete 相对于插入来说，删除的逻辑就复杂的多,函数拆分方式如上，还是一条条的看。\n下面的例子全以MaxDegree = 4，MaxSize = 4,MinSize = 2来进行讨论\n最开始通过FindLeaf() 找到Key所在的leaf_page，之后调用delete_entry进行删除，不过由于后续涉及到递归调用，因此可以把从leaf_page中删除的逻辑移到delete_entry外面\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 INDEX_TEMPLATE_ARGUMENTS void BPLUSTREE_TYPE::Remove(const KeyType \u0026amp;key, Transaction *transaction) { if (IsEmpty()) { return; } auto leaf_page = FindLeaf(key, transaction); auto *node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(leaf_page-\u0026gt;GetData()); if (node-\u0026gt;IsLeafPage()) { auto before_size = node-\u0026gt;GetSize(); auto size = node-\u0026gt;Delete(key, comparator_); if (before_size == size) { // cannot find the key; return; } } DeleteEntry(node, key); } AdjustRoot 一棵树的初始状态如下，此时如果需要从中删除掉Key = 3，叶子节点当中的key不够，并且能够与左节点合并而不达到MaxSize，因此进行合并\n合并结果如下，此时的3号节点当中只有一个指针，小于min_size，同样的与右节点合并之后不会超过max_size，因此合并\n合并的结果如下，并且合并之后会在递归调用一次delete_entry，此时传入的节点即为parent(N)，找到了根节点，并且此时的根节点只有一个孩子，达到了AdjustRoot的条件：\n最终AdjustRoot的结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::AdjustRoot(BPlusTreePage *old_root_node) -\u0026gt; bool { if (!old_root_node-\u0026gt;IsLeafPage() \u0026amp;\u0026amp; old_root_node-\u0026gt;GetSize() == 1) { auto *root_node = reinterpret_cast\u0026lt;InternalPage *\u0026gt;(old_root_node); auto only_child_page = buffer_pool_manager_-\u0026gt;FetchPage(root_node-\u0026gt;ValueAt(0)); auto only_child_node = reinterpret_cast\u0026lt;BPlusTreePage *\u0026gt;(only_child_page); only_child_node-\u0026gt;SetParentPageId(INVALID_PAGE_ID); root_page_id_ = only_child_node-\u0026gt;GetPageId(); UpdateRootPageId(0); buffer_pool_manager_-\u0026gt;DeletePage(old_root_node-\u0026gt;GetPageId()); return true; } return false; } Coalesce： 当两个节点合并之后仍小于MaxSize时，对两个节点进行合并，在逻辑上同样需要分为对叶子节点进行操作和对内部节点进行操作，由于在上述AdjustRoot的例子当中已经涉及到了根节点的合并和叶子节点的合并，因此就还用上面的例子\n叶子节点的合并\n初始树：\n将3进行删除，之后发现其左节点与之合并之后仍小于max_size，此时的N为自身，N\u0026rsquo;为其左节点，K\u0026rsquo;为3， 执行合并，将右叶子节点当中的所有KV移动到左节点当中 递归调用delete_entry，在父亲节点当中尝试删除掉K\u0026rsquo;(3) 递归完之后清除掉N 最终结果如下，而当前的3号page当中只有一个指针，因此是不稳定的，并且大小允许合并，执行内部节点的合并\n内部节点的合并​\n初始树，已经删除掉K之后\nN为自身节点，N\u0026rsquo;为其右节点，K\u0026rsquo;按照定义即为10，将N作为最终合并后的存储节点 因此先将K\u0026rsquo;添加到N当中，之后在将N\u0026rsquo;当中所有的KV添加到其中（不包括第一个空的K） 最终结果如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 auto BPlusTree\u0026lt;KeyType, ValueType, KeyComparator\u0026gt;::Coalesce( N *left_node, N *right_node, BPlusTreeInternalPage\u0026lt;KeyType, page_id_t, KeyComparator\u0026gt; *parent, int index) -\u0026gt; bool { auto middle_key = parent-\u0026gt;KeyAt(index); if (!right_node-\u0026gt;IsLeafPage()) { auto *internal_node = reinterpret_cast\u0026lt;InternalPage *\u0026gt;(right_node); auto *internal_left_node = reinterpret_cast\u0026lt;InternalPage *\u0026gt;(left_node); internal_node-\u0026gt;MoveAllTo(internal_left_node, middle_key, buffer_pool_manager_); } else { auto *leaf_node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(right_node); auto *leaf_left_node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(left_node); leaf_node-\u0026gt;MoveAllTo(leaf_left_node); parent-\u0026gt;Remove(index); } DeleteEntry(parent, middle_key); buffer_pool_manager_-\u0026gt;DeletePage(right_node-\u0026gt;GetPageId()); return true; } Redistribute 同样需要分为叶子节点和内部节点来进行讨论，内部节点由于0号位Key为空，因此无法像合并那样直接交换顺序即可，而是需要实现一个逻辑上一样的对称操作。\n叶子节点重分配​\n初始树：\n此时尝试从中删除掉23，此时该叶子节点当中就只剩下一个KV，小于min_size，并且无法和右节点进行合并，因此需要进行redistribute(代码逻辑中先判断是否能进行重分配，如果不能在后续判断合并，重分配本身不涉及到递归调用，因此对整个B+树的调整范围小，开销较小)\n自身为N，右节点为N‘，将N‘当中的第一个KV移动至N的末尾，并且将parent当中N,N\u0026rsquo;中间的key替换为移动之后的N‘当中的第一个K（25），最终结果如下：\n叶子节点的为完全对称的，因此只需要完全对称的实现即可，将第一个改为最后一个，逻辑上完全一致。\n1 2 3 4 5 6 7 8 9 10 11 if (node-\u0026gt;IsLeafPage()) { auto *leaf_node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(node); auto *leaf_neighbor_node = reinterpret_cast\u0026lt;LeafPage *\u0026gt;(node); if (from_prev) { leaf_neighbor_node-\u0026gt;MoveLastToFrontOf(leaf_node); parent-\u0026gt;SetKeyAt(index, leaf_node-\u0026gt;KeyAt(0)); } else { leaf_neighbor_node-\u0026gt;MoveFrontToLastOf(leaf_node); parent-\u0026gt;SetKeyAt(index, leaf_neighbor_node-\u0026gt;KeyAt(0)); } } 内部节点重分配​\n初始树和从中删除掉3之后的状态分别如下：\n此时左侧只剩下一个指针，并且合并会大于max_size，因此进行重分配，N为自身，N\u0026rsquo;为其右节点 找到N\u0026rsquo;当中的第一个KV，记为(K1,V1)，将其添加到N的末尾 找到在parent(N)当中的N与N\u0026rsquo;指针之间的K\u0026rsquo;，将N的最后一个K设置为K\u0026rsquo;，并将parent(N)当中的K\u0026rsquo;设置为K1 最终结果如下：\n1 2 3 4 5 6 7 8 9 10 auto *internal_node = reinterpret_cast\u0026lt;InternalPage *\u0026gt;(node); auto *internal_neighbor_node = reinterpret_cast\u0026lt;InternalPage *\u0026gt;(neighbor_node); if (from_prev) { internal_neighbor_node-\u0026gt;MoveLastToFrontOf(internal_node, parent-\u0026gt;KeyAt(index), buffer_pool_manager_); parent-\u0026gt;SetKeyAt(index, internal_node-\u0026gt;KeyAt(0)); } else { auto new_parent_key = internal_neighbor_node-\u0026gt;KeyAt(1); internal_neighbor_node-\u0026gt;MoveFrontToLastOf(internal_node, parent-\u0026gt;KeyAt(index + 1), buffer_pool_manager_); parent-\u0026gt;SetKeyAt(index + 1, new_parent_key); } 对于内部节点的重分配，存在那么一点不对称的情况：\n即上述例子为在左节点当中检测到需要进行合并，由于第一个空key的存在，K\u0026rsquo;的位置应当为index + 1,(index为N在parent当中的位置)，而如果是从右侧检测到需要进行重分配，K\u0026rsquo;即为index 右侧向左侧添加时应当添加index = 1的KV，跳过空Key，左侧向右侧添加时，同样添加到一号位置 实现的时候注意一下即可。\n","permalink":"http://itfischer.space/en/posts/tech/bustub/bustub-lab2-c1/","summary":"Lab2 对于Lab的debug，由于并没有开放测试样例，因此最好的就是找一个合适的b+树的模拟动画，然后再使用官方的画图工具比较自己的B+树，一般","title":"BusTub Lab2 B+Tree Index checkpoint1"},{"content":"BusTub Lab1 Buffer Pool Manager Task1 可扩展哈希表 相关函数 Find(K,V)​：查询一个Key是否存在，如果存在则将其V指针指向相关的值，返回true，否则返回false\nInsert(K,V):插入一个值，如果已经存在，则覆盖原本的值，返回true，如果当前k-v不能被插入（bucket满了，并且不是对原有的key进行更新），则：\n如果局部深度（容量）等于全局深度，增加全局深度，并且对dict的容量翻倍 增加当前要插入的bucket的深度 分裂当前的bucket，对其进行rehash 之后再进行重试，在此Lab当中，在插入之前进行容量的检测并进行rehash\nRemove(K):删除给定的Key，存在则返回true，否则返回false\nGetGlobalDepth():返回全局深度\nGetNumBuckets():返回当前存在的bucket的总量\nHint：\n可以使用IndexOf(K)​来求出当前Key所属的bucket Bucket作为一个内嵌类之于HashTable 保证线程安全，使用std::mutex 实现 逻辑上较为清晰，首先实现Bucket类，在KV的管理上，需要实现Find、Insert、Remove操作，并且需要有一系列获取相关信息的操作，\nBucket本身是由一个链表进行组织的，实现上直接调用std::list的API即可\n主要讨论一下Hash过程\n对于可扩展哈希，主要有四个变量：\nsize_dir_：表示整个目录的大小，扩展时进行翻倍 size_bucket_：一个bucket中能够存放的最大的元素，为固定值 global_depth：根据global_depth的位数将key hash到对应的bucket当中 local_depth：在bucket split时使用local_depth进行rehash 在引入了深度的概念之后，由于可扩展哈希分为bucket和bucket当中的元素，对应的就有全局深度和本地深度的区分，如果全局深度为2，则证明当前至多只能有4个Bucket，（而实际的bucket数量可能并没有4个，存在多个dir指向同一个bucket的情况）而如果本地深度为2，则下次bucket进行分裂时，使用local_depth +1​位进行rehash(如果认为最右一位为第1位，实际上只需要将1左移local_depth位即可）\n在搜索时，先根据global_depth​找到所属的bucket，再在bucket其中遍历寻找到元素\n明确了概念之后，可以捋一下整个过程：\n尝试向其中插入[0,1,2,3,4,5]\n当bucket满了时：\n如果局部深度（容量）等于全局深度，增加全局深度，并且对dict的容量翻倍 增加当前要插入的bucket的深度 分裂当前的bucket，对其进行rehash 最初在初始化时，global_depth = 0​ local_depth = 0​,此时只有一个bucket，并且当中能够存放一个元素 之后向其中插入元素1，而此时bucket当中有两个元素，需要对其进行区分，(触发了全局深度等于本地深度的条件)，因此：\n增加全局深度，global_depth = 1​,bucket数量翻倍 增加本地深度，local_depth = 1​ rehash 说一下最后的rehash的过程，虽然此时local_depth = 1​，但是在gloabal_depth=1​的条件下，hash码01无法定位到0号bucket，因此需要对其进行rehash\n此时再向其中插入2（10），global_depth = 1​因此会被rehash到0号bucket当中，而0号bucket并未满，因此可以将其插入到其中，3（11）同理，结果如下：\n再向其中插入4(100),在global_depth = 1​的条件下被分配到0号bucket，而此时bucket0已满，按照上面的要求步骤对bucket0再split一次，插入后的结果如下：\n插入5和4同理，将bucket1进行split,最终结果如下：\n还有一点需要讨论的是，并不是通过gloal_depth​ + local_depth​对key给进行定位，对于global_depth​，既是代表当前的dir数量，2则代表有00 01 10 11四个bucket，并且对于一个哈希码，确实可以通过global_depth​位就将其归于属于的bucket当中，但是对于local_depth​，仅仅只是在split时用于将当前bucket中的元素和新插入的元素rehash到不同的bucket当中，即与key的搜索无关，也与bucket的容量无关。\n对于local_depteh \u0026lt; global_depth的bucket，会存在多个dir指向同一个bucket的情况，当local_depth = global_depth时，则只会有一个dir指向该bucket。\n而在bucket split时，通过local_depth来决定元素都属于哪一个新的bucket当中。即将1左移local_depth位，即为local_mask，hash该位为1的归于一个bucket当中,为0的归于一个bucket当中。\n之后再去考虑两个新的bucket均需要属于哪个dir，上一步位与为0应当对应原本的dir[i]，而另外一个应当为dir[i+local_mask]。因此现在所做的就是需要找到i为多少，计算hash值然后取local_depth位，再与local_mask位与，结果为0即为low_bucket,为1即为high_bucket。\n此外需要考虑连续分裂的问题，如[0 1024 4]进行一次split不足以将这三者区分开，因此一种解决方法是在split最后调用extendiable_hash_table中的Insert，而不是调用bucket的insert，让哈希表的Insert再去判断一遍是否还需要分裂，不过这样嵌套调用无法使用RAII类型的锁，需要手动去加锁解锁。\n1 2 3 4 5 6 7 8 9 10 11 12 13 template \u0026lt;typename K, typename V\u0026gt; void ExtendibleHashTable\u0026lt;K, V\u0026gt;::Insert(const K \u0026amp;key, const V \u0026amp;value) { //std::scoped_lock\u0026lt;std::mutex\u0026gt; lock(latch_); latch_.lock(); auto index = IndexOf(key); auto bucket = this-\u0026gt;dir_[index]; bool result = bucket-\u0026gt;Insert(key, value); if (result) { latch_.unlock(); return; } RedistributeBucket(bucket,key,value); } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 template \u0026lt;typename K, typename V\u0026gt; auto ExtendibleHashTable\u0026lt;K, V\u0026gt;::RedistributeBucket(std::shared_ptr\u0026lt;Bucket\u0026gt; bucket,const K \u0026amp;key,const V \u0026amp;value) -\u0026gt; void { if (!bucket-\u0026gt;IsFull()) { latch_.unlock(); return; } if (bucket-\u0026gt;GetDepth() == this-\u0026gt;GetGlobalDepthInternal()) { auto size = dir_.size(); dir_.reserve(size * 2); std::copy_n(dir_.begin(),size,std::back_inserter(dir_)); this-\u0026gt;global_depth_++; } auto last_depth = bucket-\u0026gt;GetDepth(); auto high_bucket = std::make_shared\u0026lt;Bucket\u0026gt;(bucket_size_,last_depth + 1); auto low_bucket = std::make_shared\u0026lt;Bucket\u0026gt;(bucket_size_,last_depth + 1); num_buckets_++; int local_mask = 1 \u0026lt;\u0026lt; last_depth; for (const auto \u0026amp;[k,v] : bucket-\u0026gt;GetItems()) { if (std::hash\u0026lt;K\u0026gt;()(k) \u0026amp; local_mask) { high_bucket-\u0026gt;Insert(k,v); } else { low_bucket-\u0026gt;Insert(k,v); } } for (auto i = std::hash\u0026lt;K\u0026gt;()(key) \u0026amp; (local_mask - 1);i \u0026lt; dir_.size();i += local_mask) { if (static_cast\u0026lt;bool\u0026gt; (i \u0026amp; local_mask)) { dir_[i] = high_bucket; } else { dir_[i] = low_bucket; } } //auto new_index = IndexOf(key); latch_.unlock(); this-\u0026gt;Insert(key,value); } 在实现上先实现Bucket即可，先将Bucket设置为无锁的类型，再整个HashTable上加一把大锁，后续再考虑进行改进，以提高并发度\nTask2 LRU-K 在淘汰时选择Backward k-distance最大的进行淘汰，Backword-distance使用当前的时间戳和第前K次访问的时间戳进行计算，还未达到K次访问的Backword-distance记为正无穷，而当有多个正无穷时，则选择最早的时间戳进行淘汰。\nLRUKReplacer​和BufferPool​的大小一致，但是任何时刻下并不是所有page都会去考虑被淘汰，\n相关函数 Evict(frame_id_t*):在被Replacer记录的所有的可淘汰的frame中选择Backward k-distance最大的那一个，在参数中保存淘汰page的Id，返回true，如果没有可淘汰的frame则返回false RecordAccess(frame_id_t)​：基于当前的时间戳，记录给定的frameid，当一个page被BufferPoolManager​pin时调用 Remove(frame_id_t)​：清除一个frame的所有相关历史，当page被删除时调用 SetEvictable(frame_id_t,bool set_evictable)​：标记一个frame是否可清理 Size()​返回可清除的frame的数量。 实现 相关概念 LRU-K所针对的为缓存污染的问题，即遇到全表遍历等大量读取多个不重复的page时，如果使用LRU会将当前缓存中的所有的page全部淘汰掉，从而造成缓存失效的问题。\n在LRU-K中，主要的淘汰依据为backword k-distance​​，即据K次访问前的距离，\n而此处的距离作为时间的抽象，访问时间距离当前越久则为时间越长。而前K次访问的距离则是前K次访问的时间，可以用时间戳来表示 而对于访问次数还未到K次的情况，则是将其记为+inf，即正无穷 当进行淘汰时，选择backword k-distance​最大的frame进行淘汰 因此，由于还未访问到K次的frame会被记为正无穷，因此淘汰时会选择还未访问到k次的进行淘汰，而如果存在多个访问还不到K次的frame，则该几个frame之间使用普通的LRU进行淘汰，比较上一次的访问时间，其实即为k = 1的情况。\n而如果全部为访问了K次以及以上的frame，那么则选择distance最大的进行淘汰。\n目前能够想到的方案就是对于frame使用一个map来管理，而对每个frame封装一个frameInfo，在其中维护如是否为+inf，一次一个长度为K的链表，记录前K次的访问时间。但是这样的问题就是每次淘汰frame时需要遍历整个map，统计出K次访问最久远的frame，无法想LRU那样在O(1)的时间复杂度完成get/put。\n时间戳可使用自增的id即可。\n此外还有一个概念，BufferPool和LRU-K Replacer当中存储的为frame​，在BufferPool当中，应当预先初始化好一定的frame，之后frame的数量一直不变，每个frame对应一个frame_id，任何大于总容量的frame_id均为非法的frame_id。在replacer当中定义一个replacer_size，其大小应当和buffer_pool的size大小相同，同样对违法的frame_id进行检测\n实现 单HashMap内嵌List\n说清楚了相关概念，就再说一下如何实现，我主要想到了两种实现方案，第一种是上述的FrameInfo​，然后在一个HashMap当中去管理所有的FrameInfo，定义如下：\n1 2 3 4 5 6 struct FrameInfo{ bool isInf{true}; bool evictable{true}; std::deque\u0026lt;size_t\u0026gt; access_timestamp_; FrameInfo() = default; }; isInf​表示当前是否已经被访问了K次，如果还未，则设置为true，之后进行淘汰时优先淘汰还未访问K次的frame，当到达K次之后再设置为false。\nevictable​​表示当前的frame是否能被淘汰，用于在bufferpool当中pin住一些page，同时evictable​​也决定当前replacer的size。\ndeque​表示当前的该frame被访问的时间戳，大小上限为K，只保存K次以内的情况（不知道为什么用list就插入不进去，最后选择了deque，c++学太烂了先不管了）\n在这种实现方式下就不怎么分历史队列和缓冲队列了，反正历史队列和缓冲队列本身都要要存入bufferPool当中的，不如直接统一管理\nRecord_access：添加一次访问记录时首先判断之前是否访问过，如果访问过则找到之前的访问记录，则在尾部添加一条新的，注意是否超过K次取消掉+inf和超过K次之后把最早的删除掉即可，如果之前没有相关记录则直接新建一条即可\nRemove、Size、SetEvictable的实现较为简单，按照注释完成即可\nEvict：按照LRU-K原本的定义，应当先遍历历史队列，如果当中有frame的访问历史记录，则从历史队列中删除，历史队列当中如果没有能够可以淘汰的再选择从缓存队列中进行淘汰。因此对整个FrameInfo Map进行遍历，\n标记所有的inf，记录最小的时间戳，同时也记录访问了K次的，记录最小的时间戳，最终如果找到过inf的，则选择inf的进行淘汰，否则淘汰访问了K次的，注意跳过non-evictable​即可，弄清楚了LRU-K的概念之后实现起来就没有什么难度了。\n双List+单HashMap​\n这种方式则使用将历史记录和缓存分别使用两个List管理，之后删除是先遍历历史记录的List，如果找不到可以删除的，则再遍历缓存的List，\n在添加一次新的访问时，如果之前毫无记录，则将其添加到历史记录当中，并使用HashMap存储记录当前的迭代器位置和所属的队列， 如果访问次数i 1\u0026lt; i \u0026lt; k​,由于比较的是最早的访问时间，因此什么都不用做 如果访问次数到达K次，则将其移入到缓冲队列当中，更新Map当中相关的迭代器和所属队列 如果超过K次，则将其移入到缓冲队列的尾部，淘汰时从头部开始进行淘汰，更新迭代器 当淘汰时则先遍历历史队列，寻找第一个evictable​的frame进行淘汰，如果找不到，再去缓冲队列当中去寻找。\n严格来说这种实现方法更符合LRU-K原本的定义，这样如果不考虑 evictable​所带来的额外比较，在evict remove时的时间复杂度为O(1)，而之前那种实现方式需要遍历整个Map因此为常数复杂度，等有空考虑重构优化一下。\n本身也没有什么线程安全问题，各个函数直接一个区域锁利用 RAII保平安。\nTask3 Buffer Pool Manager Instance 从磁盘当中去读取page存储到内存当中，当Buffer Pool满了之后，再使用LRU-K将page从内存中淘汰，写回到硬盘上。\n内存中的Page使用Page​对象进行抽象，buffer pool无需关心page当中的内容，page当中包含一块内存，对应一个物理页，之后将对应的内存上的内容写入到硬盘上，一个Page​对应一个物理页，通过page_id​进行表述，如果该page没有对应的物理页，则page_id为INVALID_PAGE_ID​\nPage当中需要有一个计数器记录有多少个线程pinned​了该page，对于被pinned​的page，不允许将其释放，page需要记录是否为dirty​，如果非dirty​则淘汰时不需要回写硬盘，而dirty​的page需要先回写再重新使用\n使用之前的ExtendiableHashTable​进行page_id到frame_id的映射，使用LRUKReplacer​来记录各个page的使用情况\nFetchPgImp(page_id)​：如果没有可用的page并且其他的page全部被pinned​了,返回nullptr UnpinPgImp(page_id, is_dirty)​ FlushPgImp(page_id)​：不管是否被pin，都将page给刷盘 NewPgImp(page_id)​：AllocatePage​用于生成一个唯一的PageId，DeallocatePage​ 模拟将page刷盘到硬盘上，在DeletePgImp当中调用 DeletePgImp(page_id)​ FlushAllPagesImpl()​​ 实现 Buffer Pool的整体实现并不难，首先描述一下整个Buffer Pool的物理结构，Buffer Pool当中的容器或者说page的载体为frame，所有的frame本身是不变的，即frame的数量即为Buffer Pool的大小，在Buffer Pool进行初始化时，初始化一个Frame数组，其中每一个元素及对应着一个Frame，数组的下表即为Frame_id，如果page为尚未初始化的状态，则代表该frame当中还未存放元素，当有一个Page要受到Buffer Pool管理时，则对该Page进行初始化，设置对应的PageId和元数据 以及Page当中存储的数据设置到该Page当中。\n除此之外还有Buffer Pool的管理数据结构，即前两个task所实现的可扩展哈希表和LRU-K Replacer，以及一个用于记录空frame的链表\n当一个Page要加入到Buffer Pool当中，无论是新建一个Page还是从硬盘当中读取一个Page，首先先尝试从空frame链表当中获取一个frame_id，之后在Page数组当中对其进行初始化，而如果空闲链表当中没有额外的frame_id去分配，则证明当前的Buffer Pool已满，因此需要尝试通过LRU-K去淘汰一个当前Page的，获取一个新的Page，为了流程的统一，就淘汰之后再将frame加入到空闲链表当中，之后再从空闲链表当中尝试去获取一个Page。将新的Page重新让Replacer和page_table去管理。\n以NewPage为例，大致架构如下：\n搞清楚了整体架构再实现起来就比较方便了，这里就简单解释一下几个函数的相关作用,\nNewPage：即为上层需要创建一个新的Page用于存储新的数据，如在Insert时进行调用，即按照上图的步骤完成即可，先后尝试从free_list_​,若不能则告知replacer​进行淘汰，再从free_list_​当中进行获取，最后创建Page，并Page交给BufferPool管理。\n此外，创建Page即代表上层需要一个Page来承载数据，即当前需要使用该Page，因此需要该Page驻留在Buffer Pool当中，因此创建之后应当pin​住它，即修改pin_count_​和evictable​。\n由于涉及修改好几个数据结构，因此做了一个封装：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 auto BufferPoolManagerInstance::CreateNewPage(frame_id_t empty_frame_id) -\u0026gt; Page * { auto new_page_id = AllocatePage(); Page *new_page = \u0026amp;pages_[empty_frame_id]; ResetPage(new_page, new_page_id); // pin a new page new_page-\u0026gt;pin_count_++; AddNewPageToBufferPool(empty_frame_id, new_page_id); return new_page; } auto BufferPoolManagerInstance::AddNewPageToBufferPool(frame_id_t frame_id, page_id_t page_id) -\u0026gt; void { replacer_-\u0026gt;RecordAccess(frame_id); replacer_-\u0026gt;SetEvictable(frame_id, false); page_table_-\u0026gt;Insert(page_id, frame_id); } 通过replacer_​淘汰一个page封装成一个函数，淘汰之后如果为脏页则需要进行Flush刷盘，因为EvictPage​会被加锁的NewPage调用，因此自身并不加锁，而且调用无锁的FlushPage​\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 auto BufferPoolManagerInstance::EvictPage() -\u0026gt; bool { frame_id_t frame_id; bool result = replacer_-\u0026gt;Evict(\u0026amp;frame_id); if (!result) { return false; } // if frame can be evicted,it means page is not pinned; Page *page_to_evict = \u0026amp;pages_[frame_id]; if (page_to_evict-\u0026gt;IsDirty()) { FlushWithoutLock(page_to_evict-\u0026gt;GetPageId()); } free_list_.push_back(frame_id); page_table_-\u0026gt;Remove(page_to_evict-\u0026gt;page_id_); replacer_-\u0026gt;Remove(frame_id); return true; } FetchPage​：在流程上和NewPage​差不多，尝试从Buffer Pool当中获取一个Page，如果page_table_​当中有则证明存在于Buffer Pool当中 ，直接读取返回。而如果没有则分别从free_list_​和replacer​当中尝试找到一个frame​来承载Page，使用disk_manager_​进行读取，然后交给Buffer Pool​管理。\n由于FetchPage​的目的也是去读取一个Page为上层所用，因此同样需要将其pin住，直至使用完。\n此外如果是通过free_list_​和replacer_​所获取的Frame需要先清除上一个Page在其中残留的数据，之后加载新数据，由于grade_scope上传时不会去打包Page.h​，因此不应当对Page.h​进行修改，因此将该函数封装到Buffer Pool当中\n1 2 3 4 5 6 auto BufferPoolManagerInstance::ResetPage(Page *page, page_id_t page_id) -\u0026gt; void { page-\u0026gt;ResetMemory(); page-\u0026gt;page_id_ = page_id; page-\u0026gt;pin_count_ = 0; page-\u0026gt;is_dirty_ = false; } UnpinPage​：NewPage​和FetchPage​均为获取一个Page，之后为上层所用，因此获取的同时会Pin住该Page，而UnpinPage​即为用于在使用完page之后取消对该page的占用，让其可以被淘汰。\nUnpin​一次则对pin_count_​递减一次，当为0时证明没有任何上层函数在占用该Page，则可以set evictable​，之后replacer​即可对其进行淘汰\n同时UnpinPage​会传入一个is_dirty​，来代表之前占有Page时进行的操作是读操作还是写操作。此外Page上is_diry​的修改会有一定的限制，即如果原本为脏页，那么此次进行的是读操作，那么不能进行修改，需要保持脏页状态。而如果原本非脏页，那么即可随意修改。\nFlushPage​ DeletePage​ FlushAllPage​等逻辑比较简单，按照注释写基本就没什么问题。\ndebug 关于线程安全，保险的方案就是使用一个区域锁锁住整个函数，一把大锁保平安，但是之前写的时候在EvictPage​当中偷懒复用了一下FlushPage​，因此调用前解锁，调用完再加锁，导致了一个中间状态，再解锁的间隙所就被在并发的其他函数（UnpinPage​）抢走了，如果按照事务的说法就是违反了原子性，导致已经pin_count_ = 0​的Page又被其他的线程抢走了重新Unpin​一次\n因此最终选择单独封装了一个无锁版本的FlushWithoutLock​，之后EvictPage​ FlushPage​ FlushAllPage​全部调用该函数。即可保证线程安全，不会出现中间态。\nSummary 整个Buffer Pool的难度并不算大，写完之后也大致感受到了BusTub和6.830的侧重点 相比于6.830 BusTub少了很多dirty work，更注重于核心部分的实现，像表结构，Catalog，和HeapFile统统没有让我去写，而是专注于整个Buffer Pool。\nBuffer Pool 从0开始 可扩展哈希表+LRU-K的实现也有意思很多，相比于6.830的直接无脑一个ConcurrentHashMap，淘汰策略也没有做任何要求，FIFO也能通过测试。\n不过个人感觉刚接触数据库的话6.830可能比较友好一点，能对数据库的各方面都有一定的了解，有些dirty work首先一边体会才更深，例如HeapPage HeapFile里面的一个比较Tricky的位于算和Bit map，445的话就不能光去实现project了，其他地方的源码最好也读一读。\n‍\n","permalink":"http://itfischer.space/en/posts/tech/bustub/bustub-lab1/","summary":"BusTub Lab1 Buffer Pool Manager Task1 可扩展哈希表 相关函数 Find(K,V)​：查询一个Key是否存在，如果存在则将其V指针指向相关的值，返回true，否则返回fal","title":"BusTub Lab1 Buffer Pool Manager"},{"content":"BusTub Lab0 cpp primer 实现一个字典树，主要定义了三个类：\nTrieNode TrieNodeWithValue Trie Task1 实现一个不支持并发的字典树\nTrieNode 内部存储的数据为一个char字符， 并且有一个标识位置来表示是否到达了结束位置 对于字节点通过一个char-\u0026gt; unique_ptr的Map来进行存储 使用unique_ptr则意味着当将其分配给其他的变量时需要小心，InsertChildNode​ GetChildNode​的返回格式均为unique_ptr​，因此可以不进行copy的直接访问当中的数据\n通过移动构造函数来进行赋值，传输数据至一个新的节点上，确保没有对unique_ptr进行拷贝。\nTrieNodeWithValue 继承自TrieNode，代表最终节点，key为字符串的最后一个字符，is_end永远为true\n根据情况不同调用不同的构造函数：\n(char key_char,T value)用于根据给定的kv创建一个节点 (TrieNode \u0026amp;\u0026amp;trieNode,T value)获取传入节点的独占指针，并且将给定值设置到自身 Trie 定义了整个字典树，其中定义一个root node作为所有节点的根节点\nInsert​\n插入一个key时需要首先遍历整个树，如果不存在再插入节点，不允许插入重复的节点，重复则返回false\n遍历完之后有三种可能：\n节点不存在，调用TrieNodeWithValue(char key_char, T value)构造函数去插入一个新的节点，并且对TrieNode使用独占指针也可以对存储一个指向TrieNodeWithValue的独占指针？？ 节点存在，但是不是最终节点，需要调用TrieNodeWithValue(TrieNode \u0026amp;\u0026amp;trieNode, T value)​构造函数去将旧的TrieNode​转换为新的TrieNodeWithValue​ 节点存在但是已经是最终节点，return false即可 Remove​\n遍历寻找给定的key，如果不存在则立即返回 如果找到了则将is_end_设置为false 如果该节点没有任何的子节点，直接将其从父节点的map当中移除 遍历尝试并递归地删除没有子节点的节点。遇到有子节点时停止 GetValue​\n如果类型不匹配或者没找到key，则将success设置为false，\n‍\n","permalink":"http://itfischer.space/en/posts/tech/bustub/bustub-lab0/","summary":"BusTub Lab0 cpp primer 实现一个字典树，主要定义了三个类： TrieNode TrieNodeWithValue Trie Task1 实现一个不支持并发的字典树 TrieNode 内部存储的数据为一个char字符， 并且有一个标识位置来表示是否","title":"BusTub Lab0 cpp primer"},{"content":"今天是2023年的最后一天，大概半个月前就萌生了写一篇年度总结的想法，不过一直因为一些其他的事情拖到了最后一天，现在看大概是写不完了。鉴于我的本科也进入了最后的倒计时，因此一并回顾一下这忙忙碌碌的三年。希望在完成对2023的总结后，能对即将到来的2024有一点头绪。\n迷茫，以及自救之路 高考结束之时，带着一种考败来矿的心态，开启了我的大学生活，自踏入校门的那一刻开始，就想着通过保研来洗刷高考的失败。抱着这样的想法，大学最初自然是过得浑浑噩噩且失败的。\n严格来说，我是没有大一这个说法的，充其量就是高四罢了，在这一年中，我继续享受着做优等生的快感，活在他人的目光里。一方面是满绩拿到手软，另一方面则是完全没有思考过这个专业该学什么，自己感兴趣的又是什么。同学们晚上出去玩我嗤之以鼻，发觉机器人实验室可能影响保研就立刻退出。在大一结束时，带着“差不多”，“学的都一样”，以及本专业保研率更高的想法，我也是放弃了本科唯一一次的转专业机会。\n这样偏激的，彻底唯结果论的做法很快就带来了不可逆的后果，在大二上学期时，我才意识到自己对于AI，对于算法完全不感冒，和我所想的计算机完全不同。才意识到，整个大一只是享受那种在成绩上高人一等的快感，随即而来的则是彻夜难眠，在失眠的那些晚上，我几乎翻看完了知乎上所有跨保计算机的帖子，那些为数不多的成功案例也算是给我带来了一些宽慰，让我不至于彻底失去学习的动力。自那一刻起，便开始了我的自救之路，如何逃离AI，做自己真正感兴趣的方向。\n为了逃离AI，最初加入了学校的工作室并开始学习后端，在学习数据库的相关知识时，机缘巧合的接触到了csdiy和CMU15-445，也是在这个时候开始了解到计算机并非算法与开发二元对立，Andy讲课的幽默风趣和数据库本身的系统复杂性深深的吸引了我。如果说后端是因为我不想做AI的选择，那么数据库才是我真正发自内心的对其产生了浓厚的兴趣，经过一段时间废寝忘食的学习，也是决定了将数据库或者sys作为我今后的方向。后面的一切就比较的水到渠成了，刷课，做lab，读paper看开源项目都按部就班的进行。\n我的意思只是，我并不认为幸福可以指代一种令人满意的普适性的生活方式。你不能通过直接把它设定为目标来得到它。在我和彼得森的辩论中，我提出幸福是一个必要的副产品。假如你为其它某些东西而奋斗，它就会随之到来。它只以副产品的形式出现。如果你只把幸福当做目标，那总是会导向自我毁灭的。 ——齐泽克\n作者：Kagarino Kirie\n链接：https://www.zhihu.com/question/392634078/answer/2323699160\n来源：知乎\n著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n就像这段话一样，我想说的是，如果只把保研当作目标，那总是会导向自我毁灭的。盯着保研，然后为了保研去费劲手段的想出成果，这样大概率会像无头苍蝇一样到处做无用功，或者是直接当“绩点哥”，绩点大杀四方，技术上连排序都写不出。今年认识一位同学，也没有什么出色的科研经历，硬是靠自己扎实的基础和内功，连续拿下ruc信院和sjtu ipads的优营。所以这其实上就是一个因果关系，是为了更好的学习而去保研，还是说为了保研而去“出成果”。\n它更像是一个学习途径，如果我想学db/system，而保研可以给我提供一个很好平台，那么我就去保研。最终让我收获颇丰的不是接收到梦校offer的邮件，不是929当天在系统中点下确认接受的那一刻，而是你自己一步一步向保研走去，一路上的修炼的内功成就了自己。点下确认的那一刻，并不会像游戏中的装备那样，给我带来强力的属性加成，它最多就是给我提供了一个加速度，让我今后可以走得更快，走的更远。\n当然完全避免功利是很难做到的，人总是需要利己的，无论是名还是利，只要不搞反因果顺序就好。\n保研，终点亦是起点 如果说2023有什么足以改变人生轨迹的大事的话，那么大概就是最终如愿以偿的获得了推免资格，也收获了心仪的offer。\n带着非科班 + 非rk1的debuff，夏令营解决基本上被中九全拒德，好在人大、厦大收留了心碎鼠鼠，以及海王天大把我海进去了，让我不至于面对夏0营的惨剧。得益于疫情结束，今年的夏令营终于是转向了线下，参与感大大提升，面试体验拉满，也收到了人大的T恤和厦大的帆布包，和一些老朋友见面了，也认识了很多新朋友。虽然有点曲折，不过最后所有参加的夏令营/预推免都收获了offer。\n在浩浩荡荡的AI大军中，出于sys人抱团取暖的目的，建了一个sys保研的小群，这一路走来也少不了群友们的支持与鼓励，每次夏令营都有机会和和群友们线下面基聚一聚和群友们逛了北京、南京、厦门和武汉，可惜的是最终还是没能和所有群友都见上一面。群友们最后也都去了很好的学校，也祝他们都能够在硕士阶段继续发光发热，追求自己所热爱的。\n929当天，虽然跟着绿裙春晚乐呵了一下，也在自己的小群里面庆祝了一下，但是感觉其实并没有什么特别的，等待offer和点下确认都是水到渠成的事情。后续自然也没有什么所谓的gap year了。后面就按部就班的打了ob，看了leveldb，启动了一下rust，补了一些基础。要说最大的区别的话，大概就是再也不需要去学那些没意义的课程，刷那些没有用的绩点了。可能比较遗憾的是决定ob决赛弃赛时已经十二月初了，这时候几乎已经没有实习了，投了一圈几乎没人理我，并且刚投了一份就甲流中招了，在床上又躺了好几天。唯一的一家初创也因为我没背八股在二面挂掉了，其实也不能算背八股，问的都比较基础，在OSTEP，程序员的自我修养里面都看到过，长时间没复习遗忘了。算是一个遗憾了。\n技术，路在何方 今年技术的最大成长，或许是知道了自己应该努力的方向。如果说从系统性学习开始计算的话，今年大概是我的system元年。接下来我要作一些枯燥无味的机械列举了——希望读者能够包容这点。\nLab \u0026amp;\u0026amp; 项目\n要说第一次接触system，还是大二暑假了解到的MIT-6.824，不过当时的代码水平实在是不堪入目，MapReduce也没写出来就被劝退了。年初终于有机会重新再次挑战一下6.824了，即便是有了一些system基础之后，整个过程还是非常酸爽的，被各种tricky的bug虐的死去活来，对着几万行的日志欲哭无泪。算上看课 + 看paper，最后耗时两个多月的终于是实现了自己一个比较满意的版本。相比于Raft，6.824更多的是意志上的一个磨练。 之前看445时cpp并不太熟练，lab选择了Java实现的MIT 6.830，今年终于是有机会补上了，有了6.824的基础，445写起来就很得心应手了，虽然噩梦B+树耗费了我不少时间，不过其他三个很快就A掉了，这也算是我真正意义上的第一个modern cpp项目，总的来说收获很多，学到了很多工程性的东西 Miniob/Oceanbase：参加 ob 比赛时已经是保研结束了，这时候就是纯兴趣了，加上由于准备夏令营/预推免很久没写代码了有点手痒 (瑟提手痒难耐，渴望写代码)。Miniob 和 Bustub 的侧重点不同，更注重功能上的实现和问题的解决，初赛中也写了几个比较好玩的东西，像是 null 字段，支持 text 和 mvcc，最后非常有幸发现了系统中的一些 bug，提的 pr 也被来哥合到了主分支，也算是为开源做贡献了。决赛就比较坐牢了，在相继搞出几个优化点之后终于是遇上了瓶颈，感受到了自己能力不足带来的无力感，最后无奈弃赛，成绩从 15 最后掉到了 30 多。 书籍\n读完了DDIA，个人感觉这本书作为分布式的入门或者初学并不合适，DDIA中描述的都比较的宽泛，如果是刚接触分布式的话难以有什么深刻的体会。最好是对分布式、存储、数据库各个方向都有所涉猎之后，再看会有一种集大成的感觉，能够理解其中的各类设计所解决的问题，正如书名一样，能够体会到构建一个数据密集型的大型分布式系统都需要什么。 补了补cpp，跳着看了看《C++ Primer》，过了一遍《Effective c++》和《Effective modern c++》，意识到了自己原本写了多少屎山出来，不过即便看完，我也并不认为我是一个合格的cpp程序员。 看了一本叫做《深入理解分布式系统》的书，听名字感觉有些噱头，但是相比DDIA更适合初学者，里面对各类一致性模型，CAP，Paxos，Raft都有很详细的说明，以及各类分布式系统，如Spanner，GFS，Aurora的例子。如果夏令营前认真看了这本书，人大大概就直接优营了（悲 系统性的过了一遍《数据库系统概念》的重点章节，之前一直是当字典来用，需要什么看什么，系统性看一遍确实有新的收获，奉为数据库圣经不为过，不过翻译质量实在不对我胃口，将key翻译成“码”等对我来说实在难以接受。如果夏令营前认真看了这本书，人大大概就直接优营了（悲 接触一下 Rust，看了《Rust 圣经》，之后持续性的被编译器拷打，本来想做一下 rcore 的，结果和 ob 撞车了无奈放弃，写完 rustlings 就溜了。年底又捡起来了，之前在知乎上看到一个用 rust 科研的文章，“即然我们按照 rust 的方式去写 c++，那么我们为什么不直接写 rust 呢？” 看过的开源项目\n四月那阵联系了ecnu的老师，主要是做rocksdb相关工作的，因此就先看了看leveldb的源码，大致了解了lsm的原理与实现，在今年十一月时又重新分析并调试了一下leveldb。“跟leveldb学LSM-Tree”，“C++98的最佳实践”，个人深以为然。 五月那阵又联系了人大，开始做一些跨域Raft的内容，实验是基于ectd/raft跑的，于是就开始看ectd/raft的相关内容，这也是第一次接触到了工业级的Raft，学到了一些架构设计和工业界的优化。 年底为了补一下 Rust，看了 toydb 的源码，toydb 以很小的代码规模，实现了一个结构完善，代码清爽的分布式关系型数据库，写的也很有 Rust 的味道，看的直呼过瘾。 Paper\n准备夏令营和打ob分别在上下半年花费了我一些时间，加上目前处于入门的阶段，paper积累确实不多，列举几个自己印象比较深刻的，大概看了这样几个方向的：\n分布式：谷三篇，以及一些6.824中的经典分布式论文，大概了解了分布式是为了解决什么样的问题，和对应的解决方案。Raft大论文部分章节，跟着etcd一起看的，了解了Raft的常规优化；逻辑时钟，HLC，CRDB，在人大组里跟着学了一点，今年后续面试几次莫名就扯到CRDB上面了，感觉能拿下华科全归功于CRDB 存储：主要是一些lsm相关内容，像是Wisckey，pebblesDB,HashKV这些经典优化；还有篇盘古2.0发在今年fast上面的，也挺有意思的。 Db：九月那阵看了一些 721 中列存的相关内容，后续被预推免打断了，鉴于后续主方向也不是 db，也没再捡起来。追了一下热点，看了些 vector db 的相关内容，Manu，SPFresh 之类。 算法\n从草履虫进化到蚯蚓，夏令营前过了几遍acwing基础课，这里不评价yxc这个人怎么样，单论acwing基础课的话，确实是很适合我这种算法草履虫去入门了。\n生活 总的来说是非常无趣的，基本上每天都是烂在宿舍写代码，大概只有觅食的时候才会离开宿舍。人际交往除了水群几乎为0，而且作息是彻底的烂掉了。\n不过依然有值得庆祝的，从国庆结束回到学校开始，到十二月底回家，大概瘦了接近20斤，距离脱离肥宅又近了一步，能明显感受到身体负担减轻了。\n游戏：\n荒野大镖客 救赎2:唯一真神，无与伦比的沉浸感，不想学的时候就跑到西部，今年100多个小时玩下来感觉亚瑟更像是我的一个老朋友，雪山复仇回到比彻之愿时，听着《Come Live By My Side》有一种怅然若失的感觉 2077往日之影：个人心目中的年度最佳剧情，加上2.0系统大改，玩起来根本停不下来 潜水员戴夫：前期新鲜感扑面而来，可以给9分，后期只能1分 古墓丽影三部曲：E宝之前送的，一直没时间玩，解密和攀爬体验都很不错，想尝试同类型的盗贼之海了 控制：光敏性癫痫差点给我干出来了，遂退游 战地：偶尔去捞捞薯条 真人快打 11: b 站看到视频入坑，结果上手玩发现根本操作不过来 差生文具多\n每次口口声声说这是最后一把键盘，今年大概是真的退烧了，在分别体验过hhkb和niz之后，最后还是感觉niz接近线性轴手感的静电容更适合我，然后将其改成了hhkb佩列的键位。原本的机械键盘彻底打入了冷宫，今年回家也只带了一把niz回来。\n赛博罗宾汉\n新培养方案中，添加了劳动课作为必修，但是教务开出的课程供不应求，以至于出现了几百，几千一门课的情况，学生叫苦不迭。那时就在想，是不是可以利用一下自己学习的技术呢？于是搞了一个脚本，简单发送几次网络请求就可以了。靠这样操作，帮我室友把劳动课都选上了。后面一看到群里面有人说要卖课就把脚本打开，半路拦截掉课程，再随便找个时间给释放掉，颇有点劫富济贫的感觉。\n技术之外\n不得不说，这样的生活对我来说也过于沉闷了，游戏固然挺好玩也可以消磨时间，但是可以称为爱好吗，并让我持续性的去投入并付出吗？我想并不是的。\n国庆那时就想着自己是不是应该尝试接触一门乐器。之后被朋友问到，“读研以后除了日常工作，有没有特别想干的？”，我下意识的说出了想自己从头写个分布式kv，那一刻才发现生活已经日渐枯燥。后续这个想法无数次萦绕在我的脑海中，可以是吉他，也可以是贝斯，更重要的是应该迈出第一步。前一阵得了甲流，在躺在床上看完孤独摇滚之后，这种想法更加强烈了。但是后面忙起来之后又让我举棋不定，不得不说这成了我2023中的最大遗憾。希望在新的一年能够做出改变，迈出第一步。\n新年期望 技术上的话，要学的大概很多，目前感受比较强烈的是该补一补体系结构了，打算看一看软硬接口那一本书。其他的暂时没有想到，按部就班的学就好。\n寒假先争取比毕设的基础功能写出来，开学之后再搞一些比较意思的优化加进去。最后毕设是打算用rust自己从头搞，最近也恶补了一下rust，希望自己能坚持下来，不要逃跑到c++。\n剩下的就是些生活上的了：\n希望自己能再瘦20斤吧，国庆的时候比高中胖了快40斤，目前减肥进度50%，希望下学期能瘦回高中时期 学一门乐器，目前更喜欢贝斯一点，很希望能自己弹一下皇后乐队的《Another one bites the dust》 寒假学一学做饭，目前体重的体重一半得归功于自己嗯造外卖(另一半是嗯造宵夜)，希望寒假能自给自足，在家少点点外卖 ","permalink":"http://itfischer.space/en/posts/life/2023%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93%E8%AE%B0%E5%BF%86%E8%BF%B7%E8%8C%AB%E4%B8%8E%E6%88%90%E9%95%BF/","summary":"今天是2023年的最后一天，大概半个月前就萌生了写一篇年度总结的想法，不过一直因为一些其他的事情拖到了最后一天，现在看大概是写不完了。鉴于我","title":""},{"content":"差生文具多系列\nCS方向，个人科研 + 开发工具链分享，主要涉及到文献阅读、代码编辑、笔记等内容\n文献阅读 Zotero 集文件管理、阅读、记录为一体，基本可以解决所有问题。\n支持通过插件进行扩展，如翻译，markdown，笔记等。翻译插件可以自由配置API，选择Google、DeepL等。 可以使用官方同步，不过容量有限。但是支持第三方同步，可以使用坚果云等 缺点大概是比较吃内存，基本启动起来就会吃1GB+ 如果不想配置Zotero的话也可以使用小绿鲸，各方面也都还算不错，同样支持翻译和官方同步，比较烦的是登录需要微信扫码\nZotero | Your personal research assistant 小绿鲸英文文献阅读器——专注提高SCI阅读效率 PDF 目前使用的是Mac上自带的预览，快速且轻量，可以满足日常使用，如果有需要操作PDF，如合并，分割，格式转换等，使用ILovePDF(除了ILovePDF，还有如ILoveIMG等)\nwin上面可以考虑使用Sumatra PDF，如果学校买了Adobe系列也可以使用Adobe\niLovePDF | Online PDF tools for PDF lovers Free PDF Reader - Sumatra PDF 代码编辑 Vscode 目前笔者写的最多的是C++/Rust/Go，基本上Vscode一站式解决，vscode的配置文章网上有很多，这里就不详细展开了，放几个我目前在使用的插件： rust基本上一个rust-analyzer就够了，补充一个toml文件支持的： 其他插件\nBookmarks：为代码添加标记，方便跳转 error-lens：更加人性化的高亮显示error hex-editor：阅读和编辑二进制文件 remote-ssh：这个不必多说了，使用服务器必备 Vim：vscode上的Vim插件 VsCode Counter: 代码量统计，高级版wc Tokyo Night：个人最喜欢的主体 Cappuccin Icon / Material Icon：图标 Vim 目前在用Lazy vim，因为有时候写急眼了还是喜欢去摸鼠标，所以对我而言还属于玩具性质，没有将其当作主力来使用。\n相同的还有LunarVim，都是属于开箱即用\n笔记 笔者对笔记软件的基本要求有两个，一是高性能，二是同步。\n一些笔记软件在字数达到1-2w之后就会出现较为明显的卡顿和延迟，这种就不予考虑 同步：同步的方式主要有两种，一是笔记直接云端存储，如notion、语雀、wolai等。另外一种形式是本地存储 + 云端备份同步，代表有obsidian和思源笔记。 针对这两个问题，之前对市面上的主流笔记软件都有体验： 高性能：Typora，notion、语雀以及其他云端存储笔记软件，在笔记规模达到1w-2w左右就可以感受到存在延迟和卡顿，因此不考虑作为主力来使用 同步：这个比较好解决，可以花钱使用官方存储，也可以自己使用github来解决。 Obsidian 因此，综合考虑，最终笔者的选择是obsidian：\nob可以选择开启GPU加速，性能有基本保证，可以处理较大规模的笔记 笔记管理：ob是一种类似知识库的形式，选择一个目录作为空间，之后所有笔记都管理在这个空间内，虽然不支持像noition那样进行笔记嵌套，但是可以通过文件夹分类管理 双链笔记：用于进行知识点的融会贯通 丰富的插件生态，基本能够想到的功能都有对应的插件支持 外观丰富，可以进行赛博暖暖 同步 ob有官方的同步服务，但是需要收费。由于使用的是md文件存储，因此想要自己同步也非常简单：\n对于源文件，直接使用githubob也有对应的自动commit 和 push的插件。 对于图片，使用 PicGo + 图床，ob 也有对应的 PicGo 插件, 把图片复制到文本当中就会自动上传 + 返回引用链接。使用腾讯云的话，如果只供自己观看，一个月大概只需要几分钱。 除了github之外，还可以直接使用坚果云同步ob的根目录，多一份保险。\nObsidian - Sharpen your thinking\n思源笔记 在使用ob之前，笔者用了很长一段时间的思源笔记，个人认为在使用体验上各方面都和ob不相上下，似乎是基于electron实现的，但是性能出奇地高，号称百万级别的字数也不会卡顿，比ob还要流畅。并且内存占用只有100MB左右。同时也有丰富的插件系统，甚至还有： 不过由于不是使用md存储(可以导出为md，但是不是直接使用md存储)，手动同步起来比较费劲，最终和ob的比较还是败下阵来，只能忍痛割爱了。如果思源愿意使用md进行存储的话，我还是更愿意使用思源。\nSiYuan - Privacy-first personal knowledge management system that supports Markdown, block-level ref, and bidirectional links\nNotion 在上面的方案当中，ob的问题是文件是本地存储的，因此想要把笔记分享给别人比较费劲，可以分享github的链接，不过我设置成了private，所以这个就不行了。因此，在分享上我使用notion作为补充，导入起来也比较方便。\n其他 浏览器 主力是Arc，今年发现的宝藏，无论是其侧边栏的网页管理还是各种小功能都用着极其舒适。 完全兼容chrome生态(毕竟是同一个内核)，可以无缝从chrome切换过来，插件，各类配置，个人收藏，记录的密码都可以全部导入。\n更新很频繁，时不时会添加一些新的功能。\n很难用几句话就把Arc的优点全部描述清楚，不过个人使用下来的感觉就是极度舒适，最大的问题是只登陆了Mac。\n产品趋势02期(上)｜挑战Chrome的最强浏览器？Arc究竟牛在哪里 - 知乎 Arc from The Browser Company AI助手 主力是GPT4，如果GPT抽风的话会使用Kimi Chat，各方面都还不错，支持文件上传和联网服务，并且有手机端，在手机上不想挂梯子的时候会用。 Kimi Chat - 帮你看更大的世界\n终端 目前是Iterm2和Warp混用，使用体验都不错，Warp在命令补全和提示上效果更好，并且集成了AI，Iterm2就是更加轻量一些，如果想补全的话可以使用Fig。\nIterm2和Warp目前只登陆了Mac，比较遗憾\nFig\n很好用的补全功能，支持iterm2,vscode和jetbrain的内置终端，不过问题同样是只登陆了Mac。效果如下，绝大多数的命令都有支持。 Shell oh-my-zsh，mac和linux上都是，这个没什么好说的。\n同步 目前都交给坚果云了，被坚果云握住了命脉\n","permalink":"http://itfischer.space/en/posts/life/cs%E7%A7%91%E7%A0%94%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7%E9%93%BE%E5%88%86%E4%BA%AB/","summary":"差生文具多系列 CS方向，个人科研 + 开发工具链分享，主要涉及到文献阅读、代码编辑、笔记等内容 文献阅读 Zotero 集文件管理、阅读、记录为一体，基本可以解","title":""},{"content":"引言 toydb是一个完全由rust编写的分布式关系型数据库，相对于其代码规模，其功能实现上可以说是非常的完善了，使用大约1.5w的rust代码，实现了：\nSQL引擎 SQL解析：词法分析 + 语法分析，最后生成一个棵AST SQL执行：根据AST生成一个Planner，以及使用Optimizer进行优化，最后执行，Executor上使用了火山模型 Raft模块：整体设计上采用了类etcd的状态机结构，通过逻辑时钟 + message + step来驱动raft状态机的变更 存储引擎：使用了Log-Structured当中最简单的Bitcask 事务：提供最基本的 ACID 和 MVCC 支持 项目的整体架构图如下： 执行流程 在本章当中，以一次简单的SQL执行过程，来了解一下toydb的整体架构和调用链\nserver toydb整体是采用client-server结构的，client在这里不做分析，在src目录下存在一个server.rs，其中定义了server的一些相关逻辑，那么程序的入口自然而言的就是server.rs了,server负责与客户端以及其他的raft节点之间的通信，主要的就是三个函数：listen、serve、serve_sql\nlisten：传入两个addr，分别代表用于接收client发送的sql的ip以及用于raft通信的ip，在通信上，为了应对高并发，采用了async/await异步编程的方式(tokio)，这里不过多分析 serve：接受网络请求并进行处理，在其中分别调用raft.serve和serve_sql，raft.serve留到raft模块当中进行分析 serve_sql：sql执行的入口 serve_sql： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /// Serves SQL clients. async fn serve_sql(listener: TcpListener, engine: sql::engine::Raft) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut listener = TcpListenerStream::new(listener); while let Some(socket) = listener.try_next().await? { let peer = socket.peer_addr()?; let session = Session::new(engine.clone())?; tokio::spawn(async move { info!(\u0026#34;Client {} connected\u0026#34;, peer); match session.handle(socket).await { Ok(()) =\u0026gt; info!(\u0026#34;Client {} disconnected\u0026#34;, peer), Err(err) =\u0026gt; error!(\u0026#34;Client {} error: {}\u0026#34;, peer, err), } }); } Ok(()) } 可以看到，在其中处理完网络链接之后，调用了session.handle(socket)，在其中从tcp流当中获取了request之后，就调用了self.request(request)。后面剩下的都是些对于执行结果的处理，用于进行返回。\n在request当中，会根据request的类型，来决定如何执行，request本身为一个枚举类型：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 #[derive(Debug, Serialize, Deserialize)] pub enum Request { Execute(String), GetTable(String), ListTables, Status, } /// Executes a request. pub fn request(\u0026amp;mut self, request: Request) -\u0026gt; Result\u0026lt;Response\u0026gt; { debug!(\u0026#34;Processing request {:?}\u0026#34;, request); let response = match request { Request::Execute(query) =\u0026gt; Response::Execute(self.sql.execute(\u0026amp;query)?), Request::GetTable(table) =\u0026gt; { Response::GetTable(self.sql.read_with_txn(|txn| txn.must_read_table(\u0026amp;table))?) } Request::ListTables =\u0026gt; Response::ListTables( self.sql.read_with_txn(|txn| Ok(txn.scan_tables()?.map(|t| t.name).collect()))?, ), Request::Status =\u0026gt; Response::Status(self.engine.status()?), }; debug!(\u0026#34;Returning response {:?}\u0026#34;, response); Ok(response) } SQL引擎 普通的sql的request类型为Request::Execute，因此就会调用self.sql.execute(\u0026amp;query),在这里面，终于到真正的sql执行过程了，首先会创建一个Parser来解析sql，得到一个AST，之后根据AST的statement类型，其中有一些事务相关的，而对于普通的sql，则会走最后一部分，调用链为Plan::build -\u0026gt; optimize -\u0026gt; execute，算是比较标准的sql执行流程了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 这一部分是原本没有开启事务的，会默认开启一个事务 statement =\u0026gt; { let mut txn = self.engine.begin()?; match Plan::build(statement, \u0026amp;mut txn)?.optimize(\u0026amp;mut txn)?.execute(\u0026amp;mut txn) { Ok(result) =\u0026gt; { txn.commit()?; Ok(result) } Err(error) =\u0026gt; { txn.rollback()?; Err(error) } } } 之后就是定义了一个Executor trait，然后其中包含一个execute，之后每添加一个算子就实现这个特征即可，可以看到的是，目前该特征有17个实现，那么就是所有算子了，对于算子怎么实现的，这里不做展开 1 2 3 4 5 6 7 8 9 impl\u0026lt;T: Transaction\u0026gt; Executor\u0026lt;T\u0026gt; for Scan { fn execute(self: Box\u0026lt;Self\u0026gt;, txn: \u0026amp;mut T) -\u0026gt; Result\u0026lt;ResultSet\u0026gt; { let table = txn.must_read_table(\u0026amp;self.table)?; Ok(ResultSet::Query { columns: table.columns.iter().map(|c| Column { name: Some(c.name.clone()) }).collect(), rows: Box::new(txn.scan(\u0026amp;table.name, self.filter)?), }) } } 这里以最简单的Scan为例，继续执行流程，在其中调用了txn.scan，之后就比较的复杂了，这里简单说一下流程，细节留着后面再单独进行分析，大致流程是先查询Raft的状态机，再走到存储层进行操作，在存储层当中的执行顺序大致为kv engine -\u0026gt; mvcc -\u0026gt; bitcask。\n施工路线 bitcask storage engine raft sql engine mvcc 之所以开这个系列，主要是最近写毕设用到了rust，而自己目前的rust基本上是三脚猫水平，刚好toydb足够精简，加上功能十分的齐全，因此就以toydb作为rust实践学习，力图分析完主要代码逻辑，但是由于笔者的rust水平不佳，期间难免存在理解偏差与错误，欢迎读者批评指正,最后，希望这个系列不会烂尾:) ","permalink":"http://itfischer.space/en/posts/tech/toydb/00-architecture/","summary":"引言 toydb是一个完全由rust编写的分布式关系型数据库，相对于其代码规模，其功能实现上可以说是非常的完善了，使用大约1.5w的rust代","title":"00-Architecture"},{"content":"基础结构 Bitcask本身非常简单，要是想实现一个最基础的bitcask存储引擎，大概200-300行代码就能实现，这里先简单介绍一下Bitcask的基础结构：\nBitcask是基于日志的，即Log-Structured,即采用顺序写入的方式，无论是删除还是更新都是向日志文件当中追加一个Entry，利用磁盘顺序写入的性能大于随机写入的特点，以达到高性能，但是同样的，在存储空间上会做出牺牲(日志文件当中会存储一些无效、过时的Entry)\n结构 Bitcask在数据管理上分为两部分，分别是内存和磁盘：\n在内存当中维护一个map，key为存储的key，而value为Entry的metadata，记录长度和位置，用于进行偏移读取。map当中始终保存当前key的最新版本的位置 磁盘上使用Log-Structured进行管理，任何操作都是写入一个Entry，追加到日志文件的末尾，Log当中的存储单元通常为Entry,Entry的结构大致如下： 接口 对于最基础的存储引擎，通常只需要提供Get、Put、Delete，大致逻辑如下：\nGet:首先查询内存当中的map，如果不存在那么就是真的不存在，如果能查询到，那么就根据metadata去磁盘当中读取出对应的value Put：首先向磁盘当中写入一条新的Entry，如果并且更新内存的map，保存新Entry的offset Delete：和Put的逻辑基本一致，只不过value的类型不一样，写入的内容为tombstone，标志val已经被删除，同时删除内存当中的kv Compaction 和LSM-Tree一样，Bitcask同样有Compaction的过程，如上面所描述的，在写入过程当中，会有key被更新或者删除，但是旧版本的key依旧会存在于日志文件当中，随着时间的增加，日志文件当中的无效数据就会越来越多，占用额外的存储空间。因此就需要compaction将其清除。\n对比LSM-Tree来说，Bitcask的Compaction就会简单非常多，只需要遍历当前内存当中存在的key，读取旧文件，写入到新文件当中，之后将新旧文件进行替换即可。\n如果像具体了解Bitcask的话，可以看一下：\nddia当中的内容：第三章：存储与检索 paper：riak.com/assets/bitcask-intro.pdf 实现 如上面所说的，在src/storage/engine/bitcask.rs当中定义了对应的结构体，其中有两个成员变量，分别对应内存当中的map和磁盘当中的日志文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pub struct BitCask { /// The active append-only log file. log: Log, /// Maps keys to a value position and length in the log file. keydir: KeyDir, } /// Maps keys to a value position and length in the log file. type KeyDir = std::collections::BTreeMap\u0026lt;Vec\u0026lt;u8\u0026gt;, (u64, u32)\u0026gt;; struct Log { /// Path to the log file. path: PathBuf, /// The opened file containing the log. file: std::fs::File, } Log KeyDir没什么好说的，就是一个内存当中的map，这里使用的是BTreeMap的实现方式，便于进行顺序遍历进行compaction。\n这里看一下Log的实现，在Log当中，除了初始化的new和用于debug的print，共有三个函数，分别用于读取、写入和根据日志重新构建Bitcask。\nread_value和write_entry的实现比较简单：\nread_value当中只需要根据传入的偏移量和长度，将对应的value读取出来即可 write_entry当中，分别写入key_len，value_len(or tombstone)，key_bytes，value_bytes(如果是删除那么久不写入)，最后调用flush持久化到磁盘，最后返回一个offset和len，用于保存到BTreeMap当中 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 /// Reads a value from the log file. fn read_value(\u0026amp;mut self, value_pos: u64, value_len: u32) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt; { let mut value = vec![0; value_len as usize]; self.file.seek(SeekFrom::Start(value_pos))?; self.file.read_exact(\u0026amp;mut value)?; Ok(value) } /// Appends a key/value entry to the log file, using a None value for /// tombstones. It returns the position and length of the entry. fn write_entry(\u0026amp;mut self, key: \u0026amp;[u8], value: Option\u0026lt;\u0026amp;[u8]\u0026gt;) -\u0026gt; Result\u0026lt;(u64, u32)\u0026gt; { let key_len = key.len() as u32; let value_len = value.map_or(0, |v| v.len() as u32); let value_len_or_tombstone = value.map_or(-1, |v| v.len() as i32); let len = 4 + 4 + key_len + value_len; let pos = self.file.seek(SeekFrom::End(0))?; let mut w = BufWriter::with_capacity(len as usize, \u0026amp;mut self.file); w.write_all(\u0026amp;key_len.to_be_bytes())?; w.write_all(\u0026amp;value_len_or_tombstone.to_be_bytes())?; w.write_all(key)?; if let Some(value) = value { w.write_all(value)?; } w.flush()?; Ok((pos, len)) } 这里稍微麻烦一点的是build_keydir，即用于在数据库启动时，读取日志文件，恢复出内存当中的BTreeMap，大致逻辑为：\n从日志文件的开头开始遍历 先读取出key_len和value_len，其中，如果value_len为-1则证明当前为tombstone 如果是-1就封装一个none，否则计算出value_offset 读取出key，之后根据是否为tombstone来决定对map是插入还是删除 错误处理 循环直至日志文件末尾 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 /// Builds a keydir by scanning the log file. If an incomplete entry is /// encountered, it is assumed to be caused by an incomplete write operation /// and the remainder of the file is truncated. fn build_keydir(\u0026amp;mut self) -\u0026gt; Result\u0026lt;KeyDir\u0026gt; { let mut len_buf = [0u8; 4]; let mut keydir = KeyDir::new(); let file_len = self.file.metadata()?.len(); let mut r = BufReader::new(\u0026amp;mut self.file); // ------(1)----- let mut pos = r.seek(SeekFrom::Start(0))?; while pos \u0026lt; file_len { // Read the next entry from the file, returning the key, value // position, and value length or None for tombstones. let result = || -\u0026gt; std::result::Result\u0026lt;(Vec\u0026lt;u8\u0026gt;, u64, Option\u0026lt;u32\u0026gt;), std::io::Error\u0026gt; { // ------(2)----- r.read_exact(\u0026amp;mut len_buf)?; let key_len = u32::from_be_bytes(len_buf); r.read_exact(\u0026amp;mut len_buf)?; let value_len_or_tombstone = match i32::from_be_bytes(len_buf) { l if l \u0026gt;= 0 =\u0026gt; Some(l as u32), _ =\u0026gt; None, // -1 for tombstones }; // ------(3)----- let value_pos = pos + 4 + 4 + key_len as u64; let mut key = vec![0; key_len as usize]; r.read_exact(\u0026amp;mut key)?; if let Some(value_len) = value_len_or_tombstone { if value_pos + value_len as u64 \u0026gt; file_len { return Err(std::io::Error::new( std::io::ErrorKind::UnexpectedEof, \u0026#34;value extends beyond end of file\u0026#34;, )); } r.seek_relative(value_len as i64)?; // avoids discarding buffer } Ok((key, value_pos, value_len_or_tombstone)) }(); // ------(4)----- match result { // Populate the keydir with the entry, or remove it on tombstones. Ok((key, value_pos, Some(value_len))) =\u0026gt; { keydir.insert(key, (value_pos, value_len)); pos = value_pos + value_len as u64; } Ok((key, value_pos, None)) =\u0026gt; { keydir.remove(\u0026amp;key); pos = value_pos; } // ------(5)----- // If an incomplete entry was found at the end of the file, assume an // incomplete write and truncate the file. Err(err) if err.kind() == std::io::ErrorKind::UnexpectedEof =\u0026gt; { log::error!(\u0026#34;Found incomplete entry at offset {}, truncating file\u0026#34;, pos); self.file.set_len(pos)?; break; } Err(err) =\u0026gt; return Err(err.into()), } } Ok(keydir) } Bitcask 看完了Log，再来看一下Bitcask本体，可以看到，BitCask有五个对应的实现，除去Display和Drop以外，还有三个impl，分别来看这三个impl\n第一个实现中定义了Bitcask初始化相关操作，对应函数为new和new_compact：\nnew:新建一个Bitcask，并调用上面分析过的log.build_keydir来从日志文件当中恢复内存当中的map new_compact:toydb的定义为learning project，对应的数据也为小规模的，因此在toydb的设计当中，只有在数据库启动时才会进行compact操作，并且这个过程是会锁住日志文件的，但是由于这个过程是算在启动当中的，也无伤大雅。在new_compact当中，会计算当前的garbage_ratio，如果超出阈值，就进行compact 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 pub fn new_compact(path: PathBuf, garbage_ratio_threshold: f64) -\u0026gt; Result\u0026lt;Self\u0026gt; { let mut s = Self::new(path)?; let status = s.status()?; let garbage_ratio = status.garbage_disk_size as f64 / status.total_disk_size as f64; if status.garbage_disk_size \u0026gt; 0 \u0026amp;\u0026amp; garbage_ratio \u0026gt;= garbage_ratio_threshold { log::info!( \u0026#34;Compacting {} to remove {:.3}MB garbage ({:.0}% of {:.3}MB)\u0026#34;, s.log.path.display(), status.garbage_disk_size / 1024 / 1024, garbage_ratio * 100.0, status.total_disk_size / 1024 / 1024 ); s.compact()?; log::info!( \u0026#34;Compacted {} to size {:.3}MB\u0026#34;, s.log.path.display(), (status.total_disk_size - status.garbage_disk_size) / 1024 / 1024 ); } Ok(s) } 接下来看第二部分，封装了一些写入操作来为compact提供支持，定义了两个函数：compact与write_log，二者的逻辑都很简单：\n在write_log当中，会遍历当前的map，去原本的日志文件当中读取，写入到新的日志文件当中，并且构建新的map 在compact当中，创建一个新的文件，调用write_log重建日志文件，并且保存 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 impl BitCask { /// Compacts the current log file by writing out a new log file containing /// only live keys and replacing the current file with it. pub fn compact(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut tmp_path = self.log.path.clone(); tmp_path.set_extension(\u0026#34;new\u0026#34;); let (mut new_log, new_keydir) = self.write_log(tmp_path)?; std::fs::rename(\u0026amp;new_log.path, \u0026amp;self.log.path)?; new_log.path = self.log.path.clone(); self.log = new_log; self.keydir = new_keydir; Ok(()) } /// Writes out a new log file with the live entries of the current log file /// and returns it along with its keydir. Entries are written in key order. fn write_log(\u0026amp;mut self, path: PathBuf) -\u0026gt; Result\u0026lt;(Log, KeyDir)\u0026gt; { let mut new_keydir = KeyDir::new(); let mut new_log = Log::new(path)?; new_log.file.set_len(0)?; // truncate file if it exists for (key, (value_pos, value_len)) in self.keydir.iter() { let value = self.log.read_value(*value_pos, *value_len)?; let (pos, len) = new_log.write_entry(key, Some(\u0026amp;value))?; new_keydir.insert(key.clone(), (pos + len as u64 - *value_len as u64, *value_len)); } Ok((new_log, new_keydir)) } } 第三部分则是engine的实现，在src/storage/mod当中定义了一个trait，而这一部分就是对bitcask进行一个简单的封装，来实现该trait，逻辑都比较简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pub trait Engine: std::fmt::Display + Send + Sync { /// The iterator returned by scan(). Traits can\u0026#39;t return \u0026#34;impl Trait\u0026#34;, and /// we don\u0026#39;t want to use trait objects, so the type must be specified. type ScanIterator\u0026lt;\u0026#39;a\u0026gt;: DoubleEndedIterator\u0026lt;Item = Result\u0026lt;(Vec\u0026lt;u8\u0026gt;, Vec\u0026lt;u8\u0026gt;)\u0026gt;\u0026gt; + \u0026#39;a where Self: \u0026#39;a; /// Deletes a key, or does nothing if it does not exist. fn delete(\u0026amp;mut self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Flushes any buffered data to the underlying storage medium. fn flush(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Gets a value for a key, if it exists. fn get(\u0026amp;mut self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;; /// Iterates over an ordered range of key/value pairs. fn scan\u0026lt;R: std::ops::RangeBounds\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;(\u0026amp;mut self, range: R) -\u0026gt; Self::ScanIterator\u0026lt;\u0026#39;_\u0026gt;; /// Sets a value for a key, replacing the existing value if any. fn set(\u0026amp;mut self, key: \u0026amp;[u8], value: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Returns engine status. fn status(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Status\u0026gt;; /// Iterates over all key/value pairs starting with prefix. fn scan_prefix(\u0026amp;mut self, prefix: \u0026amp;[u8]) -\u0026gt; Self::ScanIterator\u0026lt;\u0026#39;_\u0026gt; { let start = std::ops::Bound::Included(prefix.to_vec()); let end = match prefix.iter().rposition(|b| *b != 0xff) { Some(i) =\u0026gt; std::ops::Bound::Excluded( prefix.iter().take(i).copied().chain(std::iter::once(prefix[i] + 1)).collect(), ), None =\u0026gt; std::ops::Bound::Unbounded, }; self.scan((start, end)) } } Iterator 最后，定义了一个ScanIterator，用于进行范围读取，这里的写法还是比较的rusty的。额外定义了一个map函数，调用self.log.read_value()去磁盘当中进行读取，用于将BTreeMap当中的key与offset转换为真实的kv。\n由于inner和log都是引用类型，因此标注了生命周期\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 pub struct ScanIterator\u0026lt;\u0026#39;a\u0026gt; { inner: std::collections::btree_map::Range\u0026lt;\u0026#39;a, Vec\u0026lt;u8\u0026gt;, (u64, u32)\u0026gt;, log: \u0026amp;\u0026#39;a mut Log, } impl\u0026lt;\u0026#39;a\u0026gt; ScanIterator\u0026lt;\u0026#39;a\u0026gt; { fn map(\u0026amp;mut self, item: (\u0026amp;Vec\u0026lt;u8\u0026gt;, \u0026amp;(u64, u32))) -\u0026gt; \u0026lt;Self as Iterator\u0026gt;::Item { let (key, (value_pos, value_len)) = item; Ok((key.clone(), self.log.read_value(*value_pos, *value_len)?)) } } impl\u0026lt;\u0026#39;a\u0026gt; Iterator for ScanIterator\u0026lt;\u0026#39;a\u0026gt; { type Item = Result\u0026lt;(Vec\u0026lt;u8\u0026gt;, Vec\u0026lt;u8\u0026gt;)\u0026gt;; fn next(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { self.inner.next().map(|item| self.map(item)) } } impl\u0026lt;\u0026#39;a\u0026gt; DoubleEndedIterator for ScanIterator\u0026lt;\u0026#39;a\u0026gt; { fn next_back(\u0026amp;mut self) -\u0026gt; Option\u0026lt;Self::Item\u0026gt; { self.inner.next_back().map(|item| self.map(item)) } } memory 此外，在同目录下，还定义了一个memory，同样作为Engine的一个实现，表示一个纯内存的存储引擎，使用的就是BTreeMap，将key和value直接存储在内存当中，不会对数据进行持久化\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 pub struct Memory { data: std::collections::BTreeMap\u0026lt;Vec\u0026lt;u8\u0026gt;, Vec\u0026lt;u8\u0026gt;\u0026gt;, } impl Memory { /// Creates a new Memory key-value storage engine. pub fn new() -\u0026gt; Self { Self { data: std::collections::BTreeMap::new() } } } impl Engine for Memory { type ScanIterator\u0026lt;\u0026#39;a\u0026gt; = ScanIterator\u0026lt;\u0026#39;a\u0026gt;; // 自欺欺人了属于是 fn flush(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { Ok(()) } fn delete(\u0026amp;mut self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;()\u0026gt; { self.data.remove(key); Ok(()) } fn get(\u0026amp;mut self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt; { Ok(self.data.get(key).cloned()) } fn scan\u0026lt;R: std::ops::RangeBounds\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt;(\u0026amp;mut self, range: R) -\u0026gt; Self::ScanIterator\u0026lt;\u0026#39;_\u0026gt; { ScanIterator { inner: self.data.range(range) } } fn set(\u0026amp;mut self, key: \u0026amp;[u8], value: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { self.data.insert(key.to_vec(), value); Ok(()) } fn status(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Status\u0026gt; { Ok(Status { name: self.to_string(), keys: self.data.len() as u64, size: self.data.iter().fold(0, |size, (k, v)| size + k.len() as u64 + v.len() as u64), total_disk_size: 0, live_disk_size: 0, garbage_disk_size: 0, }) } } Status 在src/storage/engine/mod.rs当中，定义了一个Status，用于表示当前存储引擎的状态，bitcask和memory在创建时都是设置一个状态，看一看就好\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /// Engine status. #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub struct Status { /// The name of the storage engine. pub name: String, /// The number of live keys in the engine. pub keys: u64, /// The logical size of live key/value pairs. pub size: u64, /// The on-disk size of all data, live and garbage. pub total_disk_size: u64, /// The on-disk size of live data. pub live_disk_size: u64, /// The on-disk size of garbage data. pub garbage_disk_size: u64, } ","permalink":"http://itfischer.space/en/posts/tech/toydb/01-bitcask/","summary":"基础结构 Bitcask本身非常简单，要是想实现一个最基础的bitcask存储引擎，大概200-300行代码就能实现，这里先简单介绍一下Bit","title":"01-Bitcask"},{"content":"基础理论 简而言之，实现MVCC的DBMS在内部维持着单个逻辑数据的多个物理版本，当事务修改某条数据时，就创建一个新的版本。当事务读取时，就根据事务的开始时间，去读取事务开始时刻之前的最新版本。\nMVCC概括起来就是两句话：\nWriters don\u0026rsquo;t block readers. Readers don\u0026rsquo;t block writers. 只读事务无需加锁就可以读取数据库某一时刻的快照，如果保留数据的所有历史版本，DBMS甚至能够支持读取任意历史版本的数据，即time-travel(这一点在toydb当中也得到了实现，即不实现gc，保留之前所有的版本，开发者还特意强调了这是一个feature而不是bug)\n并发控制 MVCC(Multi-Version Concurrency Control)的名字具有一定的误导性，虽然叫做并发控制，但是本身并不是一个完整的并发控制协议，正如上面所说的，MVCC只能解决R-W之间的冲突问题，但是对于W-W，单靠MVCC本身是无法解决的，需要引入其他的并发协议，根据并发协议的种类，又可以大致分为：\nMV2PL：使用2PL悲观锁的形式来解决W-W冲突问题 MVOCC：使用基础的时间戳或者创建private workspace的形式(事务分为read,write,validate三个过程)，二者其实有细微的区别的，但是本质都是乐观的形式，就归在一起了 还有最简单的形式,如果发现当前事务要修改的record正在被其他事务修改，就放弃并之后重试(又不是不能用 :) )，也算是一种乐观的实现方式吧 这里先看一个toydb当中给出的例子来理解一下，\n1 2 3 4 5 6 7 //! Time //! 5 //! 4 a4 //! 3 b3 x //! 2 //! 1 a1 c1 d1 //! a b c d Keys 在t1时刻，某个事务写入了a=a1,c=c1,d=d1并提交 在t3时刻，某个事务写入了b=b3,删除了d 在t4时刻，某个事务写入了a=a4 在t2时刻，开启了事务T1，那么他就能够读取到a=a1,c=c1,d=d1 在 t 5 时刻，开启了事务 T 2, 那么他就能够读取到 a=a 4, b=b 3,c=c1 事务的时间或者版本的概念是根据事务begin决定的，比如说T2读取的物理时间可能落后于T5，但是由于T2事务begin早于T5，所以他就能够读取到的数据的版本就早于T5(其实这个也是根据并发控制协议决定的，如果使用OCC的话，事务的时间就是validate的时间)。\n记录真正变成可见是根据提交的时刻决定的，在事务未提交前，该事务写入的数据对于自己是可见的，但是对于其他的事务不可见，在看一个例子：\n1 2 3 4 5 6 7 8 9 //! Active set: [2, 5] //! //! Time //! 5 (a5) //! 4 a4 //! 3 b3 x //! 2 (x) (e2) //! 1 a1 c1 d1 //! a b c d e Keys 事务T2写入的数据，但是并未提交，T2维护在Active set当中,删除c1和写入e2对于自身是可见的，但是对于后面开启的事务T5是不可见的。\nMVCC in Miniob 在介绍toydb的MVCC的实现之前，先看一下Miniob的MVCC实现,虽然存在变更无法一次性对外暴露的问题，但是实现的比较简单，很好理解：\nMVCC当中的版本是基于tid的，在开启了MVCC模式之后，每一条record会生成两个sys_field，分别存储begin和end，来标识一个事务的可见性，这里似乎并没有一个统一的标准，只要能满足MVCC协议本身的要求即可，在Miniob当中的设置如下：\nrecord通过begin和end 两个id进行状态管理，begin用于表示事务开始的时间，end为事务结束的时间，对于一条record：\n当一个事务开始时，新的record的begin设置为-trx_id，end为max_int32,表示事务写入但未提交，而删除时则将end设置为-trx_id，表示删除但未提交 写入操作提交时，将begin设置为trx_id，删除操作提交时将end设置为trx_id，最终会产生五种状态 begin_xid end_xid 自身可见 其他事务可见 说明 -trx_id MAX 可见 不可见 由当前事务写入，但还未提交，对其他事务不可见 trx_id MAX 已提交 对新事务可见 写入并且已经提交，对之后的新事务可见 任意正数 trx_id 已提交 旧事务可见，新事务不可见 在当前事务中进行删除，并事务提交 任意正数 -trx_id 不可见 可见 在当前事务中进行删除，未提交 -trx_id -trx_id 不可见 不可见 由当前事务进行写入，并且又被当前事务删除，并且未提交 其中，已提交指的就是当前事务已经结束，自然不存在什么可见不可见的问题。而Miniob当中的隔离级别应当是和toydb一样的快照隔离，因为未提交的事务的 begin \u0026lt; 0，因此是永远无法读取到新写入的record，因此是不存在幻读情况的。\n对于record是否可见的判断，在visit_record当中提供了一段代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 if (begin_xid \u0026gt; 0 \u0026amp;\u0026amp; end_xid \u0026gt; 0) { if (trx_id_ \u0026gt;= begin_xid \u0026amp;\u0026amp; trx_id_ \u0026lt;= end_xid) { rc = RC::SUCCESS; } else { rc = RC::RECORD_INVISIBLE; } } else if (begin_xid \u0026lt; 0) { // begin xid 小于0说明是刚插入而且没有提交的数据 rc = (-begin_xid == trx_id_) ? RC::SUCCESS : RC::RECORD_INVISIBLE; } else if (end_xid \u0026lt; 0) { // end xid 小于0 说明是正在删除但是还没有提交的数据 if (readonly) { // 如果 -end_xid 就是当前事务的事务号，说明是当前事务删除的 rc = (-end_xid != trx_id_) ? RC::SUCCESS : RC::RECORD_INVISIBLE; } else { // 如果当前想要修改此条数据，并且不是当前事务删除的，简单的报错 // 这是事务并发处理的一种方式，非常简单粗暴。其它的并发处理方法，可以等待，或者让客户端重试 // 或者等事务结束后，再检测修改的数据是否有冲突 rc = (-end_xid != trx_id_) ? RC::LOCKED_CONCURRENCY_CONFLICT : RC::RECORD_INVISIBLE; } } 如果想进一步了解Miniob的事务模块的话，可以看这个：miniob-transaction.md 除此之外，23fall的15-455同样也提供了MVCC，基于MVOCC实现的，以单个Field为单位实现的多版本，笔者目前即没做也没细看，读者如果感兴趣的话可以看以下链接：Project #4 - Concurrency Control | CMU 15-445/645 :: Intro to Database Systems (Fall 2023)\n实现 首先补充点理论：\n在toydb当中，MVCC所提供的隔离级别为快照隔离，事务只能看到数据库的一个一致性快照，而这个快照是根据事务创建的时间决定的，即事务只能够看到事务创建前的最新的数据，以及由自己写入的新数据，目前还未提交的活跃事务之间相互隔离互不影响。 Toydb 并没有实现 GC 功能，会保存数据的所有版本，因此就可以支持 time travel query，即传入一个时间戳，然后获取一个那时的快照，进行只读请求 (由于基于旧版本进行写请求会扰乱当前数据库的状态，如进行 set x = x + 1，原本 x = 3，但是目前已经是 x = 5 了，因此 time travel 只支持只读事务)，开发者特意强调了这是一个 feature 而不是 bug，不过感觉多少有点欲盖弥彰了。 好了，接下来来看一下具体的实现，在toydb当中，事务的相关逻辑全部定义在src/storage/mvcc.rs当中，只有一个文件，其他的像是debug.rs，keycode.rs只是提供一些辅助支持，用到的时候再看看。\n在介绍MVCC以及事务是如何实现之前，先梳理一下定义的结构体和之间的关系\nTransaction 事务最基础的结构体为Transaction：\nEngine为一个Trait，在其中提供了基础的CRUD功能，而上一章介绍的Bitcask和Memory都实现了这个Trait，具体使用的哪个上层应用无需关心 TransactionState用于表示事务的状态 1 2 3 4 5 6 7 /// An MVCC transaction. pub struct Transaction\u0026lt;E: Engine\u0026gt; { /// The underlying engine, shared by all transactions. engine: Arc\u0026lt;Mutex\u0026lt;E\u0026gt;\u0026gt;, /// The transaction state. st: TransactionState, } TransactionState 在注释当中，对于TransactionState的设计理念做了比较详细的说明，简而言之就是，事务状态的设计使得事务可以在不同的组件之间安全地传递，并且可以在不直接引用事务本身的情况下被引用，有助于简化事务管理。\nTransacationState当中提供了一个函数，用于判断给定的version对于当前事务是否可见，逻辑如下：\n如果version来自活跃事务，即处于active_set当中，那么代表为新写入的，不可见 如果为只读事务，那么能看到小于version的(之前事务创建的) 如果是普通事务，那么可以看到之前的和自身写入(\u0026lt;=) A Transaction\u0026rsquo;s state, which determines its write version and isolation. It is separate from Transaction to allow it to be passed around independently of the engine. There are two main motivations for this:\nIt can be exported via Transaction.state(), (de)serialized, and later used to instantiate a new functionally equivalent Transaction via Transaction::resume(). This allows passing the transaction between the storage engine and SQL engine (potentially running on a different node) across the Raft state machine boundary. It can be borrowed independently of Engine, allowing references to it in VisibleIterator, which would otherwise result in self-references. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub struct TransactionState { /// The version this transaction is running at. Only one read-write /// transaction can run at a given version, since this identifies its /// writes. pub version: Version, /// If true, the transaction is read only. pub read_only: bool, /// The set of concurrent active (uncommitted) transactions, as of the start /// of this transaction. Their writes should be invisible to this /// transaction even if they\u0026#39;re writing at a lower version, since they\u0026#39;re /// not committed yet. pub active: HashSet\u0026lt;Version\u0026gt;, } impl TransactionState { /// Checks whether the given version is visible to this transaction. /// /// Future versions, and versions belonging to active transactions as of /// the start of this transaction, are never isible. /// /// Read-write transactions see their own writes at their version. /// /// Read-only queries only see versions below the transaction\u0026#39;s version, /// excluding the version itself. This is to ensure time-travel queries see /// a consistent version both before and after any active transaction at /// that version commits its writes. See the module documentation for /// details. fn is_visible(\u0026amp;self, version: Version) -\u0026gt; bool { if self.active.get(\u0026amp;version).is_some() { false } else if self.read_only { version \u0026lt; self.version } else { version \u0026lt;= self.version } } } MVCC MVCC可以认为是一个wrapper，具体的逻辑是由上面的Transaction来实现的，调用Transaction当中对应的函数，在其中定义了一个存储引擎的shared_ptr，在调用时会传递给Transaction,为什么需要使用Mutex也在注释当中给出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 /// An MVCC-based transactional key-value engine. It wraps an underlying storage /// engine that\u0026#39;s used for raw key/value storage. /// /// While it supports any number of concurrent transactions, individual read or /// write operations are executed sequentially, serialized via a mutex. There /// are two reasons for this: the storage engine itself is not thread-safe, /// requiring serialized access, and the Raft state machine that manages the /// MVCC engine applies commands one at a time from the Raft log, which will /// serialize them anyway. pub struct MVCC\u0026lt;E: Engine\u0026gt; { engine: Arc\u0026lt;Mutex\u0026lt;E\u0026gt;\u0026gt;, } impl\u0026lt;E: Engine\u0026gt; MVCC\u0026lt;E\u0026gt; { /// Creates a new MVCC engine with the given storage engine. pub fn new(engine: E) -\u0026gt; Self { Self { engine: Arc::new(Mutex::new(engine)) } } /// Begins a new read-write transaction. pub fn begin(\u0026amp;self) -\u0026gt; Result\u0026lt;Transaction\u0026lt;E\u0026gt;\u0026gt; { Transaction::begin(self.engine.clone()) } /// Begins a new read-only transaction at the latest version. pub fn begin_read_only(\u0026amp;self) -\u0026gt; Result\u0026lt;Transaction\u0026lt;E\u0026gt;\u0026gt; { Transaction::begin_read_only(self.engine.clone(), None) } /// Begins a new read-only transaction as of the given version. pub fn begin_as_of(\u0026amp;self, version: Version) -\u0026gt; Result\u0026lt;Transaction\u0026lt;E\u0026gt;\u0026gt; { Transaction::begin_read_only(self.engine.clone(), Some(version)) } /// Resumes a transaction from the given transaction state. pub fn resume(\u0026amp;self, state: TransactionState) -\u0026gt; Result\u0026lt;Transaction\u0026lt;E\u0026gt;\u0026gt; { Transaction::resume(self.engine.clone(), state) } /// Fetches the value of an unversioned key. pub fn get_unversioned(\u0026amp;self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt; { self.engine.lock()?.get(\u0026amp;Key::Unversioned(key.into()).encode()?) } /// Sets the value of an unversioned key. pub fn set_unversioned(\u0026amp;self, key: \u0026amp;[u8], value: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { self.engine.lock()?.set(\u0026amp;Key::Unversioned(key.into()).encode()?, value) } /// Returns the status of the MVCC and storage engines. pub fn status(\u0026amp;self) -\u0026gt; Result\u0026lt;Status\u0026gt; { let mut engine = self.engine.lock()?; let versions = match engine.get(\u0026amp;Key::NextVersion.encode()?)? { Some(ref v) =\u0026gt; bincode::deserialize::\u0026lt;u64\u0026gt;(v)? - 1, None =\u0026gt; 0, }; let active_txns = engine.scan_prefix(\u0026amp;KeyPrefix::TxnActive.encode()?).count() as u64; Ok(Status { versions, active_txns, storage: engine.status()? }) } } Status 不怎么重要，看一下就好，作为函数返回值来表示当前事务的状态，其中的storage是存储引擎的storage，在上一章介绍过了\n1 2 3 4 5 6 7 8 9 10 /// MVCC engine status. #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub struct Status { /// The total number of MVCC versions (i.e. read-write transactions). pub versions: u64, /// Number of currently active transactions. pub active_txns: u64, /// The storage engine. pub storage: super::engine::Status, } Key 能够借助kv存储引擎实现MVCC的核心，实现上采用enum，rust当中的枚举非常强大，在 Rust 中，枚举是一种数据类型，它可以有不同的值（称为变体），但在任何给定时刻只能有其中一个值。每个枚举变体可以关联不同类型和数量的数据。\n借助rust的enum，就可以在创建key时向其中传递一个值，既可以表示当前的动作或者状态，又可以获得这个动作需要的值。举个例子，在需要写入一个key的新的version时，那么就需要以当前的key + version来作为key，那么就可以很自然的使用enum来表示一个复合的key，之后encode进行存储：\n1 2 3 4 5 6 7 8 9 /// A versioned key/value pair. Version( #[serde(with = \u0026#34;serde_bytes\u0026#34;)] #[serde(borrow)] Cow\u0026lt;\u0026#39;a, [u8]\u0026gt;, Version, ), // 某处调用 session.set(\u0026amp;Key::Version(key.into(),self.st.version).encode()?, bincode::serialize(\u0026amp;value)?) 而对于next_version而言，并不需要额外的值，只需要key为next_version，而value为一个u64即可，那么就不定义附加的值：\n1 2 3 4 5 6 7 /// The next available version. NextVersion, // 某处调用 let versions = match engine.get(\u0026amp;Key::NextVersion.encode()?)? { Some(ref v) =\u0026gt; bincode::deserialize::\u0026lt;u64\u0026gt;(v)? - 1, None =\u0026gt; 0, }; Key的完整定义如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 /// MVCC keys, using the KeyCode encoding which preserves the ordering and /// grouping of keys. Cow byte slices allow encoding borrowed values and /// decoding into owned values. #[derive(Debug, Deserialize, Serialize)] pub enum Key\u0026lt;\u0026#39;a\u0026gt; { /// The next available version. NextVersion, /// Active (uncommitted) transactions by version. TxnActive(Version), /// A snapshot of the active set at each version. Only written for /// versions where the active set is non-empty (excluding itself). TxnActiveSnapshot(Version), /// Keeps track of all keys written to by an active transaction (identified /// by its version), in case it needs to roll back. TxnWrite( Version, #[serde(with = \u0026#34;serde_bytes\u0026#34;)] #[serde(borrow)] Cow\u0026lt;\u0026#39;a, [u8]\u0026gt;, ), /// A versioned key/value pair. Version( #[serde(with = \u0026#34;serde_bytes\u0026#34;)] #[serde(borrow)] Cow\u0026lt;\u0026#39;a, [u8]\u0026gt;, Version, ), /// Unversioned non-transactional key/value pairs. These exist separately /// from versioned keys, i.e. the unversioned key \u0026#34;foo\u0026#34; is entirely /// independent of the versioned key \u0026#34;foo@7\u0026#34;. These are mostly used /// for metadata. Unversioned( #[serde(with = \u0026#34;serde_bytes\u0026#34;)] #[serde(borrow)] Cow\u0026lt;\u0026#39;a, [u8]\u0026gt;, ), } 此外，还有一个KeyPrefix用于进行前缀匹配，和Key差不多，这里就不做介绍了。\nMVCC Impl 终于，要开始分析MVCC的实现了，这一部分其实就是Transaction的impl,在这一部分，笔者会把重点放在MVCC与存储引擎的交互上，即如何使用KV存储引擎来实现MVCC。\n像前面说的那样，toydb支持time travel的只读事务，因此在开启事务这块，提供了两个函数，分别对应read-write的事务和read-only的事务\nBegin begin用于开启一个read-write的事务，大致干了一下几件事：\n获取一个Version作为当前事务的tid,或者可以视为一个时间戳，之后+1写回，由于toydb并没有实现buffer pool + wal，因此这里的NextVersion是存储在存储引擎当中的，采用的是同步写入的方式(这里单指bitcask,使用memory的话就没有什么持久化可言了，不过在MVCC模块当中，只需要在意engine的trait，通常不太需要考虑底层) 从存储引擎当中扫描，恢复出当前的active_set，这里active_set采用的是分布存储的，即没当开启一个事务后，就向存储引擎当中写入一条Key::TxnActive，带上自己的version，之后扫描出所有Key::TxnActive的key，恢复出active_set，个人认为这样设计的原因是bitcask本身是一个append-only的存储引擎，就算是将value设置为完整的active_set，那么每次写入也是追加写入，并且需要完整的写入整个active_set，写入量反而增大，不如采用分布存储，代价是读取时需要进行一个扫描，不过active_set只会在事务begin的时候进行读取，也无伤大雅 根据active_set来生成一个snapshot，用于进行time-travel read，time travel需要做的是读取到给定时间戳(version)时数据库的完整状态，不能够简单的通过版本进行读取，因为有些key虽然是由给定version之前的事务写入的，但是事务未提交，那么该key就不可见，这也是snapshot存在的意义，用于恢复某一时间的事务隔离情况，判断数据的可见性 将自己的version写入，补充active_set 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 /// Begins a new transaction in read-write mode. This will allocate a new /// version that the transaction can write at, add it to the active set, and /// record its active snapshot for time-travel queries. fn begin(engine: Arc\u0026lt;Mutex\u0026lt;E\u0026gt;\u0026gt;) -\u0026gt; Result\u0026lt;Self\u0026gt; { let mut session = engine.lock()?; // Allocate a new version to write at. let version = match session.get(\u0026amp;Key::NextVersion.encode()?)? { Some(ref v) =\u0026gt; bincode::deserialize(v)?, None =\u0026gt; 1, }; session.set(\u0026amp;Key::NextVersion.encode()?, bincode::serialize(\u0026amp;(version + 1))?)?; // Fetch the current set of active transactions, persist it for // time-travel queries if non-empty, then add this txn to it. let active = Self::scan_active(\u0026amp;mut session)?; if !active.is_empty() { session.set(\u0026amp;Key::TxnActiveSnapshot(version).encode()?, bincode::serialize(\u0026amp;active)?)? } session.set(\u0026amp;Key::TxnActive(version).encode()?, vec![])?; drop(session); Ok(Self { engine, st: TransactionState { version, read_only: false, active } }) } Begin_read_only begin_read_only用于进行read only的事务，如果传入了一个version,那么就进行time-travel，否则根据数据库最新的状态进行读取：\n获取数据库最新的version，如果传入了as_of就替换成传入的version，用于进行time travel(传入的version是不能大于数据库最新的version的) 之后如果是time travel，就根据version去读取snapshot，来恢复active_set，否则和begin一样，去扫描获取最新的active_set 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 /// Begins a new read-only transaction. If version is given it will see the /// state as of the beginning of that version (ignoring writes at that /// version). In other words, it sees the same state as the read-write /// transaction at that version saw when it began. fn begin_read_only(engine: Arc\u0026lt;Mutex\u0026lt;E\u0026gt;\u0026gt;, as_of: Option\u0026lt;Version\u0026gt;) -\u0026gt; Result\u0026lt;Self\u0026gt; { let mut session = engine.lock()?; // Fetch the latest version. let mut version = match session.get(\u0026amp;Key::NextVersion.encode()?)? { Some(ref v) =\u0026gt; bincode::deserialize(v)?, None =\u0026gt; 1, }; // If requested, create the transaction as of a past version, restoring // the active snapshot as of the beginning of that version. Otherwise, // use the latest version and get the current, real-time snapshot. let mut active = HashSet::new(); if let Some(as_of) = as_of { if as_of \u0026gt;= version { return Err(Error::Value(format!(\u0026#34;Version {} does not exist\u0026#34;, as_of))); } version = as_of; if let Some(value) = session.get(\u0026amp;Key::TxnActiveSnapshot(version).encode()?)? { active = bincode::deserialize(\u0026amp;value)?; } } else { active = Self::scan_active(\u0026amp;mut session)?; } drop(session); Ok(Self { engine, st: TransactionState { version, read_only: true, active } }) } Write \u0026amp;\u0026amp; Delete 由于MVCC的append-only的特性(没有gc)，对于Write和Delete进行统一封装，Delete视为写入一个tombstone(MVCC层面的tombstone，调用的存储引擎还是set接口，不会在存储引擎当中删除)，底层都是通过write_version来实现的:\n1 2 3 4 5 6 7 8 9 /// Deletes a key. pub fn delete(\u0026amp;self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;()\u0026gt; { self.write_version(key, None) } /// Sets a value for a key. pub fn set(\u0026amp;self, key: \u0026amp;[u8], value: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { self.write_version(key, Some(value)) } write_version\n在write_version当中，首先进行写入冲突的检查，即检查是否有其他的事务正在对当前的key进行写入操作，实现方法也很简单，扫描(key,active.min)到(key,u64::MAX)范围内的key，对于扫描出来的key当中最新的版本：\n如果对于当时事务可见，那么就证明为自身写入的，或者是比自己早并且已经提交的事务写入的，不存在冲突 如果对当前事务不可见，那么就是其他的活跃事务写入的，证明有其他事务在并发写入，存在冲突(只要version存在于active_set当中就是不可见的，否则再根据version大小去判断) 在判断没有冲突之后，分别写入一条TrnWrite和Version，TxnWrite用于标志当前的事务进行写入，用于进行回滚，而Version才是真正存储数据的key 不过有个问题是，为了实现MVCC，即便是当前要删除的key不存在，也会去写入一条version，给存储引擎带来额外的负担\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /// Writes a new version for a key at the transaction\u0026#39;s version. None writes /// a deletion tombstone. If a write conflict is found (either a newer or /// uncommitted version), a serialization error is returned. Replacing our /// own uncommitted write is fine. fn write_version(\u0026amp;self, key: \u0026amp;[u8], value: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { if self.st.read_only { return Err(Error::ReadOnly); } let mut session = self.engine.lock()?; // Check for write conflicts, i.e. if the latest key is invisible to us // (either a newer version, or an uncommitted version in our past). We // can only conflict with the latest key, since all transactions enforce // the same invariant. let from = Key::Version( key.into(), self.st.active.iter().min().copied().unwrap_or(self.st.version + 1), ) .encode()?; let to = Key::Version(key.into(), u64::MAX).encode()?; if let Some((key, _)) = session.scan(from..=to).last().transpose()? { match Key::decode(\u0026amp;key)? { Key::Version(_, version) =\u0026gt; { if !self.st.is_visible(version) { return Err(Error::Serialization); } } key =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Expected Key::Version got {:?}\u0026#34;, key))), } } // Write the new version and its write record. // // NB: TxnWrite contains the provided user key, not the encoded engine // key, since we can construct the engine key using the version. session.set(\u0026amp;Key::TxnWrite(self.st.version, key.into()).encode()?, vec![])?; session .set(\u0026amp;Key::Version(key.into(), self.st.version).encode()?, bincode::serialize(\u0026amp;value)?) } Get Get的逻辑就很简单了，找到一个key的可能看见的版本(从0到自己写入的，version范围对应[0,self.version])，从新到旧遍历，找到第一个自己能够看见的版本，返回。要是全部遍历完都没有那就是不存在能够读取到的版本\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 /// Fetches a key\u0026#39;s value, or None if it does not exist. pub fn get(\u0026amp;self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt; { let mut session = self.engine.lock()?; let from = Key::Version(key.into(), 0).encode()?; let to = Key::Version(key.into(), self.st.version).encode()?; // 调用rev从新的key开始遍历 let mut scan = session.scan(from..=to).rev(); while let Some((key, value)) = scan.next().transpose()? { match Key::decode(\u0026amp;key)? { Key::Version(_, version) =\u0026gt; { if self.st.is_visible(version) { return bincode::deserialize(\u0026amp;value); } } key =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Expected Key::Version got {:?}\u0026#34;, key))), }; } Ok(None) } Commit \u0026amp;\u0026amp; Rollback 对于read only事务，由于其对系统不会产生任何影响，也不会把自己添加到active_set当中，因此直接返回即可。\ncommit\ncommit需要做的有：\n扫描出来所有由当前事务写入的Key::TxnWrite，TxnWrite是用于事务回滚时撤销写入的，既然事务提交了就不需要了，所以全部删除 将自己从active_set当中移除，删除掉TxnActive(self.version)即可，这也是active_set分布存储的好处，更改只需要操作一个key 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 /// Commits the transaction, by removing it from the active set. This will /// immediately make its writes visible to subsequent transactions. Also /// removes its TxnWrite records, which are no longer needed. pub fn commit(self) -\u0026gt; Result\u0026lt;()\u0026gt; { if self.st.read_only { return Ok(()); } let mut session = self.engine.lock()?; let remove = session .scan_prefix(\u0026amp;KeyPrefix::TxnWrite(self.st.version).encode()?) .map(|r| r.map(|(k, _)| k)) .collect::\u0026lt;Result\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;\u0026gt;()?; for key in remove { session.delete(\u0026amp;key)? } session.delete(\u0026amp;Key::TxnActive(self.st.version).encode()?) } rollback\n在事务执行过程中，无论是写入还是删除，每次都是写入一个version，同时写入一个Key::TxnWrite，用于标志事务对数据的更改，因此在回滚时，只需要当前事务之前写入的Key::TxnWrite全部读取出来，转换为Key::Version，然后删除这个key的对应version，就完成了undo的动作。之后再将自己从active_set当中移除即可。\n还是要提一下，这里的删除是mvcc层面的删除，只会删除相同user_key相同version的Key，不会像bitcask那样写入一个tombstone前面所有的key都不可达，分析mvcc就要屏蔽掉底层的存储引擎，只将其视为一个简单的kv存储。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /// Rolls back the transaction, by undoing all written versions and removing /// it from the active set. The active set snapshot is left behind, since /// this is needed for time travel queries at this version. pub fn rollback(self) -\u0026gt; Result\u0026lt;()\u0026gt; { if self.st.read_only { return Ok(()); } let mut session = self.engine.lock()?; let mut rollback = Vec::new(); let mut scan = session.scan_prefix(\u0026amp;KeyPrefix::TxnWrite(self.st.version).encode()?); while let Some((key, _)) = scan.next().transpose()? { match Key::decode(\u0026amp;key)? { Key::TxnWrite(_, key) =\u0026gt; { rollback.push(Key::Version(key, self.st.version).encode()?) // the version } key =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Expected TxnWrite, got {:?}\u0026#34;, key))), }; rollback.push(key); // the TxnWrite record } drop(scan); for key in rollback.into_iter() { session.delete(\u0026amp;key)?; } session.delete(\u0026amp;Key::TxnActive(self.st.version).encode()?) // remove from active set } 原子性变更 到这里，MVCC的实现方式基本上已经分析完了，现在来说一下Miniob当中遗留的一个问题，就是事务在提交或者回滚时作出的更改没有办法被一次性读到。\n对于传统的2PL，通过加锁的方式，阻止其他事务进行读取，从而保证在释放锁时一次性的将更新暴露给其他的事务，而Miniob引入MVCC就是为了避免读写冲突，因此不会加锁，在写入的过程中，其他事务自然可以进行读取，从而读取到不应该存在的中间态。\n再来说一下toydb是怎样解决这个问题的，toydb同样没有引入复杂的并发控制，对于W-W冲突，解决方案也是简单的进行重试。但是在可见性上，toydb引入了额外的限制，即如果对应的version存在于active_set当中，那么就是不可见的(对其他事务).\n因此即便是将新的key非原子性的写入到存储引擎当中，只要不从active_set当中删除，那么就是不可见的。而更改active_set的这个动作是原子性的，因此就可以保证事务提交时作出的更改一次性的对外可见。\n可重复读\n事务在begin时，会从存储引擎当中读取并建立出active_set，并且之后在事务执行过程当中，都以这个active_set为准，因此即便是其他事务提交了，写入的记录对当前事务还是不可见的，就保证了可重复读的问题。\n这里的设计还是非常巧妙的，用简单的方法解决了问题，active_set既保证了原子性的提交，同时提供了可重复读\nSummary toydb借助一个kv存储实现了mvcc，对于MVCC而言笔者认为有几个关键的问题，toydb也分别给出了对应的解决方案：\n隔离级别：在toydb当中提供了快照隔离，在这种隔离级别下，保证了可重复读，并且避免了幻读的问题，但是代价是对于W-W冲突，会产生比较频繁的重试问题 并发控制：这里的实现与Miniob一样，W-R冲突通过MVCC本身解决，W-W冲突采用了最简单的冲突重试 原子性提交：记录写入并不是原子性的，但是active_set的更新是原子性的，通过active_set的更新保证记录能够一次性对外全部暴露出来 垃圾回收：没有垃圾回收，这是 feature 而不是 bug:)，借助此特点，实现了任意时间的 time travel toydb当中Key的设计也是MVCC的重要支持，通过复合类型Key的形式，toydb实现了类似leveldb当中(user_key,sequence)的形式，但是得益于rust当中枚举类型的强大，不仅可以携带上版本信息，并且能够表示key的不同意义，如TxnWrite,TxnActive等。有了复合类型Key，便可以很轻松的借助kv存储引擎来实现MVCC。\n在整个MVCC模块当中，KV存储既用于存储实际的数据，即一个个version，同时还起到了log的作用，会记录Txn::Write用于进行事务回滚，并且还会存储NextVersion,active set这样的元数据 ，可谓是用处多多了，凡事涉及到存储或者持久化的内容，都丢进kv engine\ntoydb的MVCC实现的非常简洁，真正与MVCC相关的逻辑只有400余行，除了上面分析的，还有一个Iterator的实现，用于支持范围扫描和前缀扫描，逻辑并不是很复杂，笔者就不额外展开了，此外toydb当中也提供了较为完善的测试，通过测试也可以更好的理解MVCC，rust的调试也很方便，无需任何配置，读者可自行阅读调试。\n","permalink":"http://itfischer.space/en/posts/tech/toydb/02-mvcc/","summary":"基础理论 简而言之，实现MVCC的DBMS在内部维持着单个逻辑数据的多个物理版本，当事务修改某条数据时，就创建一个新的版本。当事务读取时，就根","title":"02-MVCC"},{"content":"toydb的Raft实现，相比于6.824更接近于生产级别的，和etcd/raft在结构上比较相似，但是并没有实现Raft大论文当中的优化，并且相比Raft小论文也砍掉了一些优化，比如fast backup,snapshot等。不过在写法上还是很有rust的味道的。可以分为一下几个模块：\nNode：分别定义Leader、Candidate、Follower各自的行为和通用的行为(定义在mod.rs当中) log：以kv engine作为底层，持久化存储Raft的log，同时也会用于持久化存储元数据，所有需要进行持久化存储的都会使用log模块 State：Raft状态机，构建于Raft之上，相当于6.824当中的Lab3 Server：通信模块，负责与其他Raft节点之间进行通信，基于tcp实现 在本章当中，笔者会首先分析toydb当中的Raft的一些基础模块，之后再按照选举和日志复制的逻辑来进行分析Raft的逻辑。对于Raft本身，笔者不会做过多介绍，如果想了解Raft本身建议看Raft小论文/大论文，6.824 raft.pdf web.stanford.edu/~ouster/cgi-bin/papers/OngaroPhD.pdf Message 在toydb当中，Raft模块使用Event作为事件的载体，而Message为Event的封装，整个Raft的执行过程就是不断处理Message的过程，这与etcd当中的设计非常相似，所以先来看一下Message相关内容，定义在src/raft/message.rs当中。\nMessage的发送目标采用一个枚举来表示\nBroadcast：为广播类型，需要发送给所有的节点 Node：为单独发送，指定NodeID，在上层网络模块再转换成ip进行发送 Client：用于回复Client 1 2 3 4 5 6 7 8 9 10 11 #[derive(Clone, Copy, Debug, Eq, Hash, PartialEq, Serialize, Deserialize)] pub enum Address { /// Broadcast to all peers. Only valid as an outbound recipient (to). Broadcast, /// A node with the specified node ID (local or remote). Valid both as /// sender and recipient. Node(NodeID), /// A local client. Can only send ClientRequest messages, and receive /// ClientResponse messages. Client, } Message定义为一个结构体,其中包含发送Message时的Term，发送方与接收方，以及Event\n1 2 3 4 5 6 7 8 9 10 11 12 13 /// A message passed between Raft nodes. #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub struct Message { /// The current term of the sender. Must be set, unless the sender is /// Address::Client, in which case it must be 0. pub term: Term, /// The sender address. pub from: Address, /// The recipient address. pub to: Address, /// The message payload. pub event: Event, } Message的主体是Event，Event定义为一个枚举类型，在其中表示了Raft当中的所有事件，如果需要额外的信息，就封装成一个结构体，否则设置成一个枚举字段即可。如心跳信息，candidate开启选举等等，在代码当中每一种类型都给出了详细的注释，读者可自行阅读\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub enum Event { /// Leaders send periodic heartbeats to its followers. Heartbeat { /// The index of the leader\u0026#39;s last committed log entry. commit_index: Index, /// The term of the leader\u0026#39;s last committed log entry. commit_term: Term, }, /// Followers confirm loyalty to leader after heartbeats. ConfirmLeader { /// The commit_index of the original leader heartbeat, to confirm /// read requests. commit_index: Index, /// If false, the follower does not have the entry at commit_index /// and would like the leader to replicate it. has_committed: bool, }, /// Candidates solicit votes from all peers when campaigning for leadership. SolicitVote { // The index of the candidate\u0026#39;s last stored log entry last_index: Index, // The term of the candidate\u0026#39;s last stored log entry last_term: Term, }, /// Followers may grant a single vote to a candidate per term, on a /// first-come basis. Candidates implicitly vote for themselves. GrantVote, /// Leaders replicate log entries to followers by appending it to their log. AppendEntries { /// The index of the log entry immediately preceding the submitted commands. base_index: Index, /// The term of the log entry immediately preceding the submitted commands. base_term: Term, /// Commands to replicate. entries: Vec\u0026lt;Entry\u0026gt;, }, /// Followers may accept a set of log entries from a leader. AcceptEntries { /// The index of the last log entry. last_index: Index, }, /// Followers may also reject a set of log entries from a leader. RejectEntries, /// A client request. This can be submitted to the leader, or to a follower /// which will forward it to its leader. If there is no leader, or the /// leader or term changes, the request is aborted with an Error::Abort /// ClientResponse and the client must retry. ClientRequest { /// The request ID. This is arbitrary, but must be globally unique for /// the duration of the request. id: RequestID, /// The request. request: Request, }, /// A client response. ClientResponse { /// The response ID. This matches the ID of the ClientRequest. id: RequestID, /// The response, or an error. response: Result\u0026lt;Response\u0026gt;, }, } 对比一下etcd当中的Message类型,在rust当中使用enum Event进行封装是不是简洁很多呢，event同一事件只会有一种类型和数据，避免携带不必要的数据。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 type Message struct { Type MessageType `protobuf:\u0026#34;varint,1,opt,name=type,enum=raftpb.MessageType\u0026#34; json:\u0026#34;type\u0026#34;` To uint64 `protobuf:\u0026#34;varint,2,opt,name=to\u0026#34; json:\u0026#34;to\u0026#34;` From uint64 `protobuf:\u0026#34;varint,3,opt,name=from\u0026#34; json:\u0026#34;from\u0026#34;` Term uint64 `protobuf:\u0026#34;varint,4,opt,name=term\u0026#34; json:\u0026#34;term\u0026#34;` // logTerm is generally used for appending Raft logs to followers. For example, // (type=MsgApp,index=100,logTerm=5) means leader appends entries starting at // index=101, and the term of entry at index 100 is 5. // (type=MsgAppResp,reject=true,index=100,logTerm=5) means follower rejects some // entries from its leader as it already has an entry with term 5 at index 100. LogTerm uint64 `protobuf:\u0026#34;varint,5,opt,name=logTerm\u0026#34; json:\u0026#34;logTerm\u0026#34;` Index uint64 `protobuf:\u0026#34;varint,6,opt,name=index\u0026#34; json:\u0026#34;index\u0026#34;` Entries []Entry `protobuf:\u0026#34;bytes,7,rep,name=entries\u0026#34; json:\u0026#34;entries\u0026#34;` Commit uint64 `protobuf:\u0026#34;varint,8,opt,name=commit\u0026#34; json:\u0026#34;commit\u0026#34;` Snapshot Snapshot `protobuf:\u0026#34;bytes,9,opt,name=snapshot\u0026#34; json:\u0026#34;snapshot\u0026#34;` Reject bool `protobuf:\u0026#34;varint,10,opt,name=reject\u0026#34; json:\u0026#34;reject\u0026#34;` RejectHint uint64 `protobuf:\u0026#34;varint,11,opt,name=rejectHint\u0026#34; json:\u0026#34;rejectHint\u0026#34;` Context []byte `protobuf:\u0026#34;bytes,12,opt,name=context\u0026#34; json:\u0026#34;context,omitempty\u0026#34;` } Node Node这一部分表示在Raft运行过程中，Raft节点的角色和执行动作，在这一部分仅简单介绍涉及到的结构体，而执行逻辑分为选举和日志复制两部分进行分析。\ntoydb当中使用RoleNode来表示节点的状态和角色,其中:\nlog为一个与kv engine交互的模块，负责所有持久化存储的工作 node_tx用于进行节点之间的通信(节点之间真实传输使用的是tcp，这个mpsc更像是一个信箱，将Message添加到其中，等待之后发送，这一部分的设计也与etcd相同，不过etcd是一个批处理的形式，会攒一波消息一同发送) state_tx用于与上层状态机进行通信 1 2 3 4 5 6 7 8 9 10 // A Raft node with role R pub struct RoleNode\u0026lt;R\u0026gt; { id: NodeID, peers: HashSet\u0026lt;NodeID\u0026gt;, term: Term, log: Log, node_tx: mpsc::UnboundedSender\u0026lt;Message\u0026gt;, state_tx: mpsc::UnboundedSender\u0026lt;Instruction\u0026gt;, role: R, } Role代表当前节点的角色，分别有Leader、Candidate、Follower，封装了一些每种不同角色特有的内容：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 pub struct Leader { /// Peer replication progress. progress: HashMap\u0026lt;NodeID, Progress\u0026gt;, /// Number of ticks since last periodic heartbeat. since_heartbeat: Ticks, } pub struct Follower { /// The leader, or None if just initialized. leader: Option\u0026lt;NodeID\u0026gt;, /// The number of ticks since the last message from the leader. leader_seen: Ticks, /// The leader_seen timeout before triggering an election. election_timeout: Ticks, /// The node we voted for in the current term, if any. voted_for: Option\u0026lt;NodeID\u0026gt;, // Local client requests that have been forwarded to the leader. These are // aborted on leader/term changes. pub(super) forwarded: HashSet\u0026lt;RequestID\u0026gt;, } pub struct Candidate { /// Votes received (including ourself). votes: HashSet\u0026lt;NodeID\u0026gt;, /// Ticks elapsed since election start. election_duration: Ticks, /// Election timeout, in ticks. election_timeout: Ticks, } 在RoleNode当中，配备了几个基础的函数，后面流程中可能用到，先提一下，都比较简单：\nbecome_role():切换角色，更新role quorum():计算集群“大多数”的数量 send():将Message塞入信箱当中，之后会被读取并发送 assert_node():检查当前的term与持久化存储的term是否相同 assert_step():在step函数处理Message之前先对Message进行校验 检查Message的接收方：判断是否应该由当前节点处理，step函数不应该处理发送给client的Message和不属于自己的Message 检查Message的发送方：不应该有Broadcast类型的发送方，Broadcast只应该作为接收方存在；对client发送的Message进行检查；检查由其他节点发送而来的消息，该节点是妇女存在于集群当中，以及需要携带term 角色切换 Leader\nLeader只有一种角色切换，即接收到更高的term从而退位，转换成follower，定义在RoleNode\u0026lt;Leader\u0026gt;.become_follower()当中,由于因为检测到更高的term，因此需要更新自身的term并进行持久化的工作：\n1 2 3 4 5 6 7 8 9 10 11 12 /// Transforms the leader into a follower. This can only happen if we find a /// new term, so we become a leaderless follower. fn become_follower(mut self, term: Term) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Follower\u0026gt;\u0026gt; { assert!(term \u0026gt;= self.term, \u0026#34;Term regression {} -\u0026gt; {}\u0026#34;, self.term, term); assert!(term \u0026gt; self.term, \u0026#34;Can only become follower in later term\u0026#34;); info!(\u0026#34;Discovered new term {}\u0026#34;, term); self.term = term; self.log.set_term(term, None)?; self.state_tx.send(Instruction::Abort)?; Ok(self.become_role(Follower::new(None, None))) } Candidate\ncandidate在输掉选举之后或者接收到更高的Term就会转换为Follower：\n在输掉选举时，会接收到新的Leader的Heartbeat，因此可以知道新的Leader是谁，更新自身状态时将当前的Leader保存 接收到更高Term时，有可能只是其他进度更新的Follower给予的回应，因此无法得知Leader是谁 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /// Transforms the node into a follower. We either lost the election /// and follow the winner, or we discovered a new term in which case /// we step into it as a leaderless follower. fn become_follower(mut self, term: Term, leader: Option\u0026lt;NodeID\u0026gt;) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Follower\u0026gt;\u0026gt; { assert!(term \u0026gt;= self.term, \u0026#34;Term regression {} -\u0026gt; {}\u0026#34;, self.term, term); if let Some(leader) = leader { // We lost the election, follow the winner. assert_eq!(term, self.term, \u0026#34;Can\u0026#39;t follow leader in different term\u0026#34;); info!(\u0026#34;Lost election, following leader {} in term {}\u0026#34;, leader, term); let voted_for = Some(self.id); // by definition Ok(self.become_role(Follower::new(Some(leader), voted_for))) } else { // We found a new term, but we don\u0026#39;t necessarily know who the leader // is yet. We\u0026#39;ll find out when we step a message from it. assert_ne!(term, self.term, \u0026#34;Can\u0026#39;t become leaderless follower in current term\u0026#34;); info!(\u0026#34;Discovered new term {}\u0026#34;, term); self.term = term; self.log.set_term(term, None)?; Ok(self.become_role(Follower::new(None, None))) } } candidate如果能够成功当选，那么就可以转换为Leader，上线之后发送心跳信息并且追加一条non-op，，追加的non-op的原因在raft论文当中作出了解释，这里简单提一下，Raft不能够提交自己任期以外的日志，只能够在提交当前任期的日志时顺带提交之前的日志，否则会出现提交日志被覆盖的安全性问题(具体样例见论文figure 8)，如果开启read index线性一致性读优化的情况下，系统如果不收到写请求就无法提交之前的日志，从而阻塞系统状态(读写都会被阻塞，read index 需要Leader在当前任期提交过日志之后才能执行)，non-op可以尽快恢复系统状态。\n1 2 3 4 5 6 7 8 9 10 11 12 13 /// Transition to leader role. fn become_leader(self) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Leader\u0026gt;\u0026gt; { info!(\u0026#34;Won election for term {}, becoming leader\u0026#34;, self.term); let peers = self.peers.clone(); let (last_index, _) = self.log.get_last_index(); let mut node = self.become_role(Leader::new(peers, last_index)); node.heartbeat()?; // Propose an empty command when assuming leadership, to disambiguate // previous entries in the log. See section 8 in the Raft paper. node.propose(None)?; Ok(node) } Follower\nFollower在选举超时时会转换为Candidate，并且调用compaign()开始选举。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 fn become_candidate(mut self) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Candidate\u0026gt;\u0026gt; { // Abort any forwarded requests. These must be retried with new leader. self.abort_forwarded()?; let mut node = self.become_role(Candidate::new()); node.campaign()?; Ok(node) } /// Processes a logical clock tick. pub fn tick(mut self) -\u0026gt; Result\u0026lt;Node\u0026gt; { self.assert()?; self.role.leader_seen += 1; if self.role.leader_seen \u0026gt;= self.role.election_timeout { return Ok(self.become_candidate()?.into()); } Ok(self.into()) } Follower在接受到Leader的Heartbeat、AppendEntries时，或者更高的Term的消息时，都会更新自身状态进行跟随\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 /// Transforms the node into a follower, either a leaderless follower in a /// new term or following a leader in the current term. fn become_follower(mut self, leader: Option\u0026lt;NodeID\u0026gt;, term: Term) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Follower\u0026gt;\u0026gt; { assert!(term \u0026gt;= self.term, \u0026#34;Term regression {} -\u0026gt; {}\u0026#34;, self.term, term); // Abort any forwarded requests. These must be retried with new leader. self.abort_forwarded()?; if let Some(leader) = leader { // We found a leader in the current term. assert_eq!(self.role.leader, None, \u0026#34;Already have leader in term\u0026#34;); assert_eq!(term, self.term, \u0026#34;Can\u0026#39;t follow leader in different term\u0026#34;); info!(\u0026#34;Following leader {} in term {}\u0026#34;, leader, term); self.role = Follower::new(Some(leader), self.role.voted_for); } else { // We found a new term, but we don\u0026#39;t necessarily know who the leader // is yet. We\u0026#39;ll find out when we step a message from it. assert_ne!(term, self.term, \u0026#34;Can\u0026#39;t become leaderless follower in current term\u0026#34;); info!(\u0026#34;Discovered new term {}\u0026#34;, term); self.term = term; self.log.set_term(term, None)?; self.role = Follower::new(None, None); } Ok(self) } 消息转发 在toydb当中，Follower是允许和Client通信的，但是不能够处理信息，Follower需要将消息转发给Leader，Leader处理完之后再由Follower响应Client,这一部分的逻辑在follower.step()当中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 // Forward client requests to the leader, or abort them if there is // none (the client must retry). Event::ClientRequest { ref id, .. } =\u0026gt; { if msg.from != Address::Client { error!(\u0026#34;Received client request from non-client {:?}\u0026#34;, msg.from); return Ok(self.into()); } let id = id.clone(); if let Some(leader) = self.role.leader { debug!(\u0026#34;Forwarding request to leader {}: {:?}\u0026#34;, leader, msg); self.role.forwarded.insert(id); self.send(Address::Node(leader), msg.event)? } else { self.send(msg.from, Event::ClientResponse { id, response: Err(Error::Abort) })? } } // Returns client responses for forwarded requests. // 接收到client的消息之后，转发给leader，leader处理完之后再回应follower // 之后继续由follower去响应给client，这里为什么不告知client 真正的Leader是什么？ Event::ClientResponse { id, mut response } =\u0026gt; { if !self.is_leader(\u0026amp;msg.from) { error!(\u0026#34;Received client response from non-leader {:?}\u0026#34;, msg.from); return Ok(self.into()); } // TODO: Get rid of this field, it should be returned at the RPC // server level instead. if let Ok(Response::Status(ref mut status)) = response { status.server = self.id; } // 处理完就可以从请求队列当中移除，并且响应Client // 请求队列是用于记录Follower接收到并且未处理完的ClientRequest，用于在通知Leader时，就可以将这些未处理的请求全部告知Client,让其找到Leader再去重新执行 if self.role.forwarded.remove(\u0026amp;id) { self.send(Address::Client, Event::ClientResponse { id, response })?; } } 不过Follower既然已经转发给Leader并且正确得到回应，那么就说明Follower知道Leader是谁了，此时回复Client时可以告知Client当前节点不是Leader，之后Client就可以找Leader去通信，但是toydb并没有这样实现，告知Client当前身份这个过程在toydb当中进行了单独的封装：\n在Follower能够确认Leader时，如从Leader处接收到Heartbeat和AppendEntries时，就会调用上文的follower.become_follower()重置状态，在这个函数里面，会调用一个abort_forwarded()，在其中，会使用std::mem::take将role.forwarded给重置，并且遍历其中未处理的消息，依次告知Error，之后Client再进行重试。\n1 2 3 4 5 6 7 8 /// Aborts all forwarded requests. fn abort_forwarded(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { for id in std::mem::take(\u0026amp;mut self.role.forwarded) { debug!(\u0026#34;Aborting forwarded request {:x?}\u0026#34;, id); self.send(Address::Client, Event::ClientResponse { id, response: Err(Error::Abort) })?; } Ok(()) } 初始化 在toydb启动时，会调用Server::new()来创建一个server节点，在其中，一部分初始化与client进行通信，而另一部分则是初始化raft节点,初始化完成之后就会调用serve来提供网络服务，其中会调用到raft.serve()来启动raft节点：\n1 2 3 4 5 6 7 // toydb.rs main Server::new(cfg.id, cfg.peers, raft_log, raft_state) .await? .listen(\u0026amp;cfg.listen_sql, \u0026amp;cfg.listen_raft) .await? .serve() .await 在raft.serve()当中，就会调用raft.eventloop()正式启动raft接收和处理事件，在eventloop当中，传入了一个channel的发送端，创建了三个channel：\ntcp_tx:raft节点之间相互交流的发送端，用于将底层node塞入信箱当中的Message发送给其他的节点 node_rx:node Message消息的接收端，用于从下层的raft当中接收Message，然后交给tcp_tx去发送 tcp_rx:raft节点之间相互交流的接收端，接收其他节点传来的Message，然后交给自身的Raft去执行 Client_rx: 接收 client 发送的 Message，交给自身的 Raft 去执行，这里的 client 并不是用户的 client，而是要执行命令的 sql 端，sql 端对于要执行的命令，首先会交给 Raft 来进行共识，这个过程是以 client-server 结构来执行的，sql 端充当 client，raft-server 充当 server。 这一部分的逻辑主体是通过tokio::select!来实现的，在逻辑上与go当中的select是差不多的，只不过go的select是监听同步channel，而tokio::select!是等待异步任务的执行完成，分别从上述的三个channel当中获取Message，推动Raft节点。\n除了channel的通信之外，select当中还有一个Ticker，以100ms的间隔运行，用于将物理时钟转换为逻辑时钟，提供给下层，驱动Raft。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 /// Runs the event loop. async fn eventloop( mut node: Node, node_rx: mpsc::UnboundedReceiver\u0026lt;Message\u0026gt;, client_rx: mpsc::UnboundedReceiver\u0026lt;(Request, oneshot::Sender\u0026lt;Result\u0026lt;Response\u0026gt;\u0026gt;)\u0026gt;, tcp_rx: mpsc::UnboundedReceiver\u0026lt;Message\u0026gt;, tcp_tx: mpsc::UnboundedSender\u0026lt;Message\u0026gt;, ) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut node_rx = UnboundedReceiverStream::new(node_rx); let mut tcp_rx = UnboundedReceiverStream::new(tcp_rx); let mut client_rx = UnboundedReceiverStream::new(client_rx); let mut ticker = tokio::time::interval(TICK_INTERVAL); let mut requests = HashMap::\u0026lt;Vec\u0026lt;u8\u0026gt;, oneshot::Sender\u0026lt;Result\u0026lt;Response\u0026gt;\u0026gt;\u0026gt;::new(); loop { tokio::select! { // 监听ticker，驱动下层Raft，间隔为100ms _ = ticker.tick() =\u0026gt; node = node.tick()?, // 获取从其他raft节点发送而来的Message，交给下层的Raft去处理 Some(msg) = tcp_rx.next() =\u0026gt; node = node.step(msg)?, // 获取下层Raft放入信箱的Message，发送给对应的节点 Some(msg) = node_rx.next() =\u0026gt; { match msg { Message{to: Address::Node(_), ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Broadcast, ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Client, event: Event::ClientResponse{ id, response }, ..} =\u0026gt; { if let Some(response_tx) = requests.remove(\u0026amp;id) { response_tx .send(response) .map_err(|e| Error::Internal(format!(\u0026#34;Failed to send response {:?}\u0026#34;, e)))?; } } _ =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Unexpected message {:?}\u0026#34;, msg))), } } // 获取client发送的消息，交给下层去处理，这里的client并不是用户的client，而是要执行命令的sql端 Some((request, response_tx)) = client_rx.next() =\u0026gt; { let id = Uuid::new_v4().as_bytes().to_vec(); let msg = Message{ from: Address::Client, to: Address::Node(node.id()), term: 0, event: Event::ClientRequest{id: id.clone(), request}, }; node = node.step(msg)?; requests.insert(id, response_tx); } } } } Leader Election Raft的选举过程受到Ticker的驱动，如上面所展示的，上层server层会以100ms为间隔将物理时钟转换为逻辑时钟，下层的tick()定义在mod.rs当中,对于不同角色的节点，分别调用对应的tick()，在选举当中，对应的就是Follower的tick()：\n1 2 3 4 5 6 7 8 9 /// Moves time forward by a tick. pub fn tick(self) -\u0026gt; Result\u0026lt;Self\u0026gt; { match self { Node::Candidate(n) =\u0026gt; n.tick(), Node::Follower(n) =\u0026gt; n.tick(), Node::Leader(n) =\u0026gt; n.tick(), } } 在Follower的tick()当中,增大election_duration，如果超出了限制，就转换为candidate，开启选举，选举的入口为self.compaign(),这与etcd/raft的实现是一致的。\n在toydb当中设置的是10-20个逻辑时钟，即1-2s的随机时间，这里比论文当中(150ms-300ms)设置的长很多，和etcd设置一致。但是toydb的heartbeat为3个逻辑时钟，所以其实也不会有太大的问题\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 /// The interval between leader heartbeats, in ticks. const HEARTBEAT_INTERVAL: Ticks = 3; /// The randomized election timeout range (min-max), in ticks. This is /// randomized per node to avoid ties. const ELECTION_TIMEOUT_RANGE: std::ops::Range\u0026lt;u8\u0026gt; = 10..20; // Follower.tick，转换为candidate pub fn tick(mut self) -\u0026gt; Result\u0026lt;Node\u0026gt; { self.assert()?; self.role.leader_seen += 1; if self.role.leader_seen \u0026gt;= self.role.election_timeout { return Ok(self.become_candidate()?.into()); } Ok(self.into()) } // 转换为candidate之后会立刻开始选举 /// Transforms the node into a candidate, by campaigning for leadership in a /// new term. fn become_candidate(mut self) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Candidate\u0026gt;\u0026gt; { // Abort any forwarded requests. These must be retried with new leader. self.abort_forwarded()?; let mut node = self.become_role(Candidate::new()); node.campaign()?; Ok(node) } // Candidate.tick，在第一次选举失败之后会调用到 /// Processes a logical clock tick. pub fn tick(mut self) -\u0026gt; Result\u0026lt;Node\u0026gt; { self.assert()?; self.role.election_duration += 1; if self.role.election_duration \u0026gt;= self.role.election_timeout { self.campaign()?; } Ok(self.into()) } compaign\n在compaign当中，逻辑按照小论文当中实现：\n自增term 为自己投一票 持久化保存term 发送Message，向其他的节点请求投票，发送一条Event类型为SolicitVote类型的Message 1 2 3 4 5 6 7 8 9 10 11 12 13 14 /// Campaign for leadership by increasing the term, voting for ourself, and /// soliciting votes from all peers. pub(super) fn campaign(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { let term = self.term + 1; info!(\u0026#34;Starting new election for term {}\u0026#34;, term); self.role = Candidate::new(); self.role.votes.insert(self.id); // vote for ourself self.term = term; self.log.set_term(term, Some(self.id))?; let (last_index, last_term) = self.log.get_last_index(); self.send(Address::Broadcast, Event::SolicitVote { last_index, last_term })?; Ok(()) } SolicitVote\nSolicitVote类型的Message由其他的Follower来处理，对于所有Message的处理，都定义在step()当中，这里逻辑和上面一样，根据节点角色再去进行调用，SolicitVote在follower.step()当中:\n如果当前term投过票了就忽略 做安全性检查，只会给日志状态最新的节点投票 检查通过则投票，并做持久化记录，存储vote for 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 Event::SolicitVote { last_index, last_term } =\u0026gt; { let from = msg.from.unwrap(); // If we already voted for someone else in this term, ignore it. if let Some(voted_for) = self.role.voted_for { if from != voted_for { return Ok(self.into()); } } // Only vote if the candidate\u0026#39;s log is at least as up-to-date as // our log. let (log_index, log_term) = self.log.get_last_index(); if last_term \u0026gt; log_term || last_term == log_term \u0026amp;\u0026amp; last_index \u0026gt;= log_index { info!(\u0026#34;Voting for {} in term {} election\u0026#34;, from, self.term); self.send(Address::Node(from), Event::GrantVote)?; self.log.set_term(self.term, Some(from))?; self.role.voted_for = Some(from); } } /// 定义在log当中，用于对term和vote_for进行持久化存储 /// Sets the most recent term, and cast vote (if any). pub fn set_term(\u0026amp;mut self, term: Term, voted_for: Option\u0026lt;NodeID\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { self.engine.set(\u0026amp;Key::TermVote.encode()?, bincode::serialize(\u0026amp;(term, voted_for))?)?; self.maybe_flush() } BecomeLeader\n当candidate接收到Follower的投票回复之后，会记录票数，如果达到了quorum，那么就转换为leader，这一部分的逻辑同样是定义在step当中\n当选为Leader之后，更新一些信息之后，会发送心跳信息告知其他的节点Leader已经当选。 之后会追加一条no-op，non-op的作用已经在上文角色切换部分解释了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Event::GrantVote =\u0026gt; { self.role.votes.insert(msg.from.unwrap()); if self.role.votes.len() as u64 \u0026gt;= self.quorum() { return Ok(self.become_leader()?.into()); } } fn become_leader(self) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Leader\u0026gt;\u0026gt; { info!(\u0026#34;Won election for term {}, becoming leader\u0026#34;, self.term); let peers = self.peers.clone(); let (last_index, _) = self.log.get_last_index(); let mut node = self.become_role(Leader::new(peers, last_index)); node.heartbeat()?; // Propose an empty command when assuming leadership, to disambiguate // previous entries in the log. See section 8 in the Raft paper. node.propose(None)?; Ok(node) } toydb当中Leader Election的实现非常简单，基本上完全按照raft小论文当中的描述，像是PreVote，Check Quorum，Leader Lease一概没有，不过这也很符合其toy的定位。\nLog Replication Log 在看日志复制之前，先看一下Log的实现： Log模块并不只存储LogEntry，这里其实是对底层kv存储引擎的封装，所有需要进行持久化的数据都丢到存储引擎当中，包括current_term和vote_for，这里还额外存储了一个commit_index用于在commit时做一个安全校验,Entry是log的基本单位，实现同样是借助枚举来实现的：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /// A log key, encoded using KeyCode. #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub enum Key { /// A log entry, storing the term and command. Entry(Index), /// Stores the current term and vote (if any). TermVote, /// Stores the current commit index (if any). CommitIndex, } /// A log entry. #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub struct Entry { /// The entry index. pub index: Index, /// The term in which the entry was added. pub term: Term, /// The state machine command. None is used to commit noops during leader election. pub command: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;, } Log本体定义如下，包括一个engine用于进行存储，和一些元数据，在初始化时，会从engine当中把元数据给加载出来：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 pub struct Log { /// The underlying storage engine. engine: Box\u0026lt;dyn Engine\u0026gt;, /// The index of the last stored entry. last_index: Index, /// The term of the last stored entry. last_term: Term, /// The index of the last committed entry. commit_index: Index, /// The term of the last committed entry. commit_term: Term, /// Whether to sync writes to disk. sync: bool, } 其中的函数实现，逻辑都比较简单，并且也都提供了丰富的注释，这里就不展开。\nHeartbeat 与6.824当中使用heartbeat来携带日志不同，在toydb当中，heartbeat就仅仅是用于维护leader身份，以及推动follower的commit_index，不会携带log entry。heartbeat的发送逻辑在leader.tick()当中，heartbeat的处理在follower.step()当中：\n接收到了heartbeat就不断将自己转换为follower 根据leader传来到commit_index和自身log的情况，推动commit的进度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 // leader.tick() /// Processes a logical clock tick. pub fn tick(mut self) -\u0026gt; Result\u0026lt;Node\u0026gt; { self.assert()?; self.role.since_heartbeat += 1; if self.role.since_heartbeat \u0026gt;= HEARTBEAT_INTERVAL { self.heartbeat()?; self.role.since_heartbeat = 0; } Ok(self.into()) } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 pub enum Event { /// Leaders send periodic heartbeats to its followers. Heartbeat { /// The index of the leader\u0026#39;s last committed log entry. commit_index: Index, /// The term of the leader\u0026#39;s last committed log entry. commit_term: Term, }, /// ...... /// ...... } // follower.step()当中处理心跳信息 // The leader will send periodic heartbeats. If we don\u0026#39;t have a // leader in this term yet, follow it. If the commit_index advances, // apply state transitions. Event::Heartbeat { commit_index, commit_term } =\u0026gt; { // Check that the heartbeat is from our leader. let from = msg.from.unwrap(); match self.role.leader { Some(leader) =\u0026gt; assert_eq!(from, leader, \u0026#34;Multiple leaders in term\u0026#34;), None =\u0026gt; self = self.become_follower(Some(from), msg.term)?, } // Advance commit index and apply entries if possible. let has_committed = self.log.has(commit_index, commit_term)?; let (old_commit_index, _) = self.log.get_commit_index(); if has_committed \u0026amp;\u0026amp; commit_index \u0026gt; old_commit_index { self.log.commit(commit_index)?; let mut scan = self.log.scan((old_commit_index + 1)..=commit_index)?; // 与状态机进行交互 while let Some(entry) = scan.next().transpose()? { self.state_tx.send(Instruction::Apply { entry })?; } } self.send(msg.from, Event::ConfirmLeader { commit_index, has_committed })?; } Log Replication Log Replication的入口在Leader.step()当中，Leader会处理由Client发送而来的请求，在这里会调用propose()，在propose当中，真正开始进行日志复制，将命令转换为日志存储到本地，之后再发送给其他的节点：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // leader.step() 接受客户端请求 Event::ClientRequest { id, request: Request::Mutate(command) } =\u0026gt; { let index = self.propose(Some(command))?; self.state_tx.send(Instruction::Notify { id, address: msg.from, index })?; if self.peers.is_empty() { self.maybe_commit()?; } } // propose，真正开始日志复制的逻辑 /// Proposes a command for consensus by appending it to our log and /// replicating it to peers. If successful, it will eventually be committed /// and applied to the state machine. pub(super) fn propose(\u0026amp;mut self, command: Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;) -\u0026gt; Result\u0026lt;Index\u0026gt; { let index = self.log.append(self.term, command)?; for peer in self.peers.clone() { self.send_log(peer)?; } Ok(index) } Progress Progress的含义是，站在Leader的角度，观察到的follower的复制进度，定义了next_index和match_index，之后使用一个HashMap来管理各个节点的进度,由于和etcd相比少了流水线发送和滑动窗口流量控制等，Progress的实现简单了很多：\n1 2 3 4 5 6 7 8 9 10 11 12 13 struct Progress { /// The next index to replicate to the peer. next: Index, /// The last index known to be replicated to the peer. last: Index, } pub struct Leader { /// Peer replication progress. progress: HashMap\u0026lt;NodeID, Progress\u0026gt;, /// Number of ticks since last periodic heartbeat. since_heartbeat: Ticks, } propose 在propose当中，会首先将日志持久化到本地，之后调用send_log()进行发送，在send_log()当中，会根据Progress找到需要发送的日志，之后从本地进行读取。\n根据progress获取到next之后，先从本地读取next - 1的Entry，next - 1的Entry是用于检测日志冲突的，理应存在于Leader本地 扫描出next之后的所有Entry，连同base_index和base_term封装成一条AppendEntries发送给follower 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /// Sends pending log entries to a peer. fn send_log(\u0026amp;mut self, peer: NodeID) -\u0026gt; Result\u0026lt;()\u0026gt; { let (base_index, base_term) = match self.role.progress.get(\u0026amp;peer) { Some(Progress { next, .. }) if *next \u0026gt; 1 =\u0026gt; match self.log.get(next - 1)? { Some(entry) =\u0026gt; (entry.index, entry.term), None =\u0026gt; panic!(\u0026#34;Missing base entry {}\u0026#34;, next - 1), }, Some(_) =\u0026gt; (0, 0), None =\u0026gt; panic!(\u0026#34;Unknown peer {}\u0026#34;, peer), }; let entries = self.log.scan((base_index + 1)..)?.collect::\u0026lt;Result\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;\u0026gt;()?; debug!(\u0026#34;Replicating {} entries at base {} to {}\u0026#34;, entries.len(), base_index, peer); self.send(Address::Node(peer), Event::AppendEntries { base_index, base_term, entries })?; Ok(()) } AppendEntries Leader封装了一条AppendEntries发送给Follower，相对应的就在Follower.step()当中进行处理：\n能够接收到AppendEntries证明能够确认Leader，因此就调用become_follower()更新自身状态，对Leader进行跟随 尝试匹配日志，并将其应用于自身，如果不匹配则返回一条Reject，让Leader再去做backup，找到正确的日志 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // Replicate entries from the leader. If we don\u0026#39;t have a leader in // this term yet, follow it. Event::AppendEntries { base_index, base_term, entries } =\u0026gt; { // Check that the entries are from our leader. let from = msg.from.unwrap(); match self.role.leader { Some(leader) =\u0026gt; assert_eq!(from, leader, \u0026#34;Multiple leaders in term\u0026#34;), None =\u0026gt; self = self.become_follower(Some(from), msg.term)?, } // Append the entries, if possible. if base_index \u0026gt; 0 \u0026amp;\u0026amp; !self.log.has(base_index, base_term)? { debug!(\u0026#34;Rejecting log entries at base {}\u0026#34;, base_index); self.send(msg.from, Event::RejectEntries)? } else { let last_index = self.log.splice(entries)?; self.send(msg.from, Event::AcceptEntries { last_index })? } } 在Leader端的处理分为两种，分别是成功响应的Event::AppendEntries和EventRejectEntries：\n如果Follower能够成功匹配日志，那么Leader就会去更新进度，并且计算quorum，尝试进行commit 如果匹配失败，在Leader端progress向前回退一个，再尝试发送日志，进行探测，直至找到找到正确的位置，这里是没有实现的fast backup的，在效率上会存在一定的问题 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 Event::AcceptEntries { last_index } =\u0026gt; { assert!( last_index \u0026lt;= self.log.get_last_index().0, \u0026#34;Follower accepted entries after last index\u0026#34; ); let from = msg.from.unwrap(); self.role.progress.entry(from).and_modify(|p| { p.last = last_index; p.next = last_index + 1; }); self.maybe_commit()?; } // A follower rejected log entries we sent it, typically because it // does not have the base index in its log. Try to replicate from // the previous entry. // // This linear probing, as described in the Raft paper, can be very // slow with long divergent logs, but we keep it simple. Event::RejectEntries =\u0026gt; { let from = msg.from.unwrap(); self.role.progress.entry(from).and_modify(|p| { if p.next \u0026gt; 1 { p.next -= 1 } }); self.send_log(from)?; } Commit 每次Leader确定了有follower保存了日志之后就会尝试进行commit，在rust当中，借助函数式编程，能将这个过程写的很优雅(没接触过函数式编程看了想骂娘)：\n首先获取到progress的HashMap，之后获取到HashMap的所有value(value是一个next_index和last_index组成的元组) 再做一个转换，获取last_index，这一步会得到一个由所有last_index组成的迭代器 之后使用chain将leader的last_index追加进去，最后转换为Vec 排序并反转之后第quorum大的last_index就是与follower达成共识的index 在vscode当中，rust-analyzer能够很清楚的显示出每一步都得到了什么： 获取到commit_index之后做一些校验：\n此次计算出的commit_index要大于上一次的commit_index 获取出 commit_index 对应的 entry，判断是不是当前的 Term，Leader 只能提交属于自己任期的日志 (见 raft 小论文 figure 8) 如果是新的commit_index，就在本地进行记录，并且扫描出从上一次commit到新commit_index之间的所有的日志，向上层状态机进行apply，即向state_tx channel当中发送一条信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // Make sure the commit index does not regress. let (prev_commit_index, _) = self.log.get_commit_index(); assert!( commit_index \u0026gt;= prev_commit_index, \u0026#34;Commit index regression {} -\u0026gt; {}\u0026#34;, prev_commit_index, commit_index ); // We can only safely commit up to an entry from our own term, see // figure 8 in Raft paper. match self.log.get(commit_index)? { Some(entry) if entry.term == self.term =\u0026gt; {} Some(_) =\u0026gt; return Ok(prev_commit_index), None =\u0026gt; panic!(\u0026#34;Commit index {} missing\u0026#34;, commit_index), }; // Commit and apply the new entries. if commit_index \u0026gt; prev_commit_index { self.log.commit(commit_index)?; // TODO: Move application elsewhere, but needs access to applied index. let mut scan = self.log.scan((prev_commit_index + 1)..=commit_index)?; while let Some(entry) = scan.next().transpose()? { self.state_tx.send(Instruction::Apply { entry })?; } } Summary 以上就是toydb当中Raft模块的全部内容，这里的Raft模块指的是Raft本身的逻辑部分，不包括上层的状态机和发送命令的Client，这一部分笔者留到下一章的sql engine当中进行分析，最后以toydb与MIT6.824以及etcd进行一个简单的对比，来结束这一章\nMIT-6.824\n在MIT-6.824的Lab2当中，实现了一个基础的Raft模块，在其中几乎完整的复现了《In Search of an Understandable Consensus Algorithm (Extended Version)》一文，但没有实现集群配置变更，并且在持久化和snapshot上做了简化。\n但是MIT6.824当中的实现并不是那么工程化，或者说接近生产环境，虽然每个人的实现方式不同，但是至少初始框架是这样的，相比于Tinykv来说。在6.824当中，在需要通信的时候就直接发送rpc调用对应的函数，以一种很直观的方式复现了Raft，当中定义的函数也都遵循论文当中figure2，基本上把figure2翻译成golang就能实现出来。\netcd/raft\netcd/raft作为一个投入生产环境的raft package，代码结构自然是清晰严谨的。\n在基础结构上，etcd/raft将raft视为一个状态机，上层给予Raft一些输入，如tick或者message，Raft就会根据输入作出一些响应，更改自身状态，最后提供一些输出，上层处理完输出之后再调用Advance来继续推进Raft去更新状态。\n在etcd当中，raft package仅提供raft的逻辑，如如何进行选举和发送日志，但是如持久化存储，网络通信都由上层应用负责，raft package通过Ready将需要存储的日志，快照，以及需要发送的消息交给上层。\n在结构上，etcd/raft以Message为主体和驱动，整个Raft的执行流程就是在tick的驱动下，不断的执行和发送Message的过程，并不会像MIT6.824当中那样在一个函数当中能看到完整的rpc调用和处理流程，每个Message只会负责整体逻辑的一部分，执行完自己的内容就生成一个新的Message发送，其他拿到这条Message的来完成这个事件的剩余逻辑，举个例子，在进行选举时：\ncandidate首先会向其他节点发送一个请求投票的Message，此时candidate目前负责的逻辑已经执行结束，candidate可以去执行其他内容 follower受到candidate发送的Message就会进行处理，投票或者忽略，同样将结果封装成一条Message再发送给candidate Candidate 再收到 Message 之后继续完成剩余的逻辑，统计投票，决定是否当选 整个执行过程是松散、非连续的。但是主体逻辑全部使用 Message 来承载的话，整体结构就会非常清晰，raft 只需要不断的处理 Message 即可，并在适当的时候产生新的 Message\n在实现上，etcd/raft完整的实现了Raft小论文当中的内容，包括在MIT-6.824当中没有实现的集群变更，并且对于Raft大论文《CONSENSUS: BRIDGING THEORY AND PRACTICE》当中的优化也进行了实现：\n在结构上：采用了批处理的形式，即Ready会向上层返回一批需要处理数据，上层用户进行集中处理，在Raft运行的过程中，只需要把log添加到对应的位置，将需要发送的Message放入“信箱”，届时一并通过Ready返回(这个其实和Raft大论文没什么关系，只不过都是优化就一并提一下了) 在选举部分：ectd/raft实现了PreVote、Check Quorum、Leader Lease的优化 在日志复制上，ectd/raft采用了流水线设计，会在阻塞探测模式，流水线发送，快照发送三种模式下切换，在流水线模式下，etcd/raft可以在未接收到上次发送日志响应的情况下继续发送日志，并且使用滑动窗口来进行流量控制。并且在日志发送时，采用并行发送的方式，即存储与发送并行，可以在本地还未进行持久化存储的情况下就发送，某些情况下，follower可能在leader存储日志之前就接收到日志。在日志不匹配时，follower会返回一个hint，从而Leader可以快速进行回退，找到正确的位置。 在线性一致性读上，提供了 ReadIndex 和 Lease Read 两种方式，以绕过 Log Read，提高系统吞吐量 如果想进一步了解etcd/raft的话，推荐看这一系列，就像其名字一样，深入浅出：深入浅出etcd/raft —— 0x00 引言 - 叉鸽 MrCroxx 的博客 toydb 内容上，toydb相比于Raft小论文和MIT-6.824有所删减，同样没有实现集群成员变更，此外，没有实现日志的快速回退，和快照功能。而etcd当中的种种优化就更没有了(toydb通过Raft状态机实现了一个ReadIndex的功能，其他的就都没有了)\n在结构上toydb基本上与etcd/raft一致，使用逻辑时钟+Message进行驱动，上面对etcd的分析对toydb基本上都适用。只不过toydb的持久化是由Raft自己负责的，所有与持久化相关的内容都扔进kv存储引擎当中，无论是元数据还是日志。但是网络模块是在上层实现的，在Raft当中也是和etcd一样，只需要把Message塞入信箱即可。\n总的来说，toydb做了一个很好的取舍，用非常工程化的结构实现了一个几乎没有任何优化的，简单的Raft模块，代码逻辑非常的清晰，当然代价就是会损失一定的性能，如同步进行持久化，和日志出现不匹配时需要一条条进行回退。不过作为一个Learning Project，个人认为还是实现的非常优雅的，很值得用来学习。\n对于Raft上层状态机和Client的相关内容，涉及到比较复杂的通信和异步编程，限于篇幅就不再本章当中进行分析，会留在下一章的kv engine当中一并进行分析。\n由于分布式系统比较复杂，在toydb当中，在client，Raft，以及Raft状态机之间的逻辑比较复杂，涉及到大量的异步编程和信息交换，笔者水平有限，难免会出现错误，欢迎读者进行批评指正，再完成了Raft状态机部分，我也会重新校验整理这篇文章，以求达到一个更好的效果。\n","permalink":"http://itfischer.space/en/posts/tech/toydb/03-raft/","summary":"toydb的Raft实现，相比于6.824更接近于生产级别的，和etcd/raft在结构上比较相似，但是并没有实现Raft大论文当中的优化，","title":"03-Raft"},{"content":"在正式写这一篇文章之前，原本的规划是在这一部分介绍sql执行引擎，即如何将Raft，Raft状态机，存储引擎组合起来，为SQL的执行去提供支持，但是这一部分又不可避免的牵扯到SQL执行过程，如果全部展示出来，内容将会非常多，思来想去，还是Raft状态机这一部分比较独立，使用了一个Trait屏蔽掉了底层的存储实现，因此就先将这一模块单独拆出来。\n在toydb当中，Raft状态机的实现定义于src/raft/state.rs当中。在etcd，对于Raft Package也可以理解成一个状态机，因此，为了方便进行区分，在开头首先声明，在这一章出现的所有状态机的概念都是指构建于Raft上层的状态机，上一章分析的Raft会以Raft模块(Raft Package)来指代。 这里与MIT-6.824有一个非常大的不同是：\n在MIT-6.824当中，client是与上层状态机进行通信的，命令会先输入给状态机，之后状态机向下创建一条日志，发送给Raft模块去进行共识，当达到quorum共识之后，状态机再给client以回应，告知此次请求执行完成 在toydb当中，Client的请求是直接发送给Raft模块的，在上一章当中，我们可以看到，server从client_rx当中获取到了消息之后，直接调用step交给了Raft模块去处理，Raft模块处理完之后会发送给状态机，状态机进行一些逻辑校验，之后再发送Message给Raft模块，之后由Raft模块将消息回复给Client，所以在Raft模块当中，能够看到很多和ClientRequest，ClientResponse相关的内容。 1 2 3 4 5 6 7 8 9 10 11 12 // 获取client发送的消息，交给Raft模块去处理，这里的client并不是用户的client，而是要执行命令的sql端 Some((request, response_tx)) = client_rx.next() =\u0026gt; { let id = Uuid::new_v4().as_bytes().to_vec(); let msg = Message{ from: Address::Client, to: Address::Node(node.id()), term: 0, event: Event::ClientRequest{id: id.clone(), request}, }; node = node.step(msg)?; requests.insert(id, response_tx); } Driver 在这里，toydb当中的Raft状态机更像是一个比较存粹的状态机，从Raft模块当中接收输入(Instruction)，更新自身状态之后，再给Raft模块输出(Messsage)，不会涉及到网络通信部分的内容，不会与Client进行交互，所以状态机基本上就是起到了一个记录的作用，比如，记录当前接收到了那些请求，各个请求的apply的情况，从而确定什么时候该让Raft模块去回应Client，在这一部分定义了一个Driver，用于驱动状态机的执行：\nstate_rx用于从Raft模块当中接收信息 node_tx用于向Raft模块发送信息 notify：用于当某条日志apply之后，通知Raft模块，让其给Client回应，Raft Leader在收到Mutate类型的ClientRequest之后，会进行propose来复制日志，并且向状态机当中发送一条notify：保存在状态机当中 queries：将只读请求保存到上层状态机，达到quorum时会再告知Leader进行读取，实现了ReadIndex的效果。 1 2 3 4 5 6 7 8 9 10 /// Drives a state machine, taking operations from state_rx and sending results via node_tx. pub struct Driver { node_id: NodeID, state_rx: UnboundedReceiverStream\u0026lt;Instruction\u0026gt;, node_tx: mpsc::UnboundedSender\u0026lt;Message\u0026gt;, /// Notify clients when their mutation is applied. \u0026lt;index, (client, id)\u0026gt; notify: HashMap\u0026lt;Index, (Address, Vec\u0026lt;u8\u0026gt;)\u0026gt;, /// Execute client queries when they receive a quorum. \u0026lt;index, \u0026lt;id, query\u0026gt;\u0026gt; queries: BTreeMap\u0026lt;Index, BTreeMap\u0026lt;Vec\u0026lt;u8\u0026gt;, Query\u0026gt;\u0026gt;, } 入口为drive函数，drive函数会不断的从state.rx当中不断获取Instuction，然后调用self.execute来执行：\n1 2 3 4 5 6 7 8 9 10 11 pub async fn drive(mut self, mut state: Box\u0026lt;dyn State\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { debug!(\u0026#34;Starting state machine driver at applied index {}\u0026#34;, state.get_applied_index()); while let Some(instruction) = self.state_rx.next().await { if let Err(error) = self.execute(instruction, \u0026amp;mut *state) { error!(\u0026#34;Halting state machine due to error: {}\u0026#34;, error); return Err(error); } } debug!(\u0026#34;Stopping state machine driver\u0026#34;); Ok(()) } 在execute()当中，就是根据instruction的类型，去调用不同的函数，那我们只需要弄明白每种Instruction的类型和功能，以及对应的发送位置和响应方式即可：\nState Trait 在介绍Raft模块与Raft状态机交互之前，先来看一下Raft状态机的Trait和实现，Raft状态机是以Trait的形式定义的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 /// A Raft-managed state machine. pub trait State: Send { /// Returns the last applied index from the state machine. fn get_applied_index(\u0026amp;self) -\u0026gt; Index; /// Applies a log entry to the state machine. If it returns Error::Internal, /// the Raft node halts. Any other error is considered applied and returned /// to the caller. /// /// The entry may contain a noop command, which is committed by Raft during /// leader changes. This still needs to be applied to the state machine to /// properly track the applied index, and returns an empty result. /// /// TODO: consider using runtime assertions instead of Error::Internal. fn apply(\u0026amp;mut self, entry: Entry) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;; /// Queries the state machine. All errors are propagated to the caller. fn query(\u0026amp;self, command: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;; } 而真正的实现在src/sql/engine/raft.rs当中，定义了一个struct State\u0026lt;E\u0026gt;来作为Raft状态机的实现，在其中，只有两个变量：\nlast_applied_index用于记录当前应用的最后的日志 super::KV\u0026lt;E\u0026gt;是一个MVCC Storage的Wrapper(这一部分在02-MVCC当中已经介绍过)，提供了存储元数据和开启事务的功能。 但是这个struct是具体怎么实现这些功能的，由于牵扯到sql执行的一些内容，并且不进行展开也不会影响到后续的内容，就留到下一章了。 1 2 3 4 5 6 7 /// The Raft state machine for the Raft-based SQL engine, using a KV SQL engine pub struct State\u0026lt;E: storage::engine::Engine\u0026gt; { /// The underlying KV SQL engine engine: super::KV\u0026lt;E\u0026gt;, /// The last applied index applied_index: u64, } Instruction 在Raft State Machine当中，Instruction的地位相当于Raft模块当中Message的地位。在这一部分，主要会分析Raft状态机是如何与Raft模块进行交互的，按照Instruction的类型，理顺各类Instruction的发送和处理的方式与时机。功能实现如下：\n写请求：Notify 只读请求：Query + Vote 应用日志：Apply 终止请求：Abort 1 2 3 4 5 6 7 8 9 10 11 12 13 14 pub enum Instruction { /// Abort all pending operations, e.g. due to leader change. Abort, /// Apply a log entry. Apply { entry: Entry }, /// Notify the given address with the result of applying the entry at the given index. Notify { id: Vec\u0026lt;u8\u0026gt;, address: Address, index: Index }, /// Query the state machine when the given term and index has been confirmed by vote. Query { id: Vec\u0026lt;u8\u0026gt;, address: Address, command: Vec\u0026lt;u8\u0026gt;, term: Term, index: Index, quorum: u64 }, /// Extend the given server status and return it to the given address. Status { id: Vec\u0026lt;u8\u0026gt;, address: Address, status: Box\u0026lt;Status\u0026gt; }, /// Votes for queries at the given term and commit index. Vote { term: Term, index: Index, address: Address }, } 写请求执行 Driver.notify用于记录客户端的一次写请求，在leader接收到客户端的写请求时(类型为 Mutate)，就会向Raft状态机发送一条Instruction::Notify，将请求记录到状态机当中，等待之后该请求对应的日志Apply了，再告知Leader，让Leader去响应客户端，返回执行结果:\n发送\nLeader接收到Client发送而来的类型为Request::Mutate的ClientRequest，在Raft模块层面，会调用self.propose()来进行日志复制，在Raft状态机层面，Leader会向Raft状态机发送一条Instruction::Notify来记录请求。\n1 2 3 4 5 6 7 8 9 10 // Leader.step() pub fn step(mut self, msg: Message) -\u0026gt; Result\u0026lt;Node\u0026gt; { Event::ClientRequest { id, request: Request::Mutate(command) } =\u0026gt; { let index = self.propose(Some(command))?; self.state_tx.send(Instruction::Notify { id, address: msg.from, index })?; if self.peers.is_empty() { self.maybe_commit()?; } } } 在Driver.execute()当中，如果高于目前的appled_index，那么就插入保存，等待之后该条日志apply，否则为出现错误，让Leader告知客户端。\n1 2 3 4 5 6 7 8 9 10 11 12 13 /// Executes a state machine instruction. fn execute(\u0026amp;mut self, i: Instruction, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { match i { Instruction::Notify { id, address, index } =\u0026gt; { if index \u0026gt; state.get_applied_index() { self.notify.insert(index, (address, id)); } else { self.send(address, Event::ClientResponse { id, response: Err(Error::Abort) })?; } } } Ok(()) } 在Leader.step()当中，会处理由Raft状态机发来的Message::ClientResponse，Leader不会做什么处理，直接发送给Client就好，对于其他的Instruction，处理类型也是相同，后面不会再提及。\n1 2 3 4 5 6 Event::ClientResponse { id, mut response } =\u0026gt; { if let Ok(Response::Status(ref mut status)) = response { status.server = self.id; } self.send(Address::Client, Event::ClientResponse { id, response })?; } 只读请求优化 在Raft的基础实现当中，无论是写请求还是读请求，都需要创建一条日志进行写入，之后执行时按照日志commit的顺序进行执行，从而提供线性一致性。但这样的问题就在于，即便是读请求也需要创建日志并写入，带来了额外的磁盘IO，从而提高系统延迟。\n在Raft当中，为了保证线性一致性，需要保证能够读到目前最新的commit_index。如果想对Log Read进行优化，那么就需要想办法绕过写入Log对这个过程，比较常见的两种优化方式是ReadIndex和Lease Read。由于Raft的读写都经过Leader(不考虑Follower Read的优化)，那么只要能够确认当前的Leader身份有效，就可以直接从Leader本地进行读取，为了确认Leader的有效身份，ReadIndex和Lease Read采用了两种不同的方式：\nReadIndex:ReadIndex记录收到请求时的commit index，然后通过发送一轮心跳的方式来确认Leader的合法身份，如果能确认收到Quorum数量的响应，那么当前的Leader身份就是合法的，当Leader的apply index \u0026gt;= 之前保存的commit index时，就可以根据commit index去读取。 Lease Read:Lease Read的核心思想是利用了一个选举时间差，当收到Leader心跳信息之后，follower就会重置选举超时时间，那么直到下一次选举超时之前，目前Leader的身份一定是合法的，不会有其他的节点通过选举成为Leader，因此就可以直接读取Leader的commit index。不过Lease Read会受到时钟偏移的影响，一种比较简单的解决方法是将Lease时间设置为比超时时间短一点。(有那么点TrueTime的意思) 如果想进一步了解只读请求优化，可以看这一篇：深入浅出etcd/raft —— 0x06 只读请求优化 - 叉鸽 MrCroxx 的博客 在toydb当中，只读请求的优化采用的是ReadIndex，在Leader.step()当中，会处理Client发送来的Event::ClientRequest类型的请求，其中request类型为Request::Query代表只读请求：\nLeader首先创建一条Instruction::Query将只读请求记录到状态机当中 之后再发送一条Instruction::Vote来表示记录当前节点确认了Leader的身份 发送Heartbeat，来尝试证明自身为合法的Leader 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 pub fn step(mut self, msg: Message) -\u0026gt; Result\u0026lt;Node\u0026gt; { Event::ClientRequest { id, request: Request::Query(command) } =\u0026gt; { let (commit_index, _) = self.log.get_commit_index(); self.state_tx.send(Instruction::Query { id, address: msg.from, command, term: self.term, index: commit_index, quorum: self.quorum(), })?; self.state_tx.send(Instruction::Vote { term: self.term, index: commit_index, address: Address::Node(self.id), })?; self.heartbeat()?; } } Follower收到了Heartbeat信息之后，先更新自身的commit进度和进行apply，之后会发送一条Message::ConfirmLeader来认可Leader的身份，发送给Leader,Message::ConfirmLeader当中会携带Follower的commit进度，用于给状态机来计算quorum，相当于原本Leader计算commit index。\n1 2 3 4 5 6 7 // Follower.step() pub fn step(mut self, msg: Message) -\u0026gt; Result\u0026lt;Node\u0026gt; { // 先处理commit和apply // .... // 确认Leader身份 self.send(msg.from, Event::ConfirmLeader { commit_index, has_committed })?; } Leader会统计Message::ConfirmLeader信息，每收到一条就会向状态机发送Message::Vote，状态机会进行记录和计算是否达到quorum。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 pub fn step(mut self, msg: Message) -\u0026gt; Result\u0026lt;Node\u0026gt; {\t// A follower received one of our heartbeats and confirms that we // are its leader. If it doesn\u0026#39;t have the commit index in its local // log, replicate the log to it. Event::ConfirmLeader { commit_index, has_committed } =\u0026gt; { let from = msg.from.unwrap(); // 与上层状态机进行交互 self.state_tx.send(Instruction::Vote { term: msg.term, index: commit_index, address: msg.from, })?; if !has_committed { self.send_log(from)?; } } } 在状态机当中，只读请求使用的是一个嵌套的BTreeMap管理的，对应\u0026lt;index, \u0026lt;id, query\u0026gt;\u0026gt;,表示在当前的commit index下，都有哪些请求，这里的id是由Client发送而来的，表示请求的唯一性ID，而不是NodeID。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 struct Query { id: Vec\u0026lt;u8\u0026gt;, term: Term, address: Address, command: Vec\u0026lt;u8\u0026gt;, quorum: u64, votes: HashSet\u0026lt;Address\u0026gt;, } pub struct Driver { // ... /// Execute client queries when they receive a quorum. \u0026lt;index, \u0026lt;id, query\u0026gt;\u0026gt; queries: BTreeMap\u0026lt;Index, BTreeMap\u0026lt;Vec\u0026lt;u8\u0026gt;, Query\u0026gt;\u0026gt;, } fn execute(\u0026amp;mut self, i: Instruction, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { Instruction::Query { id, address, command, index, term, quorum } =\u0026gt; { self.queries.entry(index).or_default().insert( id.clone(), Query { id, term, address, command, quorum, votes: HashSet::new() }, ); } } 在状态机收到了Instruction::Vote之后，分别调用了两个函数：\n1 2 3 4 5 6 7 8 9 10 11 /// Executes a state machine instruction. fn execute(\u0026amp;mut self, i: Instruction, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { debug!(\u0026#34;Executing {:?}\u0026#34;, i); match i { Instruction::Vote { term, index, address } =\u0026gt; { self.query_vote(term, index, address); self.query_execute(state)?; } } Ok(()) } 在self.query_vote当中会根据Instruction::Vote当中的commit index去记录投票，只能对commit_index \u0026lt;= Vote.commit_index的Query进行投票。这样，只有Leader的apply_index \u0026gt;= commit_index之后才能过通过quorum检查。\n1 2 3 4 5 6 7 8 9 10 11 /// Votes for queries up to and including a given commit index for a term by an address. fn query_vote(\u0026amp;mut self, term: Term, commit_index: Index, address: Address) { for (_, queries) in self.queries.range_mut(..=commit_index) { for (_, query) in queries.iter_mut() { if term \u0026gt;= query.term { // 使用HashSet记录保证同一个节点(使用Address表示)，只能对同一个commit_index投票一次 query.votes.insert(address); } } } } 在self.query_execute当中，会获取出所有index \u0026lt;= applied_index的只读请求，然后调用状态机的实现来执行只读请求，然后告知Raft模块的Leader去响应Client：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /// Executes any queries that are ready. fn query_execute(\u0026amp;mut self, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { // self.query_ready()会获取出所有的达到quorum并且index \u0026lt;= applied_index的Query，并且从self.queries当中移除 for query in self.query_ready(state.get_applied_index()) { debug!(\u0026#34;Executing query {:?}\u0026#34;, query.command); let result = state.query(query.command); if let Err(error @ Error::Internal(_)) = result { return Err(error); } self.send( query.address, Event::ClientResponse { id: query.id, response: result.map(Response::Query) }, )? } Ok(()) } Abort 当Client发送了一条请求之后，Raft模块就会将请求保存到Raft状态机当中，之后等待日志commit并apply了，再给Client响应。\n如果当前的Leader收到了更高的Term，那么就会退位，不是Leader自然就不能够处理请求，因此存在状态机当中还未处理完的请求就需要全部放弃，这一部分逻辑定义在leader.become_follower()当中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // If we receive a message for a future term, become a leaderless // follower in it and step the message. If the message is a Heartbeat or // AppendEntries from the leader, stepping it will follow the leader. if msg.term \u0026gt; self.term { return self.become_follower(msg.term)?.step(msg); } /// Transforms the leader into a follower. This can only happen if we find a /// new term, so we become a leaderless follower. fn become_follower(mut self, term: Term) -\u0026gt; Result\u0026lt;RoleNode\u0026lt;Follower\u0026gt;\u0026gt; { assert!(term \u0026gt;= self.term, \u0026#34;Term regression {} -\u0026gt; {}\u0026#34;, self.term, term); assert!(term \u0026gt; self.term, \u0026#34;Can only become follower in later term\u0026#34;); info!(\u0026#34;Discovered new term {}\u0026#34;, term); self.term = term; self.log.set_term(term, None)?; self.state_tx.send(Instruction::Abort)?; Ok(self.become_role(Follower::new(None, None))) } 在Raft状态机当中，收到了旧Leader发送的Instruction::Abort之后，在execute()当中就会调用notify_abort()和query_abort()，将原本用于存储请求的notify重置，同时清除用于保存只读请求的Query，并且再封装一条Message发送回已经退位的Leader，让其告知Client请求已经被取消执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 fn execute(\u0026amp;mut self, i: Instruction, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { debug!(\u0026#34;Executing {:?}\u0026#34;, i); match i { Instruction::Abort =\u0026gt; { self.notify_abort()?; self.query_abort()?; } // ... // ... } } /// Aborts all pending notifications. fn notify_abort(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { for (_, (address, id)) in std::mem::take(\u0026amp;mut self.notify) { self.send(address, Event::ClientResponse { id, response: Err(Error::Abort) })?; } Ok(()) } /// Aborts all pending queries. fn query_abort(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { for (_, queries) in std::mem::take(\u0026amp;mut self.queries) { for (id, query) in queries { self.send( query.address, Event::ClientResponse { id, response: Err(Error::Abort) }, )?; } } Ok(()) } Apply 当一条日志在Raft模块当中得到了大多数的共识之后，就视其为commit，而commit了的日志会Apply到Raft状态机，此时代表完成了一次写入，写入结果对外可见。在toydb当中，Commit和Apply的动作是连贯的，一旦获取到了新的commit_index，就会立即向状态机发送进行apply的commit entry:\nLeader通过自身进行计算 Follower通过Heartbeat得知当前的commit进度，再结合自身的日志复制进度得出一个commit_index 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Leader if commit_index \u0026gt; prev_commit_index { self.log.commit(commit_index)?; // TODO: Move application elsewhere, but needs access to applied index. let mut scan = self.log.scan((prev_commit_index + 1)..=commit_index)?; while let Some(entry) = scan.next().transpose()? { self.state_tx.send(Instruction::Apply { entry })?; } } // Follower Event::Heartbeat { commit_index, commit_term } =\u0026gt; { // .... // Advance commit index and apply entries if possible. let has_committed = self.log.has(commit_index, commit_term)?; let (old_commit_index, _) = self.log.get_commit_index(); if has_committed \u0026amp;\u0026amp; commit_index \u0026gt; old_commit_index { self.log.commit(commit_index)?; let mut scan = self.log.scan((old_commit_index + 1)..=commit_index)?; while let Some(entry) = scan.next().transpose()? { self.state_tx.send(Instruction::Apply { entry })?; } } self.send(msg.from, Event::ConfirmLeader { commit_index, has_committed })?; } 当Raft状态机接收到了Instruction::Apply之后，调用定义在sql/engine/raft当中Raft状态机的实现来进行Apply，然后调用self.notify_applyed()，将Apply的情况通知给下层的Raft模块，此时Raft模块就可以给Client回应了。\n并且某些只读请求也可能因为apply_index的推进，从而可以执行了，调用query_execute尝试执行。query_execute()在上文已经介绍过了，会获取所有达到index \u0026lt;= apply_index的Query尝试执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 fn execute(\u0026amp;mut self, i: Instruction, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { match i { // ... Instruction::Apply { entry } =\u0026gt; { self.apply(state, entry)?; } /// ... } } /// Applies an entry to the state machine. pub fn apply(\u0026amp;mut self, state: \u0026amp;mut dyn State, entry: Entry) -\u0026gt; Result\u0026lt;Index\u0026gt; { // Apply the command. debug!(\u0026#34;Applying {:?}\u0026#34;, entry); match state.apply(entry) { Err(error @ Error::Internal(_)) =\u0026gt; return Err(error), result =\u0026gt; self.notify_applied(state.get_applied_index(), result)?, }; // Try to execute any pending queries, since they may have been submitted for a // commit_index which hadn\u0026#39;t been applied yet. self.query_execute(state)?; Ok(state.get_applied_index()) } /// Notifies a client about an applied log entry, if any. fn notify_applied(\u0026amp;mut self, index: Index, result: Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { if let Some((to, id)) = self.notify.remove(\u0026amp;index) { self.send(to, Event::ClientResponse { id, response: result.map(Response::Mutate) })?; } Ok(()) } Others 在文章的最后，再补充一些零散的内容\nDriver启动 Driver用于驱动状态机，调用Driver.drive()之后，就会不断处理由Raft模块发送而来的Instruction，Driver.drive()会在Node.new()当中调用，随着Node的创建一同启动\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 impl Node { /// Creates a new Raft node, starting as a follower, or leader if no peers. pub async fn new( id: NodeID, peers: HashSet\u0026lt;NodeID\u0026gt;, mut log: Log, mut state: Box\u0026lt;dyn State\u0026gt;, node_tx: mpsc::UnboundedSender\u0026lt;Message\u0026gt;, ) -\u0026gt; Result\u0026lt;Self\u0026gt; { let (state_tx, state_rx) = mpsc::unbounded_channel(); // Driver用于和Raft状态机进行交互，启动状态机运行 let mut driver = Driver::new(id, state_rx, node_tx.clone()); driver.apply_log(\u0026amp;mut *state, \u0026amp;mut log)?; tokio::spawn(driver.drive(state)); let (term, voted_for) = log.get_term()?; let mut node = RoleNode { id, peers, term, log, node_tx, state_tx, role: Follower::new(None, voted_for), }; if node.peers.is_empty() { info!(\u0026#34;No peers specified, starting as leader\u0026#34;); // If we didn\u0026#39;t vote for ourself in the persisted term, bump the // term and vote for ourself to ensure we have a valid leader term. if voted_for != Some(id) { node.term += 1; node.log.set_term(node.term, Some(id))?; } let (last_index, _) = node.log.get_last_index(); Ok(node.become_role(Leader::new(HashSet::new(), last_index)).into()) } else { Ok(node.into()) } } } 信息传递 Raft状态机和Raft模块之间使用Message和Instruction进行交互，Raft模块向Raft状态机发送Instruction，Raft状态机回应Message给Raft模块，Message的发送定义如下：\n1 2 3 4 5 6 7 8 /// Sends a message. /// 状态机使用node_tx进行消息发送，那么对应的接收端就是node_rx fn send(\u0026amp;self, to: Address, event: Event) -\u0026gt; Result\u0026lt;()\u0026gt; { // TODO: This needs to use the correct term. let msg = Message { from: Address::Node(self.node_id), to, term: 0, event }; debug!(\u0026#34;Sending {:?}\u0026#34;, msg); Ok(self.node_tx.send(msg)?) } Follower 如6.824一样，Follower当然也是有状态机的，只不过当中的逻辑非常简单，就没有额外介绍，Follower只需要不断的接受日志，然后更新commit_index然后向状态机发送Instruction::Apply就好，代码在上文也有展示。\nSummary 在toydb当中，相比于MIT-6.824实现了一个比较纯粹的状态机，只需要以Raft模块发送的Instruction作为输入，内部进行状态更新，之后以Message为输出即可，主要起到了一个记录和逻辑判断，通知的作用，不会进行对外的网络交互。Leader在状态机的指示下去回应Client，完成一条请求的执行。\n","permalink":"http://itfischer.space/en/posts/tech/toydb/04-raft-state-machine/","summary":"在正式写这一篇文章之前，原本的规划是在这一部分介绍sql执行引擎，即如何将Raft，Raft状态机，存储引擎组合起来，为SQL的执行去提供支","title":"04-Raft State Machine"},{"content":"在前面的几章，分别分析了Bitcask，构建于Bitcask之上的MVCC，Raft，以及Raft状态机。在本章中，笔者会将这几个模块组合起来，分析MVCC持久化存储，Raft，Raft状态机之间如何交互，来为SQL算子提供支持，处理一次请求。\n在本章，笔者会重点介绍两方面：\n一是SQL Engine的组成，为上层的算子提供支持事务的读写操作 二是 Raft 通信部分，作为分布式数据库，涉及到各个模块之间的通信，其中一部分是在同一节点上使用 channel 实现的，另一部分是跨节点通信，通过 tcp 完成 SQL Engine的逻辑定义在src/sql/engine当中。分为三部分：\n在mod.rs当中，定义了一些通用的Trait，即对上层提供的接口。 在kv.rs当中，定义了一个kv engine，对底层MVCC进行封装，来支持SQL的基础CRUD和事务操作。 在raft.rs当中，定义了Raft的Client和状态机的实现，以及为Raft实现了Transaction Trait 在bustub和miniob当中，这一部分的概念并没有很凸显，在SQL算子当中，只需要对heap_file进行简单的封装，向上提供一个table_iterator用于遍历，和基于heap_file的插入和删除接口即可。\ntoydb是一个分布式的关系型数据库，命令需要先经过Raft达成共识，之后才能够执行和存储。这里定义的SQL Engine，就是屏蔽掉底层的所有实现细节，使上层算子能够像是使用单机的关系型数据库那种完成自身的逻辑。\nExecutor调用Transaction Trait，Executor并不会去关注实现是什么，但是这里的实际调用是Raft对Transaction Trait的实现，调用之后就会想上面描述的那样，走Raft Client的流程，先共识后执行。 事务与执行 这一部分共有三个trait，Engine用于开启事务，Transaction和Catalog用于执行。 在src/sql/engine/mod.rs当中，定义了一个Transaction Trait，在toydb当中，事务是sql执行的基本单位。因此在整个执行流程上都需要按照事务的方式来执行,Raft和kv engine都实现了Transaction Trait\nTransaction继承自Catalog,Catalog当中封装的是与表结构相关的操作，即DDL，Transaction补充普通的操作类型，即DML。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 pub trait Catalog { /// Creates a new table fn create_table(\u0026amp;mut self, table: Table) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Deletes an existing table, or errors if it does not exist fn delete_table(\u0026amp;mut self, table: \u0026amp;str) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Reads a table, if it exists fn read_table(\u0026amp;self, table: \u0026amp;str) -\u0026gt; Result\u0026lt;Option\u0026lt;Table\u0026gt;\u0026gt;; /// Iterates over all tables fn scan_tables(\u0026amp;self) -\u0026gt; Result\u0026lt;Tables\u0026gt;; /// Reads a table, and errors if it does not exist fn must_read_table(\u0026amp;self, table: \u0026amp;str) -\u0026gt; Result\u0026lt;Table\u0026gt; { self.read_table(table)? .ok_or_else(|| Error::Value(format!(\u0026#34;Table {} does not exist\u0026#34;, table))) } /// Returns all references to a table, as table,column pairs. fn table_references(\u0026amp;self, table: \u0026amp;str, with_self: bool) -\u0026gt; Result\u0026lt;Vec\u0026lt;(String, Vec\u0026lt;String\u0026gt;)\u0026gt;\u0026gt; { Ok(self .scan_tables()? .filter(|t| with_self || t.name != table) .map(|t| { ( t.name, t.columns .iter() .filter(|c| c.references.as_deref() == Some(table)) .map(|c| c.name.clone()) .collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;(), ) }) .filter(|(_, cs)| !cs.is_empty()) .collect()) } } pub trait Transaction: Catalog { /// The transaction\u0026#39;s version fn version(\u0026amp;self) -\u0026gt; u64; /// Whether the transaction is read-only fn read_only(\u0026amp;self) -\u0026gt; bool; /// Commits the transaction fn commit(self) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Rolls back the transaction fn rollback(self) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Creates a new table row fn create(\u0026amp;mut self, table: \u0026amp;str, row: Row) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Deletes a table row fn delete(\u0026amp;mut self, table: \u0026amp;str, id: \u0026amp;Value) -\u0026gt; Result\u0026lt;()\u0026gt;; /// Reads a table row, if it exists fn read(\u0026amp;self, table: \u0026amp;str, id: \u0026amp;Value) -\u0026gt; Result\u0026lt;Option\u0026lt;Row\u0026gt;\u0026gt;; /// Reads an index entry, if it exists fn read_index(\u0026amp;self, table: \u0026amp;str, column: \u0026amp;str, value: \u0026amp;Value) -\u0026gt; Result\u0026lt;HashSet\u0026lt;Value\u0026gt;\u0026gt;; /// Scans a table\u0026#39;s rows fn scan(\u0026amp;self, table: \u0026amp;str, filter: Option\u0026lt;Expression\u0026gt;) -\u0026gt; Result\u0026lt;Scan\u0026gt;; /// Scans a column\u0026#39;s index entries fn scan_index(\u0026amp;self, table: \u0026amp;str, column: \u0026amp;str) -\u0026gt; Result\u0026lt;IndexScan\u0026gt;; /// Updates a table row fn update(\u0026amp;mut self, table: \u0026amp;str, id: \u0026amp;Value, row: Row) -\u0026gt; Result\u0026lt;()\u0026gt;; } Engine 这里的Engine与之前在Bitcask和MVCC当中所说的engine不是一个概念，在存储方面，engine指的是存储引擎，而这里的Engine是一个SQL引擎，Engine的本质上为一个Wrapper，对Transaction进行包裹，提供了三种类型的begin用于开启事务，为别对应普通的读写事务，只读事务和time-travel类型的只读事务。如果没有开启事务，就调用Engine当中定义的方法开启一个事务，当事务开启之后，就可以调用Transaction Trait当中的内容来执行事务。\n此外还有一个session函数，用于返回一个Session，一个Session用于执行一个独立的statements(即一条SQL，SQL解析会生成一个Statement)。\nEngine为一个Trait，kv与Raft分别给出了自己的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pub trait Engine: Clone { /// The transaction type type Transaction: Transaction; /// Begins a read-write transaction. fn begin(\u0026amp;self) -\u0026gt; Result\u0026lt;Self::Transaction\u0026gt;; /// Begins a read-only transaction. fn begin_read_only(\u0026amp;self) -\u0026gt; Result\u0026lt;Self::Transaction\u0026gt;; /// Begins a read-only transaction as of a historical version. fn begin_as_of(\u0026amp;self, version: u64) -\u0026gt; Result\u0026lt;Self::Transaction\u0026gt;; /// Begins a session for executing individual statements fn session(\u0026amp;self) -\u0026gt; Result\u0026lt;Session\u0026lt;Self\u0026gt;\u0026gt; { Ok(Session { engine: self.clone(), txn: None }) } } Session 一个Session对应一条SQL的执行，在其中只有两个字段，engine用于真正执行sql，txn保存当前的事务：\n1 2 3 4 5 6 pub struct Session\u0026lt;E: Engine\u0026gt; { /// The underlying engine engine: E, /// The current session transaction, if any txn: Option\u0026lt;E::Transaction\u0026gt;, } Session只有一个方法execute()，这是一条SQL执行的起始点，在完成了网络连接部分之后，就会调用到这里，传入未解析的SQL，在其中首先进行解析，生成对应的Statement，之后根据Statement的类型决定如何执行，大致分为各种类型的Begin，Commit,Rollback,和正常SQL这四种类型。\n然后在这其中，会完成 Planner → Optimizer → Executor的这个过程，完成SQL执行的过程,这里简单列举几个：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 pub fn execute(\u0026amp;mut self, query: \u0026amp;str) -\u0026gt; Result\u0026lt;ResultSet\u0026gt; { // FIXME We should match on self.txn as well, but get this error: // error[E0009]: cannot bind by-move and by-ref in the same pattern // ...which seems like an arbitrary compiler limitation match Parser::new(query).parse()? { ast::Statement::Begin { read_only: false, as_of: None } =\u0026gt; { let txn = self.engine.begin()?; let result = ResultSet::Begin { version: txn.version(), read_only: false }; self.txn = Some(txn); Ok(result) } ast::Statement::Commit =\u0026gt; { let txn = self.txn.take().unwrap(); let version = txn.version(); txn.commit()?; Ok(ResultSet::Commit { version }) } ast::Statement::Rollback =\u0026gt; { let txn = self.txn.take().unwrap(); let version = txn.version(); txn.rollback()?; Ok(ResultSet::Rollback { version }) } ast::Statement::Explain(statement) =\u0026gt; self.read_with_txn(|txn| { Ok(ResultSet::Explain(Plan::build(*statement, txn)?.optimize(txn)?.0)) }), statement @ ast::Statement::Select { .. } =\u0026gt; { let mut txn = self.engine.begin_read_only()?; let result = Plan::build(statement, \u0026amp;mut txn)?.optimize(\u0026amp;mut txn)?.execute(\u0026amp;mut txn); txn.rollback()?; result } statement =\u0026gt; { let mut txn = self.engine.begin()?; match Plan::build(statement, \u0026amp;mut txn)?.optimize(\u0026amp;mut txn)?.execute(\u0026amp;mut txn) { Ok(result) =\u0026gt; { txn.commit()?; Ok(result) } Err(error) =\u0026gt; { txn.rollback()?; Err(error) } } } } 事务实现 在toydb当中，SQL的执行单位为事务，即便是一条SQL，也会默认开启一个事务，SQL的执行首先会去Raft层进行共识，达成quorum之后再交给底层的MVCC去执行和存储，因此对于一个事务，其无论是begin，commit还是正常的sql执行，都需要先走raft，然后再走MVCC。\n那么对于二者事务的动作，就会进行统一的定义，即上面介绍的三个trait：Transacion、Catalog、Engine。MVCC和Raft都给出了自己的实现。\n以begin为例，调用begin时会先调用到Raft当中对Transaction中begin的实现，之后Apply时调用到MVCC中对begin的实现，完成一次完整的请求。\n在本段中，由于Raft会依赖MVCC，因此以一种自底向上的模式，先介绍MVCC部分，之后再分析Raft。在Raft当中，除了事务相关的部分，还有上一章遗留的状态机实现，也一并在这里分析。\nKV Engine 在kv.rs当中，主要定义了两部分内容，首先是一个KV结构体，为一个storage::mvcc::MVCC的Wrapper。而另一部分就是Transaction Trait的实现。\nKV 在KV当中，主要利用带有MVCC逻辑的kv存储引擎来实现事务的功能和存储元数据，供raft.rs当中的某些实现调用，为其提供支持。 KV本身有三个方法：\nresume():通过传入的State，恢复出事务的状态，这里的state就是在02-MVCC当中介绍的TransactionState get_metadata():存储元数据，供Raft状态机使用，存储last_applied_index,调用的是底层的不带版本的kv存储 set_metadata():存储元数据，同上 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /// A SQL engine based on an underlying MVCC key/value store. pub struct KV\u0026lt;E: storage::engine::Engine\u0026gt; { /// The underlying key/value store. pub(super) kv: storage::mvcc::MVCC\u0026lt;E\u0026gt;, } impl\u0026lt;E: storage::engine::Engine\u0026gt; KV\u0026lt;E\u0026gt; { /// Creates a new key/value-based SQL engine pub fn new(engine: E) -\u0026gt; Self { Self { kv: storage::mvcc::MVCC::new(engine) } } /// Resumes a transaction from the given state pub fn resume( \u0026amp;self, state: storage::mvcc::TransactionState, ) -\u0026gt; Result\u0026lt;\u0026lt;Self as super::Engine\u0026gt;::Transaction\u0026gt; { Ok(\u0026lt;Self as super::Engine\u0026gt;::Transaction::new(self.kv.resume(state)?)) } /// Fetches an unversioned metadata value pub fn get_metadata(\u0026amp;self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt; { self.kv.get_unversioned(key) } /// Sets an unversioned metadata value pub fn set_metadata(\u0026amp;self, key: \u0026amp;[u8], value: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { self.kv.set_unversioned(key, value) } } 此外KV还实现了定义在mod.rs当中的Engine Trait，同样也是直接调用底层的对应的方法即可。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 impl\u0026lt;E: storage::engine::Engine\u0026gt; super::Engine for KV\u0026lt;E\u0026gt; { type Transaction = Transaction\u0026lt;E\u0026gt;; fn begin(\u0026amp;self) -\u0026gt; Result\u0026lt;Self::Transaction\u0026gt; { Ok(Self::Transaction::new(self.kv.begin()?)) } fn begin_read_only(\u0026amp;self) -\u0026gt; Result\u0026lt;Self::Transaction\u0026gt; { Ok(Self::Transaction::new(self.kv.begin_read_only()?)) } fn begin_as_of(\u0026amp;self, version: u64) -\u0026gt; Result\u0026lt;Self::Transaction\u0026gt; { Ok(Self::Transaction::new(self.kv.begin_as_of(version)?)) } } Transaction 和KV一样，Transaction也是一个Wrapper，其中只有一个字段，就是mvcc当中的Transaction。 这一部分的逻辑就是先做一些预处理，将原本只有get set和begin等方法的MVCC kv存储引擎，扩展为MVCC的SQL存储引擎，支持建表，插入删除一行数据，全表扫描等SQL功能\n1 2 3 pub struct Transaction\u0026lt;E: storage::engine::Engine\u0026gt; { txn: storage::mvcc::Transaction\u0026lt;E\u0026gt;, } Transaction的实现分成了三部分：\n自身定义了一些存储和加载索引的操作 实现Transaction trait和 实现Catalog trait。 Index 首先，toydb当中的索引是非聚簇索引 由于toydb的底层是KV存储，因此并不能像bustub那样去分文件存储，toydb的索引格式为：\u0026lt;table + column + key,HashSet\u0026lt;primary_key\u0026gt;\u0026gt;的复合key-value。这样便可以使用kv存储引擎来保存。在实现上value使用HashSet保存一个表中同一列所有值相同的主键id\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 fn index_save( \u0026amp;mut self, table: \u0026amp;str, column: \u0026amp;str, value: \u0026amp;Value, index: HashSet\u0026lt;Value\u0026gt;, ) -\u0026gt; Result\u0026lt;()\u0026gt; { let key = Key::Index(table.into(), column.into(), value.into()).encode()?; if index.is_empty() { self.txn.delete(\u0026amp;key) } else { self.txn.set(\u0026amp;key, serialize(\u0026amp;index)?) } } Transaction 到处都是Transaction，这里的Transaction是构建在storage::mvcc之上，来实现mod.rs当中的Transaction Trait。将kv造作转换为基础的sql操作，如Insert、Delete、SeqScan、IndexScan等。挑几个看一下：\ncreate\ncreate用于在已经存在的表当中插入一条数据：\n根据table name读取出table的metadata 根据metadata校验插入的row，检验其在column上与table是否符合 根据主键id进行读取，如果存在则放弃插入，不允许有primary_key重复的row 调用底层storage::mvcc存储row，key为table + primary_key，val为row 调用上面的index_save，index_load更新索引 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 fn create(\u0026amp;mut self, table: \u0026amp;str, row: Row) -\u0026gt; Result\u0026lt;()\u0026gt; { // (1) let table = self.must_read_table(table)?; // (2) table.validate_row(\u0026amp;row, self)?; let id = table.get_row_key(\u0026amp;row)?; // (3) if self.read(\u0026amp;table.name, \u0026amp;id)?.is_some() { return Err(Error::Value(format!( \u0026#34;Primary key {} already exists for table {}\u0026#34;, id, table.name ))); } // (4) self.txn.set(\u0026amp;Key::Row((\u0026amp;table.name).into(), (\u0026amp;id).into()).encode()?, serialize(\u0026amp;row)?)?; // (5) Update indexes for (i, column) in table.columns.iter().enumerate().filter(|(_, c)| c.index) { let mut index = self.index_load(\u0026amp;table.name, \u0026amp;column.name, \u0026amp;row[i])?; index.insert(id.clone()); self.index_save(\u0026amp;table.name, \u0026amp;column.name, \u0026amp;row[i], index)?; } Ok(()) } scan\nscan用于实现一个SeqScan + Filter的功能(函数式编程魅力时刻)：\n首先根据table name读取出table metadata 创建一个KeyPrefix，传入table.name，这样进行前缀扫描会从存储引擎当中获取出该table的所有的key，转换为迭代器 使用一个map来处理读取结果，iter遍历返回的是一个Result\u0026lt;(Vec\u0026lt;u8\u0026gt;,Vec\u0026lt;u8\u0026gt;),Error\u0026gt;的key-val对，如果Ok()，就读取出val进行反序列化 对反序列化得到的value进行谓词匹配，根据匹配结果进行进一步的封装 collect成Vec，转换为iter向上返回 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 fn scan(\u0026amp;self, table: \u0026amp;str, filter: Option\u0026lt;Expression\u0026gt;) -\u0026gt; Result\u0026lt;super::Scan\u0026gt; { let table = self.must_read_table(table)?; Ok(Box::new( self.txn .scan_prefix(\u0026amp;KeyPrefix::Row((\u0026amp;table.name).into()).encode()?)? .iter() .map(|r| r.and_then(|(_, v)| deserialize(\u0026amp;v))) .filter_map(move |r| match r { Ok(row) =\u0026gt; match \u0026amp;filter { Some(filter) =\u0026gt; match filter.evaluate(Some(\u0026amp;row)) { Ok(Value::Boolean(b)) if b =\u0026gt; Some(Ok(row)), Ok(Value::Boolean(_)) | Ok(Value::Null) =\u0026gt; None, Ok(v) =\u0026gt; Some(Err(Error::Value(format!( \u0026#34;Filter returned {}, expected boolean\u0026#34;, v )))), Err(err) =\u0026gt; Some(Err(err)), }, None =\u0026gt; Some(Ok(row)), }, err =\u0026gt; Some(err), }) .collect::\u0026lt;Vec\u0026lt;_\u0026gt;\u0026gt;() .into_iter(), )) } read \u0026amp; read_index\n这两个的实现差不多，都是构建出一个key，然后去存储引擎当中读取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 fn read(\u0026amp;self, table: \u0026amp;str, id: \u0026amp;Value) -\u0026gt; Result\u0026lt;Option\u0026lt;Row\u0026gt;\u0026gt; { self.txn .get(\u0026amp;Key::Row(table.into(), id.into()).encode()?)? .map(|v| deserialize(\u0026amp;v)) .transpose() } fn read_index(\u0026amp;self, table: \u0026amp;str, column: \u0026amp;str, value: \u0026amp;Value) -\u0026gt; Result\u0026lt;HashSet\u0026lt;Value\u0026gt;\u0026gt; { if !self.must_read_table(table)?.get_column(column)?.index { return Err(Error::Value(format!(\u0026#34;No index on {}.{}\u0026#34;, table, column))); } self.index_load(table, column, value) } 其他的受限于篇幅，就不在本文展开了，读者可自行阅读\nCatalog\n在Catalog当中，实现的方法都是和表结构相关的，即DDL，如创建删除表等。总体来说和Transaction逻辑差不多\ncreate_table\n和create差不多，创建一个table的key，存到存储引擎当中，key为table.name，val为table.metadata。\n1 2 3 4 5 6 7 8 9 10 11 12 pub struct Table { pub name: String, pub columns: Vec\u0026lt;Column\u0026gt;, } fn create_table(\u0026amp;mut self, table: Table) -\u0026gt; Result\u0026lt;()\u0026gt; { if self.read_table(\u0026amp;table.name)?.is_some() { return Err(Error::Value(format!(\u0026#34;Table {} already exists\u0026#34;, table.name))); } table.validate(self)?; self.txn.set(\u0026amp;Key::Table((\u0026amp;table.name).into()).encode()?, serialize(\u0026amp;table)?) } 其他的都差不多，就不展示了。\nRaft 除了Raft Package之外，其他的与Raft相关的逻辑都定义在src/sql/engine/raft.rs当中，大致分为：\nRaft状态机的实现 Raft对事务的支持，与上面KV相对应，包括Transaction、Catalog和Engine Raft Client，负责与Raft Server进行通信，将需要执行的命令发送给Server 状态机实现 在介绍完KV Engine的实现之后，可以补全上一章留下的Raft状态机的实现了。在上一章中，说明了Raft状态机会使用Notify和Querys来分别管理写请求和只读请求，二者的处理方式不一样，写请求需要走日志达成共识，而读请求可以使用ReadIndex来进行优化。\n请求类型\n为了区分读写请求，使用枚举进行定义： 首先在src/raft/message.rs当中，定义了一个枚举，用于确定Request的类型，是读(Query)还是写(Mutate):\n1 2 3 4 5 6 7 /// A client request. #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub enum Request { Query(Vec\u0026lt;u8\u0026gt;), Mutate(Vec\u0026lt;u8\u0026gt;), Status, } 在src/sql/engine/raft.rs当中将请求类型进行的细化，使用枚举根据sql算子的类型进行分类(也并不是与SQL算子一一对应的，只不过是算子执行过程中需要的操作，但总体上还是和SQL还是能对的上号的)：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 enum Query { /// Fetches engine status Status, /// Reads a row Read { txn: TransactionState, table: String, id: Value }, /// Reads an index entry ReadIndex { txn: TransactionState, table: String, column: String, value: Value }, /// Scans a table\u0026#39;s rows Scan { txn: TransactionState, table: String, filter: Option\u0026lt;Expression\u0026gt; }, /// Scans an index ScanIndex { txn: TransactionState, table: String, column: String }, /// Scans the tables ScanTables { txn: TransactionState }, /// Reads a table ReadTable { txn: TransactionState, table: String }, } enum Mutation { /// Begins a transaction Begin { read_only: bool, as_of: Option\u0026lt;u64\u0026gt; }, /// Commits the given transaction Commit(TransactionState), /// Rolls back the given transaction Rollback(TransactionState), /// Creates a new row Create { txn: TransactionState, table: String, row: Row }, /// Deletes a row Delete { txn: TransactionState, table: String, id: Value }, /// Updates a row Update { txn: TransactionState, table: String, id: Value, row: Row }, /// Creates a table CreateTable { txn: TransactionState, schema: Table }, /// Deletes a table DeleteTable { txn: TransactionState, table: String }, } 状态机\n状态机实现定义如下，其中有两个字段，一个是在上文当中介绍的KV Engine，用于持久化存储数据，另外一个是applied_index：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 pub struct State\u0026lt;E: storage::engine::Engine\u0026gt; { /// The underlying KV SQL engine engine: super::KV\u0026lt;E\u0026gt;, /// The last applied index applied_index: u64, } pub trait State: Send { /// Returns the last applied index from the state machine. fn get_applied_index(\u0026amp;self) -\u0026gt; Index; /// Applies a log entry to the state machine. If it returns Error::Internal, /// the Raft node halts. Any other error is considered applied and returned /// to the caller. /// /// The entry may contain a noop command, which is committed by Raft during /// leader changes. This still needs to be applied to the state machine to /// properly track the applied index, and returns an empty result. /// /// TODO: consider using runtime assertions instead of Error::Internal. fn apply(\u0026amp;mut self, entry: Entry) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;; /// Queries the state machine. All errors are propagated to the caller. fn query(\u0026amp;self, command: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;; } get_applied_index实现很简单，直接返回即可\napply将Raft当中commit的日志交给Raft状态机来执行，这里需要Apply的一定是Mutate类型的，在Raft状态机当中，解析出对应的命令之后，根据Mutate的具体类型，调用mutate(\u0026amp;mut self, mutation: Mutation)来执行命令，在该函数当中，根据命令类型来调用底层KV Engine对应的实现来执行，之后使用set_metadata()来更新last_applied_index。\n在02-MVCC当中，留下了一个伏笔：\n在注释当中，对于TransactionState的设计理念做了比较详细的说明，简而言之就是，事务状态的设计使得事务可以在不同的组件之间安全地传递，并且可以在不直接引用事务本身的情况下被引用，有助于简化事务管理。 A Transaction\u0026rsquo;s state, which determines its write version and isolation. It is separate from Transaction to allow it to be passed around independently of the engine. There are two main motivations for this:\nIt can be exported via Transaction.state(), (de)serialized, and later used to instantiate a new functionally equivalent Transaction via Transaction::resume(). This allows passing the transaction between the storage engine and SQL engine (potentially running on a different node) across the Raft state machine boundary. It can be borrowed independently of Engine, allowing references to it in VisibleIterator, which would otherwise result in self-references. 在enum Mutate当中，会携带一个TransactionState，这个State会随着命令被传递，在mutate()当中，获取到TransactionState就可以恢复出事务的状态，然后就可以继续执行该事务，实现如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 fn apply(\u0026amp;mut self, entry: Entry) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt; { assert_eq!(entry.index, self.applied_index + 1, \u0026#34;entry index not after applied index\u0026#34;); let result = match \u0026amp;entry.command { Some(command) =\u0026gt; match self.mutate(bincode::deserialize(command)?) { error @ Err(Error::Internal(_)) =\u0026gt; return error, // don\u0026#39;t record as applied result =\u0026gt; result, }, None =\u0026gt; Ok(Vec::new()), }; self.applied_index = entry.index; self.engine.set_metadata(b\u0026#34;applied_index\u0026#34;, bincode::serialize(\u0026amp;entry.index)?)?; result } fn mutate(\u0026amp;mut self, mutation: Mutation) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt; { match mutation { // ... // ... // 使用resume恢复出事务状态，继续事务的执行 Mutation::CreateTable { txn, schema } =\u0026gt; { bincode::serialize(\u0026amp;self.engine.resume(txn)?.create_table(schema)?) } Mutation::DeleteTable { txn, table } =\u0026gt; { bincode::serialize(\u0026amp;self.engine.resume(txn)?.delete_table(\u0026amp;table)?) } } } Query也差不多，反序列化后调用KV Engine当中对应的实现即可，完成一条只读请求。\nRaft Client Raft Client并不是一个独立运行的Client，只是用于在SQL执行前，将对应的命令先发送给Server去共识，之后再真实执行，一次完整的请求过程如下：\n解析sql，调用对应的sql算子开始执行，将需要执行的命令通过client发送给server server的Leader拿到命令之后开始进行日志复制，尝试达成共识 达成共识之后，在Raft状态机调用Apply或者Query执行对应的命令 Leader从状态机当中接收到对应的执行结果，返回给Client Client 处理执行结果，向上返回完成 SQL 的执行。 在raft.rs当中，定义了一个Client的结构体，其中只有一个字段，就是用来发送请求的channel,通过该channel，会将请求发送到Server Leader处，client-server之间怎样进行通信的，会留到后面单独分析。\n1 2 3 4 5 6 7 /// A client for the local Raft node. #[derive(Clone)] struct Client { // 这里的tx为raft_tx，raft_server那边拿到的为raft_rx，raft_rx在Raft模块当中被重命名为client_tx // 所以这里的tx对应的是client_tx tx: mpsc::UnboundedSender\u0026lt;(raft::Request, oneshot::Sender\u0026lt;Result\u0026lt;raft::Response\u0026gt;\u0026gt;)\u0026gt;, } 在实现上，定义一个execute()用于承担发送命令的功能，在此之上分别封装mutate和query用于发送读写请求:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 impl Client { /// Creates a new Raft client. fn new( tx: mpsc::UnboundedSender\u0026lt;(raft::Request, oneshot::Sender\u0026lt;Result\u0026lt;raft::Response\u0026gt;\u0026gt;)\u0026gt;, ) -\u0026gt; Self { Self { tx } } /// Executes a request against the Raft cluster. /// 执行一个请求，创建一个一次性的channel，将channel的发送端交给raft.server，raft server再将request /// 发送到channeld当中，Raft node就会受到对应的Request，然后在此处阻塞，等待Leader的回应 fn execute(\u0026amp;self, request: raft::Request) -\u0026gt; Result\u0026lt;raft::Response\u0026gt; { let (response_tx, response_rx) = oneshot::channel(); self.tx.send((request, response_tx))?; futures::executor::block_on(response_rx)? } /// Mutates the Raft state machine, deserializing the response into the /// return type. fn mutate\u0026lt;V: DeserializeOwned\u0026gt;(\u0026amp;self, mutation: Mutation) -\u0026gt; Result\u0026lt;V\u0026gt; { match self.execute(raft::Request::Mutate(bincode::serialize(\u0026amp;mutation)?))? { raft::Response::Mutate(response) =\u0026gt; Ok(bincode::deserialize(\u0026amp;response)?), resp =\u0026gt; Err(Error::Internal(format!(\u0026#34;Unexpected Raft mutation response {:?}\u0026#34;, resp))), } } /// Queries the Raft state machine, deserializing the response into the /// return type. fn query\u0026lt;V: DeserializeOwned\u0026gt;(\u0026amp;self, query: Query) -\u0026gt; Result\u0026lt;V\u0026gt; { match self.execute(raft::Request::Query(bincode::serialize(\u0026amp;query)?))? { raft::Response::Query(response) =\u0026gt; Ok(bincode::deserialize(\u0026amp;response)?), resp =\u0026gt; Err(Error::Internal(format!(\u0026#34;Unexpected Raft query response {:?}\u0026#34;, resp))), } } /// Fetches Raft node status. fn status(\u0026amp;self) -\u0026gt; Result\u0026lt;raft::Status\u0026gt; { match self.execute(raft::Request::Status)? { raft::Response::Status(status) =\u0026gt; Ok(status), resp =\u0026gt; Err(Error::Internal(format!(\u0026#34;Unexpected Raft status response {:?}\u0026#34;, resp))), } } } 在这一部分，定义了一个Raft结构体，作为Client的Wrapper，并且为Raft实现了engine trait，在地位上与上文的KV Engine相同，用于开启一个事务。\n1 2 3 pub struct Raft { client: Client, } Transaction 这一部分和KV Engine当中的Transaction一样，都是一个Wrapper，在KV Engine中，Transaction作为Wrapper调用了底层MVCC的事务实现，将SQL操作转换成了KV操作。\n在Raft Transaction当中，同样是一个Wrapper，封装成各种细化的请求类型通过Client进行发送,类型很多，不一一列举了，Catalog同理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 pub struct Transaction { client: Client, state: TransactionState, } impl super::Transaction for Transaction { fn create(\u0026amp;mut self, table: \u0026amp;str, row: Row) -\u0026gt; Result\u0026lt;()\u0026gt; { self.client.mutate(Mutation::Create { txn: self.state.clone(), table: table.to_string(), row, }) } fn delete(\u0026amp;mut self, table: \u0026amp;str, id: \u0026amp;Value) -\u0026gt; Result\u0026lt;()\u0026gt; { self.client.mutate(Mutation::Delete { txn: self.state.clone(), table: table.to_string(), id: id.clone(), }) } fn read(\u0026amp;self, table: \u0026amp;str, id: \u0026amp;Value) -\u0026gt; Result\u0026lt;Option\u0026lt;Row\u0026gt;\u0026gt; { self.client.query(Query::Read { txn: self.state.clone(), table: table.to_string(), id: id.clone(), }) } // ... } 至此，sql engine部分的内容全部介绍完毕，单论某一部分都很简单，没有什么复杂的逻辑，无非是定义了一些trait，和两个wrapper，将MVCC存储和Raft Client封装成SQL事务的模式。这一部分比较复杂的是各个模块之间的交互，和信息传递。在本章的剩余部分，笔者会重点介绍各部分之间是如何进行通信的。\n共识粒度 在toydb当中，sql engine的基本单位为一条事务命令，这里的事务命令指的是在src/sql/engine/mod.rs当中定义的Transaction Trait当中的命令，如create,delete,scan等，或者说，共识的粒度是对存储引擎的一次操作。\n到了sql engine这里，实际上是已经没有的SQL的概念的，sql engine只实现了Transaction Trait并对外提供，因此并没有采用一条SQL来作为共识的单位。\n更重要的是，相同的SQL在不同的时刻，不同的节点上执行，得到的执行计划，执行结果都有可能是不一样的，即无法保证幂等性，最简单是SELECT NOW()。经过Raft同步到从节点上执行会产生时间差，从而导致执行结果不一致。此外，SQL本身也充满了复杂性包括多表查询、子查询、事务嵌套等，使用SQL进行同步也会为系统引入额外的复杂性。\n虽然本系列还没有分析SQL是如何执行的，这里挑一个比较简单的Insert为例，说明一下toydb当中共识的粒度：\n执行insert时，首先需要读取出当前的table，这里调用了txn.must_read_table()，作为一条只读请求交给Raft去形成共识 之后每插入一行，都会调用txn.create()，作为一条写请求发送给Raft 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 impl\u0026lt;T: Transaction\u0026gt; Executor\u0026lt;T\u0026gt; for Insert { fn execute(self: Box\u0026lt;Self\u0026gt;, txn: \u0026amp;mut T) -\u0026gt; Result\u0026lt;ResultSet\u0026gt; { let table = txn.must_read_table(\u0026amp;self.table)?; let mut count = 0; for expressions in self.rows { let mut row = expressions.into_iter().map(|expr| expr.evaluate(None)).collect::\u0026lt;Result\u0026lt;_\u0026gt;\u0026gt;()?; if self.columns.is_empty() { row = Self::pad_row(\u0026amp;table, row)?; } else { row = Self::make_row(\u0026amp;table, \u0026amp;self.columns, row)?; } txn.create(\u0026amp;table.name, row)?; count += 1; } Ok(ResultSet::Create { count }) } } 通信方式 toydb作为一个分布式的关系型数据库，由于引入了Raft，从而导致存在很多网络通信和信息交换的方式，大致类型有：\n最基础的Client-Server之间的通信，发送sql与执行sql Raft Client与Raft Server之间的通信 Raft Node节点之间的通信 Raft Node与Raft状态机之间的通信 SQL Client-Server 作为一个Client-Server结构的数据库，最基础的通信就是Client与Server之间的，Client发送sql，Server执行sql。Client与Server显然不会在同一台计算机上运行，二者之间是使用tcp进行连接的。 Client端不是主要的内容，Server端就是监听端口，不断获取请求，然后创建一个Session来执行SQL,之后将结果返回给Client。\n这一部分的逻辑定义在src/server.rs当中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /// Serves SQL clients. async fn serve_sql(listener: TcpListener, engine: sql::engine::Raft) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut listener = TcpListenerStream::new(listener); while let Some(socket) = listener.try_next().await? { let peer = socket.peer_addr()?; let session = Session::new(engine.clone())?; tokio::spawn(async move { info!(\u0026#34;Client {} connected\u0026#34;, peer); match session.handle(socket).await { Ok(()) =\u0026gt; info!(\u0026#34;Client {} disconnected\u0026#34;, peer), Err(err) =\u0026gt; error!(\u0026#34;Client {} error: {}\u0026#34;, peer, err), } }); } Ok(()) } Server 剩余三种通信都是属于Server端的内部通信，在toydb当中，通过一个eventloop处理了所有的通信，在创建eventloop时，传入了四个channel：\ntcp_tx:raft节点之间相互交流的发送端，用于将底层node塞入信箱当中的Message发送给其他的节点 node_rx:node Message消息的接收端，用于从raft模块当中接收Message，然后交给tcp_tx去发送，同时也会处理状态机发送给Raft模块的Messaga tcp_rx:raft节点之间相互交流的接收端，接收其他节点传来的Message，然后交给自身的Raft去执行 client_rx:接收Raft client发送的Request，交给自身的Raft去执行，达成共识 这一部分的逻辑主体是通过tokio::select!来实现的，在逻辑上与go当中的select是差不多的，只不过go的select是监听同步channel，而tokio::select!是等待异步任务的执行完成，分别从上述的三个channel当中获取Message，推动Raft节点，后续其他三种类型的通信都会依赖于这一部分的逻辑。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 loop { tokio::select! { // 监听ticker，驱动下层Raft，间隔为100ms _ = ticker.tick() =\u0026gt; node = node.tick()?, // 获取从其他raft节点发送而来的Message，交给下层的Raft去处理 Some(msg) = tcp_rx.next() =\u0026gt; node = node.step(msg)?, // 获取下层Raft放入信箱的Message，发送给对应的节点 Some(msg) = node_rx.next() =\u0026gt; { match msg { Message{to: Address::Node(_), ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Broadcast, ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Client, event: Event::ClientResponse{ id, response }, ..} =\u0026gt; { if let Some(response_tx) = requests.remove(\u0026amp;id) { response_tx .send(response) .map_err(|e| Error::Internal(format!(\u0026#34;Failed to send response {:?}\u0026#34;, e)))?; } } _ =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Unexpected message {:?}\u0026#34;, msg))), } } // 获取client发送的消息，交给Raft模块去处理，这里的client并不是用户的client，而是要执行命令的sql端 Some((request, response_tx)) = client_rx.next() =\u0026gt; { let id = Uuid::new_v4().as_bytes().to_vec(); let msg = Message{ from: Address::Client, to: Address::Node(node.id()), term: 0, event: Event::ClientRequest{id: id.clone(), request}, }; node = node.step(msg)?; requests.insert(id, response_tx); } } } Raft Client-Server Client Raft Client的定义在src/sql/engine/raft.rs当中Client当中只有一个字段tx，为一个消息类型为(raft::Request, oneshot::Sender\u0026lt;Result\u0026lt;raft::Response\u0026gt;)的channel发送端，使用这个给Raft Server发送消息。\ntx的来源为在Server.serve()中创建:\nraft_tx为raft client-server之间消息的发送端，交给sql engine去发送 raft_rx为raft client-server之间消息的接收端，交给raft server去接收,raft_rx在server端被重命名为client_rx。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 struct Client { // 这里的tx为raft_tx，raft_server那边拿到的为raft_rx，raft_rx在Raft模块当中被重命名为client_rx // 所以这里的tx对应的是client_tx tx: mpsc::UnboundedSender\u0026lt;(raft::Request, oneshot::Sender\u0026lt;Result\u0026lt;raft::Response\u0026gt;\u0026gt;)\u0026gt;, } pub async fn serve(self) -\u0026gt; Result\u0026lt;()\u0026gt; { // listener // ... // raft_tx为raft client-server之间消息的发送端，交给sql_engine去发送 // raft_rx为raft client-server之间消息的接收端，交给raft server去收 let (raft_tx, raft_rx) = mpsc::unbounded_channel(); let sql_engine = sql::engine::Raft::new(raft_tx); tokio::try_join!( self.raft.serve(raft_listener, raft_rx), Self::serve_sql(sql_listener, sql_engine), )?; Ok(()) } 读写请求都会调用execute()来向Raft Server去通信。在execute()当中，会调用oneshot::channel()创建一个一次性的channel，将channel的消息发送端response_tx和request一同发送给Raft Server。\nServer接收并执行完之后，会使用传过去的发送端再将响应发送回来，Client等待从接收端获取消息，拿到Raft Server对request的执行结果向上返回。此时，创建的oneshot channel就可以销毁了。\nServer\n接收：\nServer端的接收逻辑就在上面所说的eventloop当中，从client_rx(上面传来的raft_rx)接收到Client发送的request和response_tx。\n由于请求需要先通过Raft完成共识，之后Apply了才能够执行，这里先使用一个HashMap保存请求，等到Apply并执行完之后再从HashMap中获取出暂存的response_rx，再响应Client。\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 获取client发送的消息，交给Raft模块去处理，这里的client并不是用户的client，而是要执行命令的sql端 // 暂存命令，等到日志Apply并执行完之后再响应客户端 Some((request, response_tx)) = client_rx.next() =\u0026gt; { let id = Uuid::new_v4().as_bytes().to_vec(); let msg = Message{ from: Address::Client, to: Address::Node(node.id()), term: 0, event: Event::ClientRequest{id: id.clone(), request}, }; node = node.step(msg)?; requests.insert(id, response_tx); } 发送：\n在Raft状态机当中，当请求Apply时，就会发送一条消息给Raft Leader Node，写请求通过notify_applied()，读请求通过query_execute()，通知Leader目前该命令已经Apply，并且执行完成，可以响应Client了,这一部分的逻辑定义在src/raft/state.rs中：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 pub fn apply(\u0026amp;mut self, state: \u0026amp;mut dyn State, entry: Entry) -\u0026gt; Result\u0026lt;Index\u0026gt; { // Apply the command. debug!(\u0026#34;Applying {:?}\u0026#34;, entry); match state.apply(entry) { Err(error @ Error::Internal(_)) =\u0026gt; return Err(error), result =\u0026gt; self.notify_applied(state.get_applied_index(), result)?, }; // Try to execute any pending queries, since they may have been submitted for a // commit_index which hadn\u0026#39;t been applied yet. self.query_execute(state)?; Ok(state.get_applied_index()) } /// Notifies a client about an applied log entry, if any. fn notify_applied(\u0026amp;mut self, index: Index, result: Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { if let Some((to, id)) = self.notify.remove(\u0026amp;index) { self.send(to, Event::ClientResponse { id, response: result.map(Response::Mutate) })?; } Ok(()) } /// Executes any queries that are ready. fn query_execute(\u0026amp;mut self, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { for query in self.query_ready(state.get_applied_index()) { debug!(\u0026#34;Executing query {:?}\u0026#34;, query.command); let result = state.query(query.command); if let Err(error @ Error::Internal(_)) = result { return Err(error); } self.send( query.address, Event::ClientResponse { id: query.id, response: result.map(Response::Query) }, )? } Ok(()) } 当Leader接收到了Event::ClientResponse之后，不会做额外处理，设置接收端的地址为Client，直接发送塞入到信箱(node_tx)，交给上层Server去发送，这一部分的逻辑定义在src/raft/leader.rs中：\n1 2 3 4 5 6 Event::ClientResponse { id, mut response } =\u0026gt; { if let Ok(Response::Status(ref mut status)) = response { status.server = self.id; } self.send(Address::Client, Event::ClientResponse { id, response })?; } Message塞入到node_tx中，自然就会在Server中被node_rx接收到，根据发送端到地址匹配处理，如果是Address::Client,那么就获取并删除当时存入的那个response_tx，通过response_tx将执行结果发送给Raft Client。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // 获取下层Raft放入信箱的Message，发送给对应的节点 Some(msg) = node_rx.next() =\u0026gt; { match msg { Message{to: Address::Node(_), ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Broadcast, ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Client, event: Event::ClientResponse{ id, response }, ..} =\u0026gt; { if let Some(response_tx) = requests.remove(\u0026amp;id) { response_tx .send(response) .map_err(|e| Error::Internal(format!(\u0026#34;Failed to send response {:?}\u0026#34;, e)))?; } } _ =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Unexpected message {:?}\u0026#34;, msg))), } } 至此，Raft Client-Server完成了一次完整的通信过程。\nRaft Node 这一部分是Raft模块中各个节点之间的通信，对应6.824当中的直接调用AppendEntries,RequsetVoteRPC的过程，只不过在toydb中使用Message处理信息交换，因此就是发送Message。\nRaft节点如果需要发送一条Message，那么就调用send，将其塞入到node_tx当中，交给Server去发送，比如进行heartbeat时，就传入一个接收地址为Address::Broadcast，消息类型为Event::Heartbeat的Message，表示要将心跳信息进行广播\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 /// Broadcasts a heartbeat to all peers. pub(super) fn heartbeat(\u0026amp;mut self) -\u0026gt; Result\u0026lt;()\u0026gt; { let (commit_index, commit_term) = self.log.get_commit_index(); self.send(Address::Broadcast, Event::Heartbeat { commit_index, commit_term })?; // NB: We don\u0026#39;t reset self.since_heartbeat here, because we want to send // periodic heartbeats regardless of any on-demand heartbeats. Ok(()) } /// Sends an event fn send(\u0026amp;self, to: Address, event: Event) -\u0026gt; Result\u0026lt;()\u0026gt; { let msg = Message { term: self.term, from: Address::Node(self.id), to, event }; debug!(\u0026#34;Sending {:?}\u0026#34;, msg); Ok(self.node_tx.send(msg)?) } 这一部分的发送逻辑和上面有些重合，同样是在eventloop中从node_rx获取Message，再把Message传入到tcp_tx中，准备进行发送。\n和eventloop一同启动的还有一个tcp_sender的task,在其中，又将Message经历了一次在channel中的发送和接收，最终调用到tcp_send_peer(),将Message通过tcp发送给对应的Raft Node\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 async fn tcp_send( peers: HashMap\u0026lt;NodeID, String\u0026gt;, out_rx: mpsc::UnboundedReceiver\u0026lt;Message\u0026gt;, ) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut out_rx = UnboundedReceiverStream::new(out_rx); let mut peer_txs: HashMap\u0026lt;NodeID, mpsc::Sender\u0026lt;Message\u0026gt;\u0026gt; = HashMap::new(); for (id, addr) in peers.into_iter() { let (tx, rx) = mpsc::channel::\u0026lt;Message\u0026gt;(1000); peer_txs.insert(id, tx); tokio::spawn(Self::tcp_send_peer(addr, rx)); } while let Some(message) = out_rx.next().await { let to = match message.to { Address::Broadcast =\u0026gt; peer_txs.keys().copied().collect(), Address::Node(peer) =\u0026gt; vec![peer], addr =\u0026gt; { error!(\u0026#34;Received outbound message for non-TCP address {:?}\u0026#34;, addr); continue; } }; for id in to { match peer_txs.get_mut(\u0026amp;id) { Some(tx) =\u0026gt; match tx.try_send(message.clone()) { Ok(()) =\u0026gt; {} Err(mpsc::error::TrySendError::Full(_)) =\u0026gt; { debug!(\u0026#34;Full send buffer for peer {}, discarding message\u0026#34;, id) } Err(error) =\u0026gt; return Err(error.into()), }, None =\u0026gt; error!(\u0026#34;Received outbound message for unknown peer {}\u0026#34;, id), } } } Ok(()) } /// Sends outbound messages to a peer, continuously reconnecting. async fn tcp_send_peer(addr: String, out_rx: mpsc::Receiver\u0026lt;Message\u0026gt;) { let mut out_rx = ReceiverStream::new(out_rx); loop { match TcpStream::connect(\u0026amp;addr).await { Ok(socket) =\u0026gt; { debug!(\u0026#34;Connected to Raft peer {}\u0026#34;, addr); match Self::tcp_send_peer_session(socket, \u0026amp;mut out_rx).await { Ok(()) =\u0026gt; break, Err(err) =\u0026gt; error!(\u0026#34;Failed sending to Raft peer {}: {}\u0026#34;, addr, err), } } Err(err) =\u0026gt; error!(\u0026#34;Failed connecting to Raft peer {}: {}\u0026#34;, addr, err), } tokio::time::sleep(Duration::from_millis(1000)).await; } debug!(\u0026#34;Disconnected from Raft peer {}\u0026#34;, addr); } Raft Node \u0026amp; State Machine Raft节点与状态机之间是通过state_rx，state_tx，node_tx,node_rx进行交互的:\nRaft节点通过state_tx中向状态机发送Instruction，从node_rx中接收状态机发送的Message Raft状态机从state_rx中接收Raft节点发送的Instruction，通过node_tx向Raft节点发送Message，状态机。通过node_tx发送的消息最后会和其他的Message一样，走一遍tcp的流程，最后又传到的当前的节点，然后调用step()交给Leader Raft节点发送与接收：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 // Raft节点发送：commit时通过state_tx向状态机发送Instruction fn maybe_commit(\u0026amp;mut self) -\u0026gt; Result\u0026lt;Index\u0026gt; { // ... // ... if commit_index \u0026gt; prev_commit_index { self.log.commit(commit_index)?; // TODO: Move application elsewhere, but needs access to applied index. let mut scan = self.log.scan((prev_commit_index + 1)..=commit_index)?; while let Some(entry) = scan.next().transpose()? { self.state_tx.send(Instruction::Apply { entry })?; } } } // Raft节点接收：接收状态机发送的Message，走一遍tcp发送回当前节点，调用step去处理 Some(msg) = node_rx.next() =\u0026gt; { match msg { Message{to: Address::Node(_), ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Broadcast, ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Client, event: Event::ClientResponse{ id, response }, ..} =\u0026gt; { if let Some(response_tx) = requests.remove(\u0026amp;id) { response_tx .send(response) .map_err(|e| Error::Internal(format!(\u0026#34;Failed to send response {:?}\u0026#34;, e)))?; } } _ =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Unexpected message {:?}\u0026#34;, msg))), } } 状态机发送与接收：\n状态机是通过Driver.drive()进行驱动的，会不断地从state_rx中获取Raft节点发送而来的Instruction，然后进行执行,发送逻辑在之前介绍过了，传入到node_tx中即可：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // 状态机接收： /// Drives a state machine. pub async fn drive(mut self, mut state: Box\u0026lt;dyn State\u0026gt;) -\u0026gt; Result\u0026lt;()\u0026gt; { debug!(\u0026#34;Starting state machine driver at applied index {}\u0026#34;, state.get_applied_index()); while let Some(instruction) = self.state_rx.next().await { if let Err(error) = self.execute(instruction, \u0026amp;mut *state) { error!(\u0026#34;Halting state machine due to error: {}\u0026#34;, error); return Err(error); } } debug!(\u0026#34;Stopping state machine driver\u0026#34;); Ok(()) } // 状态机发送： fn send(\u0026amp;self, to: Address, event: Event) -\u0026gt; Result\u0026lt;()\u0026gt; { // TODO: This needs to use the correct term. let msg = Message { from: Address::Node(self.node_id), to, term: 0, event }; debug!(\u0026#34;Sending {:?}\u0026#34;, msg); Ok(self.node_tx.send(msg)?) } 小结 在这一部分，主要完成了一个组装的过程，将MVCC存储引擎与Raft结合，为SQL的执行提供支持。在这种模式下，在算子中调用提供的Transaction接口，就会先通过Raft Client将请求发送给Raft Server，等待Apply之后再调用MVCC存储引擎去执行，最后依次响应，返回到算子当中。结束一次执行。\n除此之外，补全了Raft状态机的实现和节点间的通信这一部分涉及到多个channel和tcp通信，同一节点上的命令可以使用channel发送，不同节点之间使用tcp发送，虽然没有直接调用rpc直观，但是结构上更加清晰，易于管理。\n","permalink":"http://itfischer.space/en/posts/tech/toydb/05-sql-engine/","summary":"在前面的几章，分别分析了Bitcask，构建于Bitcask之上的MVCC，Raft，以及Raft状态机。在本章中，笔者会将这几个模块组合起","title":"05-SQL Engine"},{"content":"本系列的第一章00-Architecture以SQL执行流程为整个系列做了一个引子，目前本系列通过六篇文章，已经将toydb的各个模块都分析完毕，在本章中，不妨在了解了各个模块的基础上，补全SQL执行的全流程，为整个系列收尾。\n笔者会先补全Schema部分，介绍一下在toydb中如何构建起关系模型，之后是SQL执行的全流程，大致可以分为两部分：\n一是在SQL层中，完成Parse -\u0026gt; Optimize -\u0026gt; Execute的过程 二是在SQL Engine中，完成分布式 + 存储的过程 由于调用链比较长，并且每一步都会处理多种类型，从而代码量非常庞大，如果一味全部复制粘贴的话会导致逻辑非常不连贯，观感非常不好，因此在这一章当中，对于近似类型的处理，笔者只会保留其中的几个，如果想了解全部实现的话，还请读者自行拉取源码阅读。\nSchema 数据类型 toydb中提供了四种数据类型的支持，分别是Boolean、Integer、Float、String，定义在src/sql/types/mod.rs中：\n1 2 3 4 5 6 7 8 /// A datatype #[derive(Clone, Debug, Hash, PartialEq, Serialize, Deserialize)] pub enum DataType { Boolean, Integer, Float, String, } Value使用一个enum来实现，表示不同类型的值，除了上面的四种数据类型，还允许值为Null，然后提供了一些hash和数据类型转换的支持：\n1 2 3 4 5 6 7 8 9 /// A specific value of a data type #[derive(Clone, Debug, PartialEq, Serialize, Deserialize)] pub enum Value { Null, Boolean(bool), Integer(i64), Float(f64), String(String), } toydb的Schema非常简单，只有两级，分别是table和column，定义在src/schema.rs当中，而Row只是一个Vec\u0026lt;Value\u0026gt;,没有封装其他逻辑。\nTable 由于底层toydb采用的是kv存储，不需要在Table当中定义存储的行为，Table的结构和实现 都非常简洁，结构体中只有两个字段，分别用于表示表名和包含的字段，然后定义了一个迭代器，表示多个Table，用于扫描当前所有的Table：\n1 2 3 4 5 6 7 8 9 /// A table scan iterator pub type Tables = Box\u0026lt;dyn DoubleEndedIterator\u0026lt;Item = Table\u0026gt; + Send\u0026gt;; /// A table schema #[derive(Clone, Debug, PartialEq, Deserialize, Serialize)] pub struct Table { pub name: String, pub columns: Vec\u0026lt;Column\u0026gt;, } 方法 功能 pub fn get_column(\u0026amp;self, name: \u0026amp;str) 根据名称获取列 pub fn get_column_index(\u0026amp;self, name: \u0026amp;str) 根据名称获取列索引 pub fn get_primary_key(\u0026amp;self) 获取主键的列 pub fn get_row_key(\u0026amp;self, row: \u0026amp;[Value]) 获取一行中主键的值 pub fn validate(\u0026amp;self, txn: \u0026amp;mut dyn Transaction) 验证表结构 pub fn validate_row(\u0026amp;self, row: \u0026amp;[Value], txn: \u0026amp;mut dyn Transaction) 验证行结构 各个方法的实现逻辑都很简单：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 impl Table { /// Creates a new table schema pub fn new(name: String, columns: Vec\u0026lt;Column\u0026gt;) -\u0026gt; Result\u0026lt;Self\u0026gt; { let table = Self { name, columns }; Ok(table) } /// Fetches a column by name pub fn get_column(\u0026amp;self, name: \u0026amp;str) -\u0026gt; Result\u0026lt;\u0026amp;Column\u0026gt; { self.columns.iter().find(|c| c.name == name).ok_or_else(|| { Error::Value(format!(\u0026#34;Column {} not found in table {}\u0026#34;, name, self.name)) }) } /// Fetches a column index by name pub fn get_column_index(\u0026amp;self, name: \u0026amp;str) -\u0026gt; Result\u0026lt;usize\u0026gt; { self.columns.iter().position(|c| c.name == name).ok_or_else(|| { Error::Value(format!(\u0026#34;Column {} not found in table {}\u0026#34;, name, self.name)) }) } /// Returns the primary key column of the table pub fn get_primary_key(\u0026amp;self) -\u0026gt; Result\u0026lt;\u0026amp;Column\u0026gt; { self.columns .iter() .find(|c| c.primary_key) .ok_or_else(|| Error::Value(format!(\u0026#34;Primary key not found in table {}\u0026#34;, self.name))) } /// Returns the primary key value of a row pub fn get_row_key(\u0026amp;self, row: \u0026amp;[Value]) -\u0026gt; Result\u0026lt;Value\u0026gt; { row.get( self.columns .iter() .position(|c| c.primary_key) .ok_or_else(|| Error::Value(\u0026#34;Primary key not found\u0026#34;.into()))?, ) .cloned() .ok_or_else(|| Error::Value(\u0026#34;Primary key value not found for row\u0026#34;.into())) } /// Validates the table schema pub fn validate(\u0026amp;self, txn: \u0026amp;mut dyn Transaction) -\u0026gt; Result\u0026lt;()\u0026gt; { if self.columns.is_empty() { return Err(Error::Value(format!(\u0026#34;Table {} has no columns\u0026#34;, self.name))); } match self.columns.iter().filter(|c| c.primary_key).count() { 1 =\u0026gt; {} 0 =\u0026gt; return Err(Error::Value(format!(\u0026#34;No primary key in table {}\u0026#34;, self.name))), _ =\u0026gt; return Err(Error::Value(format!(\u0026#34;Multiple primary keys in table {}\u0026#34;, self.name))), }; for column in \u0026amp;self.columns { column.validate(self, txn)?; } Ok(()) } /// Validates a row pub fn validate_row(\u0026amp;self, row: \u0026amp;[Value], txn: \u0026amp;mut dyn Transaction) -\u0026gt; Result\u0026lt;()\u0026gt; { if row.len() != self.columns.len() { return Err(Error::Value(format!(\u0026#34;Invalid row size for table {}\u0026#34;, self.name))); } let pk = self.get_row_key(row)?; for (column, value) in self.columns.iter().zip(row.iter()) { column.validate_value(self, \u0026amp;pk, value, txn)?; } Ok(()) } } Column Column同样很简洁，定义如下，均以给出注释：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 /// A table column schema #[derive(Clone, Debug, PartialEq, Deserialize, Serialize)] pub struct Column { /// Column name pub name: String, /// Column datatype pub datatype: DataType, /// Whether the column is a primary key pub primary_key: bool, /// Whether the column allows null values pub nullable: bool, /// The default value of the column pub default: Option\u0026lt;Value\u0026gt;, /// Whether the column should only take unique values pub unique: bool, /// The table which is referenced by this foreign key pub references: Option\u0026lt;String\u0026gt;, /// Whether the column should be indexed pub index: bool, } 在column当中，定义了validate和validate_column分别验证列和列当中的值是否合法： 在validate中做了如下检查：\n主键不能设置为null 主键必须是唯一的，其中的值不允许重复，因为要用于生成key来存储 验证默认值：默认值的类型必须与column的类型相同；不允许将null设置为非null column的默认值；允许为null的column必须设置默认值。 检查外键引用 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 /// Validates the column schema pub fn validate(\u0026amp;self, table: \u0026amp;Table, txn: \u0026amp;mut dyn Transaction) -\u0026gt; Result\u0026lt;()\u0026gt; { // Validate primary key if self.primary_key \u0026amp;\u0026amp; self.nullable { return Err(Error::Value(format!(\u0026#34;Primary key {} cannot be nullable\u0026#34;, self.name))); } if self.primary_key \u0026amp;\u0026amp; !self.unique { return Err(Error::Value(format!(\u0026#34;Primary key {} must be unique\u0026#34;, self.name))); } // Validate default value if let Some(default) = \u0026amp;self.default { if let Some(datatype) = default.datatype() { if datatype != self.datatype { return Err(Error::Value(format!( \u0026#34;Default value for column {} has datatype {}, must be {}\u0026#34;, self.name, datatype, self.datatype ))); } } else if !self.nullable { return Err(Error::Value(format!( \u0026#34;Can\u0026#39;t use NULL as default value for non-nullable column {}\u0026#34;, self.name ))); } } else if self.nullable { return Err(Error::Value(format!( \u0026#34;Nullable column {} must have a default value\u0026#34;, self.name ))); } // Validate references if let Some(reference) = \u0026amp;self.references { let target = if reference == \u0026amp;table.name { table.clone() } else if let Some(table) = txn.read_table(reference)? { table } else { return Err(Error::Value(format!( \u0026#34;Table {} referenced by column {} does not exist\u0026#34;, reference, self.name ))); }; if self.datatype != target.get_primary_key()?.datatype { return Err(Error::Value(format!( \u0026#34;Can\u0026#39;t reference {} primary key of table {} from {} column {}\u0026#34;, target.get_primary_key()?.datatype, target.name, self.datatype, self.name ))); } } Ok(()) } 在validate_value中做了如下检查：\n检验数据类型是否与column的数据类型匹配 String类型不能过长，最大为1024个字节 检查外键 检查unique，处理方法是全表遍历检测是否有相同的值 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 /// Validates a column value pub fn validate_value( \u0026amp;self, table: \u0026amp;Table, pk: \u0026amp;Value, value: \u0026amp;Value, txn: \u0026amp;mut dyn Transaction, ) -\u0026gt; Result\u0026lt;()\u0026gt; { // Validate datatype match value.datatype() { None if self.nullable =\u0026gt; Ok(()), None =\u0026gt; Err(Error::Value(format!(\u0026#34;NULL value not allowed for column {}\u0026#34;, self.name))), Some(ref datatype) if datatype != \u0026amp;self.datatype =\u0026gt; Err(Error::Value(format!( \u0026#34;Invalid datatype {} for {} column {}\u0026#34;, datatype, self.datatype, self.name ))), _ =\u0026gt; Ok(()), }?; // Validate value match value { Value::String(s) if s.len() \u0026gt; 1024 =\u0026gt; { Err(Error::Value(\u0026#34;Strings cannot be more than 1024 bytes\u0026#34;.into())) } _ =\u0026gt; Ok(()), }?; // Validate outgoing references if let Some(target) = \u0026amp;self.references { match value { Value::Null =\u0026gt; Ok(()), Value::Float(f) if f.is_nan() =\u0026gt; Ok(()), v if target == \u0026amp;table.name \u0026amp;\u0026amp; v == pk =\u0026gt; Ok(()), v if txn.read(target, v)?.is_none() =\u0026gt; Err(Error::Value(format!( \u0026#34;Referenced primary key {} in table {} does not exist\u0026#34;, v, target, ))), _ =\u0026gt; Ok(()), }?; } // Validate uniqueness constraints if self.unique \u0026amp;\u0026amp; !self.primary_key \u0026amp;\u0026amp; value != \u0026amp;Value::Null { let index = table.get_column_index(\u0026amp;self.name)?; let mut scan = txn.scan(\u0026amp;table.name, None)?; while let Some(row) = scan.next().transpose()? { if row.get(index).unwrap_or(\u0026amp;Value::Null) == value \u0026amp;\u0026amp; \u0026amp;table.get_row_key(\u0026amp;row)? != pk { return Err(Error::Value(format!( \u0026#34;Unique value {} already exists for column {}\u0026#34;, value, self.name ))); } } } Ok(()) } 初始化 在main函数当中，会调用serve启动服务：\n在serve当中会首先创建一个sql_engine，这里可以看到，sql_engine对应的是sql::engine::Raft,这也是SQL Engine的入口，executor会首先将命令发送给Raft 调用raft.serve()启动Raft，创建用于节点之间交互的tcp sender和receiver，创建一个eventloop处理Message，eventloop如何处理Message，在上一章中已经介绍过。 调用serve_sql()启动server，处理client发送而来的请求。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 /// Serves Raft and SQL requests until the returned future is dropped. Consumes the server. pub async fn serve(self) -\u0026gt; Result\u0026lt;()\u0026gt; { let sql_listener = self .sql_listener .ok_or_else(|| Error::Internal(\u0026#34;Must listen before serving\u0026#34;.into()))?; let raft_listener = self .raft_listener .ok_or_else(|| Error::Internal(\u0026#34;Must listen before serving\u0026#34;.into()))?; let (raft_tx, raft_rx) = mpsc::unbounded_channel(); let sql_engine = sql::engine::Raft::new(raft_tx); // 分别启动Raft server和Sql server tokio::try_join!( self.raft.serve(raft_listener, raft_rx), Self::serve_sql(sql_listener, sql_engine), )?; Ok(()) } pub async fn serve( self, listener: TcpListener, client_rx: mpsc::UnboundedReceiver\u0026lt;(Request, oneshot::Sender\u0026lt;Result\u0026lt;Response\u0026gt;\u0026gt;)\u0026gt;, ) -\u0026gt; Result\u0026lt;()\u0026gt; { let (tcp_in_tx, tcp_in_rx) = mpsc::unbounded_channel::\u0026lt;Message\u0026gt;(); let (tcp_out_tx, tcp_out_rx) = mpsc::unbounded_channel::\u0026lt;Message\u0026gt;(); let (task, tcp_receiver) = Self::tcp_receive(listener, tcp_in_tx).remote_handle(); tokio::spawn(task); let (task, tcp_sender) = Self::tcp_send(self.peers, tcp_out_rx).remote_handle(); tokio::spawn(task); let (task, eventloop) = Self::eventloop(self.node, self.node_rx, client_rx, tcp_in_rx, tcp_out_tx) .remote_handle(); tokio::spawn(task); tokio::try_join!(tcp_receiver, tcp_sender, eventloop)?; Ok(()) } /// Serves SQL clients. async fn serve_sql(listener: TcpListener, engine: sql::engine::Raft) -\u0026gt; Result\u0026lt;()\u0026gt; { let mut listener = TcpListenerStream::new(listener); while let Some(socket) = listener.try_next().await? { // 处理单个请求 let peer = socket.peer_addr()?; let session = Session::new(engine.clone())?; tokio::spawn(async move { info!(\u0026#34;Client {} connected\u0026#34;, peer); match session.handle(socket).await { Ok(()) =\u0026gt; info!(\u0026#34;Client {} disconnected\u0026#34;, peer), Err(err) =\u0026gt; error!(\u0026#34;Client {} error: {}\u0026#34;, peer, err), } }); } Ok(()) } 执行流程 SQL执行 SQL执行的入口在serve_sql()当中，从listener当中获取到单个请求之后，就会创建一个Session，然后把传下来的raft engine交给Session，一个Session用于负责一条SQL，在处理完网络连接之后，会调用到Session.execute()来从解析SQL的过程开始，执行一条SQL。\n在execute中，会先使用Parser进行sql解析，然后根据生成的AST Statement去生成一个执行计划，然后再通过Optimizer进行优化，得到一个最终的执行计划，最后根据执行计划构建一个Executor，调用Executor的execute开始执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 /// Executes a query, managing transaction status for the session pub fn execute(\u0026amp;mut self, query: \u0026amp;str) -\u0026gt; Result\u0026lt;ResultSet\u0026gt; { // FIXME We should match on self.txn as well, but get this error: // error[E0009]: cannot bind by-move and by-ref in the same pattern // ...which seems like an arbitrary compiler limitation match Parser::new(query).parse()? { ast::Statement::Begin { .. } if self.txn.is_some() =\u0026gt; { Err(Error::Value(\u0026#34;Already in a transaction\u0026#34;.into())) } // ... // ... statement =\u0026gt; { let mut txn = self.engine.begin()?; match Plan::build(statement, \u0026amp;mut txn)?.optimize(\u0026amp;mut txn)?.execute(\u0026amp;mut txn) { Ok(result) =\u0026gt; { txn.commit()?; Ok(result) } Err(error) =\u0026gt; { txn.rollback()?; Err(error) } } } } } Parser 在toydb当中，通过Parser会生成一个Statement，对于parser如何解析sql的过程，由于笔者没有系统学过编译原理，也几乎不懂编译原理，这里就不分析了，感兴趣的读者可自行阅读，这一部分定义在src/parser下。\n在Statement当中，包含了SQL执行所需要的所有信息，使用enum进行表示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 pub enum Statement { Begin { read_only: bool, as_of: Option\u0026lt;u64\u0026gt;, }, Commit, Rollback, Select { select: Vec\u0026lt;(Expression, Option\u0026lt;String\u0026gt;)\u0026gt;, from: Vec\u0026lt;FromItem\u0026gt;, r#where: Option\u0026lt;Expression\u0026gt;, group_by: Vec\u0026lt;Expression\u0026gt;, having: Option\u0026lt;Expression\u0026gt;, order: Vec\u0026lt;(Expression, Order)\u0026gt;, offset: Option\u0026lt;Expression\u0026gt;, limit: Option\u0026lt;Expression\u0026gt;, }, } Plan 在拿到了Statement之后，就可以根据Statement去生成一个基础的执行计划了，toydb中没有区分逻辑计划和物理计划，只定义了plan和对应的plan node。Plan node是使用enum进行定义的，结构上与Executor是一一对应的。\nPlan作为Plan Node的Wrapper，为其封装了一些方法：\nbuild:根据Statement的类型，构建一个Plan Node optimize:对plan进行优化生成一个最终的执行计划， execute:执行一个plan，会先根据自身的Plan Node去创建一个Executor，然后会调用到Executor的execute 在optimize当中，共涉及到了五种类型的优化：\nConstantFolder:提前计算一些常量表达式，如 where a \u0026gt; 5 - 3优化为where a \u0026gt; 2。 FilterPushdown:谓词下推，为了让底层算子尽可能多的过滤数据，从而减少上层的计算量 IndexLookup:如果当前Table上要查询的column存在索引，就将TableScan转换为IndexScan NoopCleaner:Noop意味No Operation，在这一部分清除掉一些没有意义的Node和Filter，例如where 1 = 1这类恒为true的 JoinType:优化Join类型，目前会将NestedLoopJoin优化为HashJoin。 build和execute都会返回一个Self，从而进行链式调用，按照build -\u0026gt; optimize -\u0026gt; execute的流程来执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 /// A plan node #[derive(Debug, PartialEq, Serialize, Deserialize)] pub enum Node { Aggregation { source: Box\u0026lt;Node\u0026gt;, aggregates: Vec\u0026lt;Aggregate\u0026gt;, }, HashJoin { left: Box\u0026lt;Node\u0026gt;, left_field: (usize, Option\u0026lt;(Option\u0026lt;String\u0026gt;, String)\u0026gt;), right: Box\u0026lt;Node\u0026gt;, right_field: (usize, Option\u0026lt;(Option\u0026lt;String\u0026gt;, String)\u0026gt;), outer: bool, }, // ... // ... NestedLoopJoin { left: Box\u0026lt;Node\u0026gt;, left_size: usize, right: Box\u0026lt;Node\u0026gt;, predicate: Option\u0026lt;Expression\u0026gt;, outer: bool, Update { table: String, source: Box\u0026lt;Node\u0026gt;, expressions: Vec\u0026lt;(usize, Option\u0026lt;String\u0026gt;, Expression)\u0026gt;, }, } pub struct Plan(pub Node); impl Plan { /// Builds a plan from an AST statement. pub fn build\u0026lt;C: Catalog\u0026gt;(statement: ast::Statement, catalog: \u0026amp;mut C) -\u0026gt; Result\u0026lt;Self\u0026gt; { Planner::new(catalog).build(statement) } /// Executes the plan, consuming it. pub fn execute\u0026lt;T: Transaction + \u0026#39;static\u0026gt;(self, txn: \u0026amp;mut T) -\u0026gt; Result\u0026lt;ResultSet\u0026gt; { \u0026lt;dyn Executor\u0026lt;T\u0026gt;\u0026gt;::build(self.0).execute(txn) } /// Optimizes the plan, consuming it. pub fn optimize\u0026lt;C: Catalog\u0026gt;(self, catalog: \u0026amp;mut C) -\u0026gt; Result\u0026lt;Self\u0026gt; { let mut root = self.0; root = optimizer::ConstantFolder.optimize(root)?; root = optimizer::FilterPushdown.optimize(root)?; root = optimizer::IndexLookup::new(catalog).optimize(root)?; root = optimizer::NoopCleaner.optimize(root)?; root = optimizer::JoinType.optimize(root)?; Ok(Plan(root)) } } Executor 在执行这一部分，toydb定义了一个trait，其中只有execute一个方法，只要实现了这个trait，就可以作为Executor被调用。\n1 2 3 4 pub trait Executor\u0026lt;T: Transaction\u0026gt; { /// Executes the executor, consuming it and returning a result set fn execute(self: Box\u0026lt;Self\u0026gt;, txn: \u0026amp;mut T) -\u0026gt; Result\u0026lt;ResultSet\u0026gt;; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 pub struct Insert { table: String, columns: Vec\u0026lt;String\u0026gt;, rows: Vec\u0026lt;Vec\u0026lt;Expression\u0026gt;\u0026gt;, } impl\u0026lt;T: Transaction\u0026gt; Executor\u0026lt;T\u0026gt; for Insert { fn execute(self: Box\u0026lt;Self\u0026gt;, txn: \u0026amp;mut T) -\u0026gt; Result\u0026lt;ResultSet\u0026gt; { let table = txn.must_read_table(\u0026amp;self.table)?; let mut count = 0; for expressions in self.rows { let mut row = expressions.into_iter().map(|expr| expr.evaluate(None)).collect::\u0026lt;Result\u0026lt;_\u0026gt;\u0026gt;()?; if self.columns.is_empty() { row = Self::pad_row(\u0026amp;table, row)?; } else { row = Self::make_row(\u0026amp;table, \u0026amp;self.columns, row)?; } txn.create(\u0026amp;table.name, row)?; count += 1; } Ok(ResultSet::Create { count }) } } 以Insert为例，首先定义一个Insert结构体，然后为Insert实现Executor trait。后续就是利用SQL Engine来先共识后存储了。\nSQL Engine Read 这里继续以Insert为例，走完SQL Engine的整个流程。Insert中需要先读取到对应的Table，之后再添加多条Row，分为一次读请求和多次写请求，刚好可以用来分析读写请求的执行流程。 Insert时需要首先读取到对应的Table，这里就会调用SQL Engine的txn.must_read_table()，在该函数内，最终会调用到sql::engine::raft::Transaction的read_table()，在这里，会通过Raft Client向Raft Server去发送一条类型为Query::ReadTable的请求。\nself.client.query()，会调用到execute()。在execute()当中，会调用oneshot::channel()创建一个一次性的channel，将channel的消息发送端response_tx和request一同发送给Raft Server。\n之后，Client就会监听这个channel，获取Server端发送回来了执行结果,Server接收并执行完之后，会使用传过去的发送端再将响应发送回来，Client等待从接收端获取消息，拿到Raft Server对request的执行结果向上返回。此时，创建的oneshot channel就可以销毁了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // (1) fn read_table(\u0026amp;self, table: \u0026amp;str) -\u0026gt; Result\u0026lt;Option\u0026lt;Table\u0026gt;\u0026gt; { self.client.query(Query::ReadTable { txn: self.state.clone(), table: table.to_string() }) } // (2) /// Queries the Raft state machine, deserializing the response into the /// return type. fn query\u0026lt;V: DeserializeOwned\u0026gt;(\u0026amp;self, query: Query) -\u0026gt; Result\u0026lt;V\u0026gt; { match self.execute(raft::Request::Query(bincode::serialize(\u0026amp;query)?))? { raft::Response::Query(response) =\u0026gt; Ok(bincode::deserialize(\u0026amp;response)?), resp =\u0026gt; Err(Error::Internal(format!(\u0026#34;Unexpected Raft query response {:?}\u0026#34;, resp))), } } // (3) fn execute(\u0026amp;self, request: raft::Request) -\u0026gt; Result\u0026lt;raft::Response\u0026gt; { let (response_tx, response_rx) = oneshot::channel(); self.tx.send((request, response_tx))?; futures::executor::block_on(response_rx)? } Server端的接收逻辑就在server的eventloop当中，从client_rx(上面传来的raft_rx)接收到Client发送的request和response_tx。\n请求需要先通过Raft完成共识，通过step()将Message交给Leader(通常是Leader，如果不是Leader也会转发给Leader，后面全部假设请求发送给了Leader)，之后Apply了才能够执行，这里先使用一个HashMap保存请求，等到Apply并执行完之后再从HashMap中获取出暂存的response_rx，响应Client。\n1 2 3 4 5 6 7 8 9 10 11 12 13 // 获取client发送的消息，交给Raft模块去处理，这里的client并不是用户的client，而是要执行命令的sql端 // 暂存命令，等到日志Apply并执行完之后再响应客户端 Some((request, response_tx)) = client_rx.next() =\u0026gt; { let id = Uuid::new_v4().as_bytes().to_vec(); let msg = Message{ from: Address::Client, to: Address::Node(node.id()), term: 0, event: Event::ClientRequest{id: id.clone(), request}, }; node = node.step(msg)?; requests.insert(id, response_tx); } Leader\n在toydb中，只读请求会通过ReadIndex来优化，因此不会创建日志。\n调用了step()之后会再调用到Leader.step()，在这其中会完成对Message::ClientRequest的处理，这里的request类型为request::Query，因此Leader就会先向状态机发送一条Instruction::Query，让状态机先记录这一次只读请求，之后再发送一条Instruction::Vote，表示Leader对此次的只读请求投票，让状态机记录。\n同时，Leader会发送一次heartbeat，请求其他节点也对其投票。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Event::ClientRequest { id, request: Request::Query(command) } =\u0026gt; { let (commit_index, _) = self.log.get_commit_index(); self.state_tx.send(Instruction::Query { id, address: msg.from, command, term: self.term, index: commit_index, quorum: self.quorum(), })?; self.state_tx.send(Instruction::Vote { term: self.term, index: commit_index, address: Address::Node(self.id), })?; self.heartbeat()?; } Follower\nFollower在接收到了Leader发送的Message::Heartbeat之后，首先会根据Leader的进度和自身拥有的日志来更新commit进度，回应Leader一条Message::ConfirmLeader，表示接收到了Leader的心跳信息。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 Event::Heartbeat { commit_index, commit_term } =\u0026gt; { // Check that the heartbeat is from our leader. let from = msg.from.unwrap(); match self.role.leader { Some(leader) =\u0026gt; assert_eq!(from, leader, \u0026#34;Multiple leaders in term\u0026#34;), None =\u0026gt; self = self.become_follower(Some(from), msg.term)?, } // Advance commit index and apply entries if possible. let has_committed = self.log.has(commit_index, commit_term)?; let (old_commit_index, _) = self.log.get_commit_index(); if has_committed \u0026amp;\u0026amp; commit_index \u0026gt; old_commit_index { self.log.commit(commit_index)?; let mut scan = self.log.scan((old_commit_index + 1)..=commit_index)?; while let Some(entry) = scan.next().transpose()? { self.state_tx.send(Instruction::Apply { entry })?; } } self.send(msg.from, Event::ConfirmLeader { commit_index, has_committed })?; } Leader\nLeader在接收到Follower响应的Message::ConfirmLeader之后，Leader再将Follower的commit进度封装成一条Instruction::Vote发送给状态机。此外，如果follower发现自身进度落后于Leader，此时Leader还会向Follower去发送Log，补齐进度。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // A follower received one of our heartbeats and confirms that we // are its leader. If it doesn\u0026#39;t have the commit index in its local // log, replicate the log to it. Event::ConfirmLeader { commit_index, has_committed } =\u0026gt; { let from = msg.from.unwrap(); // 与上层状态机进行交互 self.state_tx.send(Instruction::Vote { term: msg.term, index: commit_index, address: msg.from, })?; if !has_committed { self.send_log(from)?; } } 状态机\n状态机是通过Driver.drive()来驱动的，在其中不断处理由下层Raft模块的发送而来的Instruction执行，状态机这一部分是和上一部分同时执行的。\n对于上面发送而来的Instruction::Query，状态机会将其记录，而发送而来的Instrcution::Vote，会先调用self.query_vote()，然后调用self.query_execute()\n在self.query_vote()中，简单记录投票 在self.query_execute()中,会遍历还未执行的，并且index \u0026lt;= last_apply_index的所有请求，获取出当前的投票数，如果达到了quorum，就交给状态机的实现去执行，然后向Leader发送一条Message::ClientResponse，告知Leader状态机已经执行结束。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 /// Votes for queries up to and including a given commit index for a term by an address. fn query_vote(\u0026amp;mut self, term: Term, commit_index: Index, address: Address) { for (_, queries) in self.queries.range_mut(..=commit_index) { for (_, query) in queries.iter_mut() { if term \u0026gt;= query.term { query.votes.insert(address); } } } } /// Executes any queries that are ready. fn query_execute(\u0026amp;mut self, state: \u0026amp;mut dyn State) -\u0026gt; Result\u0026lt;()\u0026gt; { for query in self.query_ready(state.get_applied_index()) { debug!(\u0026#34;Executing query {:?}\u0026#34;, query.command); let result = state.query(query.command); if let Err(error @ Error::Internal(_)) = result { return Err(error); } self.send( query.address, Event::ClientResponse { id: query.id, response: result.map(Response::Query) }, )? } Ok(()) } 单机存储\n在state.query()中，就会找到状态机的实现，而状态机的实现是一个MVCC的Wrapper,在query当中，根据请求类型进行分类，恢复当前事务的状态，然后会调用到\nTransaction\u0026lt;storage::engine::Engine\u0026gt;.read_table(),在这里，终于脱离了分布式的部分，转为单机执行了。构建一个table.name的key去存储引擎中查询：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 // 状态机中的query fn query(\u0026amp;self, command: Vec\u0026lt;u8\u0026gt;) -\u0026gt; Result\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt; { match bincode::deserialize(\u0026amp;command)? { // ... Query::ReadTable { txn, table } =\u0026gt; { bincode::serialize(\u0026amp;self.engine.resume(txn)?.read_table(\u0026amp;table)?) } // ... } } // 单机存储引擎的读取操作 fn read_table(\u0026amp;self, table: \u0026amp;str) -\u0026gt; Result\u0026lt;Option\u0026lt;Table\u0026gt;\u0026gt; { self.txn.get(\u0026amp;Key::Table(table.into()).encode()?)?.map(|v| deserialize(\u0026amp;v)).transpose() } 再往下走就是MVCC了，MVCC又封装了storage trait(Bitcask或者Memory),MVCC调用了storage 的scan，对以user_key为前缀的key进行扫描，找到对当前事务能见的版本，从磁盘中读取到数据，完成了执行，剩下的就是返回结果了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // MVCC pub fn get(\u0026amp;self, key: \u0026amp;[u8]) -\u0026gt; Result\u0026lt;Option\u0026lt;Vec\u0026lt;u8\u0026gt;\u0026gt;\u0026gt; { let mut session = self.engine.lock()?; let from = Key::Version(key.into(), 0).encode()?; let to = Key::Version(key.into(), self.st.version).encode()?; let mut scan = session.scan(from..=to).rev(); while let Some((key, value)) = scan.next().transpose()? { match Key::decode(\u0026amp;key)? { Key::Version(_, version) =\u0026gt; { if self.st.is_visible(version) { return bincode::deserialize(\u0026amp;value); } } key =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Expected Key::Version got {:?}\u0026#34;, key))), }; } Ok(None) } Leader回应\n当单机存储执行完，并且状态机也向Leader发送了Message::ClientResponse之后，Leader就会将Message::ClientResponse通过接收请求时一同接受的oneshot channel的发送端，响应给Client，Client接收到响应，完成执行，继续Executor的执行。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 Event::ClientResponse { id, mut response } =\u0026gt; { if let Ok(Response::Status(ref mut status)) = response { status.server = self.id; } self.send(Address::Client, Event::ClientResponse { id, response })?; } Some(msg) = node_rx.next() =\u0026gt; { match msg { Message{to: Address::Node(_), ..} =\u0026gt; tcp_tx.send(msg)?, Message{to: Address::Broadcast, ..} =\u0026gt; tcp_tx.send(msg)?, // 响应Raft Client Message{to: Address::Client, event: Event::ClientResponse{ id, response }, ..} =\u0026gt; { if let Some(response_tx) = requests.remove(\u0026amp;id) { response_tx .send(response) .map_err(|e| Error::Internal(format!(\u0026#34;Failed to send response {:?}\u0026#34;, e)))?; } } _ =\u0026gt; return Err(Error::Internal(format!(\u0026#34;Unexpected message {:?}\u0026#34;, msg))), } } Write 再后面，会调用txn.create发送写请求，大致逻辑和流程都是一样的，只不过由于是写请求，需要通过日志来进行共识(读请求的ReadIndex优化可以不创建日志)，在Raft行为和与状态机上的交互会略有不同，但是整体上是一致的，这里只简单写一下不同的部分，读者可以按照这个流程，去自行分析。\nLeader\n在Leader.step()中，如果是Request::Mutate类型的请求，就会调用propose()来创建日志并向Follower去发送日志。并且向状态机中发送一条Instruction::Notify类型的信息，用于记录一次写请求的开始：\nFollower\nFollower接收到的为Message::AppendEntries，Follower会根据自身的情况，决定是拒绝还是将日志追加到自身，同时响应Leader。\nLeader\nLeader接收到Follower响应的Message::AcceptEntries时，就会计算是否有日志在大多数节点上达到了大多数，尝试进行commit与apply，如果达到了大多数就会更新commit_index，并且向状态机发送commit了的log entry。\n状态机\n状态机拿到了commit了的log entry，首先会通过状态机的实现(KV Engine)去执行该条日志，日志被执行会导致last_applied_index被更新，从而一些读请求也有可能达到执行的条件，因此调用self.query_execute()来执行读请求。\n最后移除请求记录，通知Leader已经执行完成。\nLeader\nLeader将由状态机发送而来的ClientResponse经过几次channel通信，转发给Client。回到Executor中继续执行。\n后记 至此，toydb的源码分析已经全部结束。起初开这个系列的主要原因是自己想尝试使用Rust作为毕业设计的技术栈，用来恶补一下自己的Rust。就目前而言感觉是可以用Rust写下来的，不至于中途逃跑到c++去。\ntoydb代码规模很小，并且结构很完善，可以作为一个很好的分布式数据库的学习项目，较为直观的展示了如何将SQL、分布式、存储引擎相结合，以及如何使用kv存储引擎来构建关系模型，这也是像Bustub，Miniob没有能够做到的(二者均是heap file)。\ntoydb作为一个学习项目，性能不是很好，后续如果有时间的话，可能考虑再补一期toydb的性能分析的文章，如果懒或者没时间的话就摸了。\n后续一段时间内估计应该是不会再更新这种源码阅读类型的文章了，大概会发一些看过的paper。如果后面毕设写的比较有意思的话，也会考虑把毕设给分享出来。\n最后，希望该系列文章能够对想学习Rust和入门分布式数据库的读者有所帮助，笔者水平有限，如果出现错误，也希望读者能够进行批评指正。\n","permalink":"http://itfischer.space/en/posts/tech/toydb/06-sql-execution/","summary":"本系列的第一章00-Architecture以SQL执行流程为整个系列做了一个引子，目前本系列通过六篇文章，已经将toydb的各个模块都分析","title":"06-SQL Execution"},{"content":"相比于传统c++当中的手动管理内存，rust采用了所有权 + RAII的机制，对于一些复杂的情况，rust还有生命周期等约束，从而保证在编译期能够解决大多数的内存安全问题。\n栈 \u0026amp;\u0026amp; 堆 如果要说内存管理的话，首先需要明确栈和堆的概念，这一点对于所有编程语言都适用，但是在rust当中尤为重要：\n基本类型和小的复合类型，像整数、浮点数、布尔值、字符以及小的数组、元组和结构体这样的基本类型通常是在栈上分配的。这些类型的大小在编译时是已知的，并且通常是固定的。此外引用同样是分配在栈的上，他指向另外一个数据(可能在栈上也可能在堆上) 分配在堆上的主要有两个类型：动态类型和智能指针，智能指针自身是分配在栈上的，但是其管理的数据分配在堆上。而动态类型如Vec、String等需要进行动态扩容，因此分配在堆上进行管理数据。 单一所有权 对于基础类型，rust将其分配在栈上，会随着栈桢的弹出而被回收，和其他语言相同，不需要担心栈上内存安全的问题。\n为了能够管理堆上的数据，保证内存安全，rust对于堆上的数据都定义了一个所有权。即堆上的数据在某一事件只能被一个变量所持有(基础情况)，数据会随着栈上变量被RAII回收而回收。从而保证了内存安全。\n在下面，堆上的String会随着变量s的回收而被回收\n1 2 3 fn test() { let s = String::from(\u0026#34;hello\u0026#34;); } 赋值 在cpp当中，如果将一个指针赋值给另外一个指针，两个指针会指向同一个地址，从而两个指针都能够操作这一段地址对应的数据。\n但是在Rust中，由于引入了所有权的概念，一段地址只能被一个引用所持有，在基础情况下，不允许两个引用指向堆上同一段地址，因此，如果是像cpp进行赋值操作，则会发生所有权的转移，即原本的引用失去对地址的所有权，将所有权转交给新的引用。\n1 2 3 4 fn test() { let s = String::from(\u0026#34;hello\u0026#34;); let s1 = s; // s1获取所有权，s失去所有权 } 又或者说，在rust当中，每个引用类型都对应了C++当中的unique_ptr，如果想让转移给一个新的指针，就需要进行\u0026quot;move\u0026quot;操作。\n而如果在某些情况下，只是需要该地址中的数据，那就可以进行深拷贝，只获取数据，根据数据在堆上重新分配一段地址创建变量，并不会影响原本的数据，同样也不会发生所有权的转移，实现深拷贝的话需要实现Clone Trait。\n1 2 3 4 fn test() { let s = String::from(\u0026#34;hello\u0026#34;); let s1 = s.clone(); } 引用与生命周期 引用 有些情况下，我们并不想去获取所有权，只是想临时借用一下这个变量。这种时候，rust就可以使用引用来处理。\n这里的引用其实和c++当中的基本类似，只不过，rust为了保证安全性，对于可变性有着更加严格的设定，在c++当中对于一个const类型的变量，是无法去获取一个非const的引用来修改其中的值的，这一点在rust当中也得到了保留：\n1 2 let x = 5; // 不可变变量 let y = \u0026amp;mut x; // 错误：不能获取不可变变量的可变引用 如果想修改的话，在c++当中就需要使用const_cast来进行转换，而rust中可以通过内部可变性来解决，不过这就是后话了。\n在c++ const的基础上，rust又多了一个新的限制，即在同一作用域当中，对同一数据不能同时拥有可变引用和不可变引用，这里主要是为了避免数据竞争的问题，需要注意的是，这里的数据竞争和多线程毫无关系，单线程同样存在数据竞争问题。\n通过这种冲突约束，能够保证当获取到一个不可变引用时，在该引用失效之前，能够保证数据确实是不变的。可变引用和可变引用之间的冲突同样如此，任何人都不希望自己在修改过程中，有其他引用来修改数据，达到和预期不一致的结果。\n这里有点像一个读写锁的设计，如果将作用域理解为一段时间，而两个引用视为在交替执行的线程，这样整个过程就可以视为并发过程中的数据竞争问题，\n1 2 3 4 5 6 7 8 fn test() { let mut data = 5; let r1 = \u0026amp;data; // 不可变引用 let r2 = \u0026amp;mut data; // 可变引用，这里会产生编译错误 println!(\u0026#34;{}\u0026#34;,r1); println!(\u0026#34;{}\u0026#34;,r2); } 在最新的rust编译器当中，r1对data的引用会持续到最后一次使用r1，而不是整个作用域的结尾，因此这样就可以通过合理的编排，将“并发”的过程转换为串行，从而解决数据竞争的问题，这类的数据竞争通常出现在，先对一个容器进行条件性检索，然后通过检索结果去更新容器。在搜索过程中会获取一个不可变引用，在修改时又会去获取一个可变引用，从而产生了冲突，正确的写法是在检索过程中保存结果，完成检索之后再去更新容器，而不是一边检索一边更新。\n上例中正确的写法如下：\n1 2 3 4 5 6 7 8 fn test() { let mut data = 5; let r1 = \u0026amp;data; // 不可变引用 println!(\u0026#34;{}\u0026#34;,r1); // r1生命周期结束，释放引用 let r2 = \u0026amp;mut data; // 可变引用，r1被释放从而r2可以正确引用 println!(\u0026#34;{}\u0026#34;,r2); } 生命周期 rust中生命周期标注，核心目的是解决垂悬引用的问题，通俗来讲，就是谁比谁活得长的一个问题，如果a比b活得长，但是a却引用了b，那么就存在一段时间，b已经被释放了，但是a依旧持有一个b的引用，去操作b，这时候就会出现内存安全的问题。\n最经典的违反生命周期约束的例子如下,r的生命周期长于x，此时引用x就会产生垂悬引用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { let r; { let x = 5; r = \u0026amp;x; } println!(\u0026#34;r: {}\u0026#34;, r); } error[E0597]: `x` does not live long enough --\u0026gt; src/main.rs:7:5 | 6 | r = \u0026amp;x; | - borrow occurs here 7 | } | ^ `x` dropped here while still borrowed ... 10 | } | - borrowed value needs to live until here 函数中的生命周期 和C++相同，rust同样不允许返回局部变量的引用，因为会被RAII在函数执行完之后释放掉，一定会产生垂悬引用问题。因此对于函数中的生命周期，只能来自输入。\n一聊到rust函数的生命周期，就会有这样一个经典的例子：\n1 2 3 4 5 6 7 fn longest(x: \u0026amp;str, y: \u0026amp;str) -\u0026gt; \u0026amp;str { if x.len() \u0026gt; y.len() { x } else { y } } 看起来写的毫无问题，获取两个引用，返回其中之一，但实际上会被rust编译器给无情的拒绝掉，而具体原因是无法确定x和y究竟谁能够活的更久的问题，如果像以下这样，就会出现问题，此时s2的作用域更小，提前释放，从而res产生了垂悬引用的问题，这种情况在不使用函数的情况下不应该发生，而在进行函数调用时同样不应该发生，因此如果不进行生命周期的标注，编译器就会严苛的拒绝掉，以避免风险，正确的标注方法如下：\n1 2 3 4 5 6 7 8 9 10 11 fn test() { let res; let s1 = String::from(\u0026#34;1\u0026#34;); { let s2 = String::from(\u0026#34;11111\u0026#34;); res = longeest(\u0026amp;s1,\u0026amp;s2); } } fn longest\u0026lt;\u0026#39;a\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;str) -\u0026gt; \u0026amp;\u0026#39;a str { } 结构体中的生命周期 相比于函数的生命周期，结构体中的生命周期可能更为常见，因为结构体当中的字段，并不是都是自己所有的，有些字段需要引用其他的变量，这种写法随处可见，比如说一个容器的迭代器，具体如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 type SkipListIter\u0026lt;\u0026#39;a\u0026gt; = Range\u0026lt;\u0026#39;a,InternalKey,(Bound\u0026lt;InternalKey\u0026gt;,Bound\u0026lt;InternalKey\u0026gt;),InternalKey,ByteVec\u0026gt;; pub struct MemTableIterator \u0026lt;\u0026#39;a\u0026gt; { skip_list: \u0026amp;\u0026#39;a SkipMap\u0026lt;InternalKey,ByteVec\u0026gt;, iter: SkipListIter\u0026lt;\u0026#39;a\u0026gt;, key: ByteVec, value: ByteVec, } impl \u0026lt;\u0026#39;a\u0026gt; Iterator\u0026lt;\u0026#39;a\u0026gt; for MemTableIterator\u0026lt;\u0026#39;a\u0026gt; { fn seek(\u0026amp;mut self, target: \u0026amp;[u8]) { let target_key_min = InternalKey::new(target, MAX_SEQUENCE,K_VALUE_TYPE_FOR_SEEK); let min_bound = Bound::Included(target_key_min); self.iter = self.skip_list.range((min_bound,Bound::Unbounded)); self.next(); } } 作为一个迭代器的Wrapper，其中封装了skip_map的iter，例如seek等功能，需要根据skip_map去重新生成一个iter，并保存到iter的字段当中，iter本身是对容器的引用，因此这个生成的过程就是去获取一个引用。\n而此时就需要去进行一个生命周期的保证，原本的容器的引用skip_map至少要和iter活得一样久，才能够生成一个引用容器的iter，并复制给结构体中的字段。\n在这种生命周期的标注下，表明了：\n在创建结构体MemTableIterator时，设当前结构体的生命周期为a 在结构体当中会引用一个至少和当前结构体生命周期a一样长的容器 保存一个 iter 字段，其生命周期至少 和当前结构体一样长 对于上面的情况，如果结构体当中保存一个Arc\u0026lt;SkipMap\u0026gt;的话，对于该方法，传入了\u0026amp;self，而在rust当中，\u0026amp;self是独立于结构体中声明的生命周期'a的，可以这里可以定义为'b，编译器无法得知\u0026rsquo;a \u0026lsquo;b 之间的生命周期关系，因此就会拒绝掉。该函数实际的声明如下：\n1 2 3 4 5 6 7 8 impl \u0026lt;\u0026#39;a\u0026gt; Iterator\u0026lt;\u0026#39;a\u0026gt; for MemTableIterator\u0026lt;\u0026#39;a\u0026gt; { fn seek(\u0026amp;\u0026#39;b mut self, target: \u0026amp;[u8]) { let target_key_min = InternalKey::new(target, MAX_SEQUENCE,K_VALUE_TYPE_FOR_SEEK); let min_bound = Bound::Included(target_key_min); self.iter = self.skip_list.range((min_bound,Bound::Unbounded)); self.next(); } } 而至于rust为什么要这样做，主要是为了灵活性。例如，某些方法可能只是临时借用结构体的数据，而不需要持有与整个结构体相同的生命周期。通过允许独立的生命周期，Rust 可以更准确地表示这种借用行为:\n1 2 3 4 5 6 7 8 9 struct MyStruct\u0026lt;\u0026#39;a\u0026gt; { reference: \u0026amp;\u0026#39;a i32, } impl\u0026lt;\u0026#39;a\u0026gt; MyStruct\u0026lt;\u0026#39;a\u0026gt; { fn get_reference(\u0026amp;self) -\u0026gt; \u0026amp;i32 { self.reference } } 生命周期消除 如果所有的引用都需要手动进行标注，那么编程体验自然是灾难的，因此编译器设置了三条规则，如果满足了就可以自动完成生命周期标注，从而不需要手动标注：\n每个引用参数都会获得独自的生命周期 如果只有一个输入生命周期，那么该生命周期就会赋给所有的输出生命周期 如果存在多个输入生命周期，其中一个是\u0026amp;self或者\u0026amp;mut self，那么\u0026amp;self的生命周期被赋给所有输出生命周期 来几个例子：\n1 2 3 4 fn first_word(s: \u0026amp;str) -\u0026gt; \u0026amp;str { // 实际项目中的手写代码 // 根据规则1，得到： fn first_word(s: \u0026amp;\u0026#39;a str) -\u0026gt; \u0026amp;str { //之后根据规则2完成了生命 1 2 3 fn longest(x: \u0026amp;str, y: \u0026amp;str) -\u0026gt; \u0026amp;str { // 实际项目中的手写代码 // 根据规则1，得到： fn longest\u0026lt;\u0026#39;a, \u0026#39;b\u0026gt;(x: \u0026amp;\u0026#39;a str, y: \u0026amp;\u0026#39;b str) -\u0026gt; \u0026amp;str { 后续就不满足任何规则了，因此无法自动消除，需要手动进行生命周期的标注。 而对于第三条，带上\u0026amp;self的就是方法了，对于方法的生命周期，得益于一三条规则，通常不需要进行手动标注。\n小结：生命周期的标注不会改变任何引用的实际作用域，他只是为了取悦编译器，让编译器不要难为我们，在进行标注之后，就会按照标注去进行检查，从而保证内存内存安全。\n共享所有权 在上述的情况中，数据的所有权永远只属于同一个变量。其他想要访问该数据只能通过引用来获取，这样的问题是，原始数据必须有最长的生命周期，才能够保证其他的引用有效。但是有些情况，需要多个所有者持有同一个数据，并且使用者之间是对等关系，无法确定一个最长的持有者。：\n在双向链表中，每个节点都会被前一个节点和后一个节点保存(持有)。 在多线程编程中，多个线程持有同一个数据，对其进行修改，由于 rust 的单一可变引用的限制，无法使用引用来完成。 在Rust当中，给出的解决方法就是借助引用计数的思想，使用智能指针Rc\u0026lt;T\u0026gt;与Arc\u0026lt;T\u0026gt;，其实现的作用类似于C++当中的share_ptr，不过做了更多的限制来保证安全。\n正如名字，Arc实现的引用计数是Atomic的，可以用于多线程环境当中，Rc反之。 相比于C++的share_ptr而言，Rc与Arc最大的差别就是实现的是不可变引用，通过该指针无法直接修改指向的数据，只能够进行读取，而如果进行读取，就需要通过内部可变性来实现，即RefCell和Mutex\n内部可变性 关于内部可变性，大概有两个比较重要的概念，一个是“共享”，即通过引用计数来令数据可以在多个持有者之间进行共享，并且允许进行修改。另一个是“内部”，体现了封装的思想。\n可变性\n对于基础的Rc和Arc，rust只允许对其进行读取，而无法修改数据，通过RefCell和Mutex允许对其进行修改，但是可变引用和不可变引用之间的冲突无法绕过，只是将这个过程从编译器推迟到了运行期，如果检测到违反约束，程序会直接panic。\n数据共享\n在单线程时大多数情况下，共享数据可以通过引用来解决，只要小心的保证只有一个可变引用的原则就可以实现，但是有些情况就很难处理了，即在逻辑上很难确定一个主从关系，将数据的所有权归于谁，而其他的去进行引用。由于原节点如果被释放的话，其他的引用全部失效，因此需要确定严格的生命周期关系，在有些情况下，各个使用者之间是对等的关系，因此很难确定出这样一种关系和生命周期，比较经典的一个例子就是双端链表，各个节点之间都是对等的，每个节点都可能因为移出链表而被释放，不存在一个明确的生命周期关系，这种时候再使用引用就不太符合逻辑了。 而对于多线程，那么就更随处可见了，全局原子性计数器、消息队列、cache、任务队列都需要进行共享，无法说出数据到底该归属于谁，就拿消息队列来说，队列究竟该属于谁？无论属于哪一方然后另外一方去引用都是不符合逻辑的，二者是一个共享的关系。其实这就是一个设计哲学的问题，在C++98当中，硬把队列归属于某一方，然后让另外一方去引用也没什么问题，只不过rust在设计上强调了共享的这个概念，并且在编译器层面做了限制而已。\n内部是什么\n在说明内部时，需要对于可变做一些诠释，在C++当中，通常我们去获取一个const引用，这时能够保证的是我们无法通过这个引用来修改原本的数据，但是在rust当中，我们获取了一个不可变引用，这时候我们所期待的是在我持有这个引用的这一段时间内，这个引用指向的数据都不会被改变，通过引用，能够对其进行“可重复读”。二者的出发点是有所不同的，C++的const是保证自身不去进行修改，而rust的非mut是保证没有其他的引用能够修改(同样也保证了自身不去修改)。 所以，这里我对rust中不可变的理解是：我能够获取到数据，并且在我使用数据的过程中，数据都是一直保证不变的。\n那么内部究竟该如何理解呢？这里rust有一个比较有意思的实现，就是对于一个可变的方法，如果他所属对象A在结构体B中被Arc\u0026lt;Mutex\u0026lt;T\u0026gt;\u0026gt;包裹，那么在B中就可以使用不可变的方法来进行调用，这里实际上是进行修改了的，但是可以保证同一时间只有一个可变引用，我们来看一个例子： 这里定义了一个ShardLRUCache，其中有多个LRUCache，而由于LRU的get会刷新缓存，因此他是一个\u0026amp;mut self的，但是在ShardLRUCache当中，在进行加锁之后，可以使用\u0026amp;self方法来对其进行调用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 struct BaseCache { replacer: LRUReplacer, block_table: HashMap\u0026lt;BlockId,Arc\u0026lt;Block\u0026gt;\u0026gt;, cache_size: usize, } impl BaseCache { pub fn look_up(\u0026amp;mut self,block_id: BlockId) -\u0026gt; Option\u0026lt;Arc\u0026lt;Block\u0026gt;\u0026gt; { match self.block_table.get(\u0026amp;block_id) { Some(block) =\u0026gt; { self.replacer.record_access(block_id.clone()); Some(block.clone()) }, None =\u0026gt; { None } } } } // 使用Arc\u0026lt;Mutex\u0026lt;T\u0026gt;\u0026gt;来完成多线程间的共享。 pub struct ShardCache { shards: Vec\u0026lt;Arc\u0026lt;Mutex\u0026lt;BaseCache\u0026gt;\u0026gt;\u0026gt;, last_id: usize, } // 使用不可变引用的方法对其进行调用 impl ShardCache { pub fn look_up(\u0026amp;self,block_id: BlockId) -\u0026gt; Option\u0026lt;Arc\u0026lt;Block\u0026gt;\u0026gt; { let idx = Self::find_shard(block_id); self.shards[idx] .lock().unwrap() .look_up(block_id) } } 而这里，rust在一个不可变方法中通过加锁的形式，调用了一个可变的方法，在进行加锁后，就可以保证自身在使用该引用时，不会有其他线程来篡改数据，从而将一个可变方法转换成了不可变方法，这也印证了我上面的说法，rust的不可变所说的是使用过程中不会被其他使用者改变。因此，这里的内部所说明的就是虽然内部实现是可变的，但是通过约束，可以保证在使用过程中的对外看起来是不变的接口，所以称为内部可变性。\n那么C++如何实现这种保证呢？在单线程环境中想要实现就需要程序员自身来进行约束，而多线程就需要锁来保证了。而rust同样在多线程时同样是使用mutex来解决的，如果不使用mutex包裹而去修改，就无法通过编译，从而在编译层面上解决了多线程的数据竞争的问题。单线程环境也是同理，在上面已经分析过了。\n综上，Rust通过引用计数 + 可变性，很优雅地在编译期就解决了数据共享所有权、并发、以及数据竞争的问题，从而极大地保证了内存安全，在code review时就可以专注于业务逻辑，而不是内存的管理。\n小结 在本章中，笔者分析了Rust的内存管理方案，既然能够通过编译器进行约束，那么无非就是定义一些规则，然后按照规则去进行检查，而这些规则对于其他非gc的语言同样是适用的：\n对于作用域内用完即销毁的：rust使用RAII，当离开变量作用域，结束生命周期时，对于堆上的数据同样进行回收，从而避免了手动进行delete 对于需要传递出作用域的：rust定义了所有权的概念，如果需要交给作用域外去继续使用，那么就需要移交所有权，将堆上的数据转移给另外一个变量负责，从而保证不会对该数据丢失管理，后续再按照其他方法继续持有或者gc 对于临时借用的引用：rust对引用标注生命周期，被引用者的生命周期至少要和引用者一样长，这样才不会再引用者使用时已经释放掉，产生垂悬引用。 对于局部变量，无论是在栈上还是堆上，由于生命周期会随作用域而结束，因此编译器直接拒绝对外传递引用 如果想要在多个所有者之间共享数据，那么就通过引用计数的方式来完成共享 对于数据竞争：单线程的数据竞争，rust通过一个r-w冲突来约束，即允许同一作用域内存在多个不可变引用或者单一可变引用，保证了获取到的不可变引用在使用过程中一定是不可变的。而对于多线程环境，则使用Mutex强制约束，不使用则无法通过编译，从而保证了在引用时的独占性和不可变性。 循环引用：和 c++相同，可以使用 weak_ptr 来处理。 其实这些规则都是一些不成文乃至成文的规定，在modern c++当中部分规则也早就支持，比如RAII、智能指针。但是其他的依旧会给程序员带来较大的心智负担，如果不去认真遵循，就会在运行期产生难以检测的bug，rust通过编译器强制约束，将大多数问题限制在编译器，一旦通过编译，就可以专注于业务逻辑，从而极大的降低程序员的心智负担。\nc++和rust之间也并不是什么对立关系，优秀的c++程序员接受起来rust没有什么难度，反过来，学习rust也有助于写出更高质量的c++。\n","permalink":"http://itfischer.space/en/posts/tech/%E6%89%80%E6%9C%89%E6%9D%83%E4%B8%8E%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F/","summary":"相比于传统c++当中的手动管理内存，rust采用了所有权 + RAII的机制，对于一些复杂的情况，rust还有生命周期等约束，从而保证在编译期能","title":"Rust内存管理"},{"content":"物理存储系统 这一部分重点并不多，主要是各种存储类型上的差异，比较了几种物理存储上的差异，以及SSD，HDD 之间的差异，最后又介绍了RAID的思想以及RAID分层，总结几个问题\n说一下常见物理存储介质？ 高速缓存（cache），通常指CPU的cache，处于速度最高但是最昂贵的级别，在数据库当中，通常不需要我们来进行手动管理 主存，即通常所说的内存，在数据库当中通常作为缓存使用，通过构建bufferpool manager来进行管理。至此仍为易失的(volatile) 闪存：早些年用于U盘等存储介质当中，目前的SSD也基于闪存，用途与磁盘（HDD）同效，可以保持非易失(non-volatile)，但是在访问速度上会领先HDD很多 磁盘：最传统的非易失存储介质，通过物理的方式进行存储，因此在速度上受限于物理速度，即disk arm的移动速度。购买磁盘时也会说明disk arm的转速 磁盘的物理特性 磁盘在垂直上由多个扇片组成，延中心，每一圈可称为一个磁道，在一个磁道上，有个氛围多个扇区，扇区即为磁盘写入和读取的最早单位。 读取和写入的过程极为disk arm转动找到对应的位置，再通过磁头进行写入，这个过程中涉及到了物理运动，因此速度较慢。\nRAID RAID(Redundant Array of Independent Disk) 独立磁盘冗余阵列，主要思想为将数据存储在不同的硬盘上，主要可以带来两点好处：\n通过冗余提高可靠性：即将同一份数据以page或者自定的块大小来存储在不同的磁盘之上，如果其中的一个磁盘损坏，还可以切换到备份磁盘上，从而保证能够提供正常服务，只有在第一份磁盘被修复前，备份全部挂掉时，才会真正的数据丢失。 通过并行提高性能：通过磁盘镜像，可以提高读取的速度，由于所有的备份全部为一致的，因此读请求可以发送到任意一个磁盘上，从而形成负载均衡或者减少单一磁盘的压力。此外，还一刻通过跨多张磁盘的方式进行数据拆分，从而提高数据传输率： 比特级拆分(bit-level striping) :较为简单或者常见的数据拆分方式为对一个字节进行拆分，将一个字节按照8bit进行拆分，如如果有一个八张磁盘组成的阵列，将每个字节的第i位写到第i张磁盘上。 块级拆分(block-level striping) :将磁盘阵列是为一个完整的大磁盘，按照一个完整磁盘来对各个块进行编号，之后根据编号取模于磁盘数得到该块存放的位置 RAID级别 镜像提供了高可靠性，但是很昂贵。拆分提供了高数据传输率，但是并未提高可靠性。我们系统通过较低的一个成本来获取可靠性并提高性能，这之间就涉及到了一些相关的trade off，从而RAID产生了各个级别：\nRAID 0:有块拆分但是没有任何冗余，可以提高数据传输率但是无法提高可靠性 RAID 1:提供冗余，有块级别拆分的磁盘镜像 RAID 5:具有交叉分布的奇偶校验，对于N + 1个磁盘，其中一个磁盘存储奇偶校验，另外N张磁盘存储这些块 RAID 6:P+Q的冗余方案，在RAID5点基础上存储了额外的冗余信息 RAID 234均不再使用 RAID1提供了最佳的写入性能，通常用于数据库系统的日志文件当中。RAID5具有比RAID1更低的存储开销，但是具有更高的写入时间通常用于写入较少的系统当中 电梯算法 电梯算法通常应用于磁盘臂的调度当中，工作方式与电梯的工作方式相同，假设一开始，磁盘臂从最内侧的磁道向磁盘的外侧移动，在电梯算法的控制下，对每条有访问请求的磁道，磁盘臂都会在那条磁道停下来为请求提供服务，之后继续向外移动，知道没有对更外侧磁道的请求，之后再调转方向向内侧移动，按照同样的思路进行，知道导师没有更靠近中心的有请求的磁道，之后调转方向开始一个新的周期\n数据存储结构 上一章所讨论的是用于存储的物理介质，而在此之上，通常有操作系统来进行进一步的抽象，对上层提供一个文件系统，而数据库即是在该文件系统之上来完成数据的存储。这一章所讨论的就是如何将数据组织安放到文件当中。\n文件组织 数据库使用操作系统所提供的文件系统进行存储，操作系统的文件在逻辑上进行划分，通常可以划分为4KB-8KB的page为单位，数据库可以默认使用操作系统的page大小，也可以自定义page大小，之后根据page大小进行访问和读取，一次性加载一个page到内存当中进行读写操作。 首先假定没有比块更大的记录，之后一条记录不会跨page\n定长记录的存储 对于定长记录，当确定长度之后，每条长度分配合适的空间，之后从前向后排列即可，存储时不需要在意顺序，因此在删除时选择将最后一条数据移动到当前删除的位置，之后拆入再从后插入即可。另外一种解决方案为设置一个header，通过free list的形式管理所有的被删除的数据的空白空间，之后即可插入时先通过free list进行获取\n变长记录的存储 思路基本一致，只不过对于一条记录，需要创建一个Entry，Entry的前几个元素用于存储后面每一个属性的长度，之后跟随记录当中定长的属性，最后放置变长属性，在整体存储上采用header来表示具体位置，新的记录从page的后面创建，header与记录之间的区域为自由空间。\n文件组织 堆文件组织： 记录可以存储在对应一个关系的文件中的任何位置，通常一旦确定位置，之后就不会再移动。对于堆文件(HeapFile)来说，一个HeapFile通常由多个HeapPage组成，堆文件需要通过一个自由空间图(free space map)来标识各个HeapPage当中的剩余空间，通常是以比例的形式存在，之后需要添加新的记录时，只需要通过遍历free-space map即可找到对应的空白位置。将数据写入到对应的HeapPage当中。而当heapPage过多时，可以考虑建立二级索引来提高效率。 多表聚簇组织 ： 对于某些常用的join语句，可以考虑将join的两张表进行联合存储，可以加快特定的join查询速度，但是带来的问题就是对于单表查询需要查询额外的字段 顺序文件组织 ： 在文件当中，根据某个属性去维护顺序存储，可以有利于某些依赖于顺序的查询操作，但是插入时的代价较高，需要进行整体移动，通常是一个page当中采用顺序组织，而各个page之间通过B+ Tree的形式进行组织，可以减少开销。\n缓冲池替换策略 对于操作系统，操作系统无法预知未来的page的访问情况，因此操作系统根据局部性原理使用LRU进行page的缓冲控制。 但是数据库能够比操作系统更准确的预测未来的访问方式，通过查看执行计划即可得知之后需要用到哪些page，可以针对计划进行设计：\n1 2 select * from instructor natural join department; 对于上述的sql，在使用完一条记录之后后面则不会再用到， 因此可以设计最近最常使用(Most Recently Used，MRU)，使用完立刻淘汰，与LRU刚好相反。\n事务 事务的四大特性 事务的四大特性分别为原子性、一致性、隔离性与持久性 对于原子性，则事务本身作为一个整体，要么全部成功执行，要么全部失败回滚，不存在执行成功一半的中间态。 一致性表明数据库在事务执行前和执行后，数据库的状态必须满足预定的一致性规则，即数据库只能从一种状态转换为另外一种一致性状态，最简单的例子即为银行转账，只有转账前与转账后的两种一致性状态，不存在一人转出而另一人未收到的中间状态。此外一致性分为数据库一致性和事务一致性，数据库只保证数据库一致性，就像上面所说的那样，而事务一致性则更是一种逻辑关系，需要操作人员去手动约束 此外 数据库的一致性应当与分布式系统当中的一致性的概念相区分，分布式当中的一致性指的是在多个节点上数据保持一致的状态，比如其中最严格的线性一致性就要求对外需要表现成单机节点，如果一次写入完成那么后续就一定要能够读取到数据 隔离型：并发执行的多个事务之间应当隔离，使每个事务感觉在独立的操作数据库，防止事务之间的相互干扰与数据污染 持久性：持久性表明一旦事务提交，那么其做出的更改应当为永久性的\n不过个人认为这四者不是简单的平级或者独立的关系，其中，一致性需要原子性和隔离型的支持，系统如果无法保证所有命令全部成功或者全部失败的话，自然会出现只执行了一半命令的中间态，而如果无法保证隔离性的话，一个事务还未提交的变更就能被其他的事务读取到，那么同样无法保证一致性，持久性可能相对的独立一点，和其他的两个没什么太大的联系\n可串行化 如果能够保证一个调度策略能够在结果上等价于串行化调度（即没有任何并发），那么就称为该调度为可串行化或者冲突可串行化。而冲突指的是读写冲突 判断可串行化主要有两个方案：\n通过交换指令进行判断，如果能够通过交换不冲突的指令来得到一个串行化调度，那么就称为其为可串行化的。 通过构造有向图的方式，即如果T1,T2之间有指令冲突，那么就构建一个从先执行的指令的事务指向后执行的指令的事务，而如果出现了回路，则证明其为不可串行化的。 而冲突的一种解决或者保护方式即为通过加锁来实现，如果加锁采用同样的读写锁模型+两阶段锁协议的话，那么冲突不可串行化就代表着死锁，同样可以通过寻找环路来判断和解决。 此外需要注意的是，存在一个调度与串行调度在结果上等价，但是为不可串行化的。 可恢复调度与无级联调度 可恢复调度指的是可以通过级联中止的方式来恢复脏读对数据库的影响。考虑这样一种情况，T1wtrieA之后T2进行了readA，而如果T1进行了回滚就会导致A称为脏数据，T2进行了脏读，而此时如果T2在T1之后才提交就为可恢复，反之为不可恢复。 而恢复的方式就是通过级联中止的方式进行恢复，即T1回滚会连同T2一同进行回滚。 为保证可恢复调度，可以通过标记依赖的方式，即T2依赖于T1，T2会等到T1commit之后再进行commit，或者通过强两阶段锁的方式(一同解决可恢复性和无级联) 正如上面所说，如果不想产生脏读，那么T1的回滚就会导致T2的回滚，这种情况成为级联回滚，级联回滚在涉及到大量事务时会产生相当大的开销，因此希望避免级联回滚的发生，如果一个调度不会产生级联回滚，那么称为无级联调度，在实现上通常采用两阶段锁的方式进行\n事务的隔离级别 四种隔离级别的差别主要聚焦于读操作上，对于写操作，为了保证数据库一致性，只有写入的锁只有数据提交时才能够释放，如果中途释放锁，写入的操作结果可能会被其他的覆盖，从而违反了数据库一致性。单独看写锁的话，都是采用了严格两阶段锁的策略，保证一致性，同时避免级联中止。\nREAD UNCOMMITTED\n最宽松的隔离级别，根据上表可以看到脏读 不可重复读 幻读都会发生\n在实现上读操作不需要加锁，写操作和其他的一致，因此所有读相关的锁都不支持，如果想尝试加[S,IS,SIX]，都会抛出异常。由于只支持写锁，在shrinking阶段，任何加锁操作都是不允许的，会抛出异常。\n因此并不保证读取到的数据一定是提交的数据，由于并没有加读锁和其他的事务进行互斥，其他的事务一旦写入之后就可以被读取，因此当读取到之后，如果写入数据的事务中止回滚，那么读取到的数据就变成了脏数据产生脏读。\nREAD COMMITTED\n相比于读未提交解决了脏读的问题，但是依旧会出现不可重复读和幻读\n在实现上读操作需要在Table上加IS，在Row上加S，并且当完成一次读取之后即可将锁释放，无需等待事务的提交。释放S锁并不会导致事务从growing向shrinking转变。\n由于通过S锁进行互斥，从而保证那些正在写入的事务在提交前不应被读取到，能够读取到的数据一定是提交之后稳定写入的数据。\nREPEATABLE READ\n在读提交的基础上又解决了不可重复读的问题，存在幻读。\n实现上加锁策略和READ COMMITTED并没有区别，但是在释放锁上读写锁全部采用严格两阶段锁的策略，此次事务会一直持有读锁，从而中途不可能被其他的事务重新写入更新，在一次事务当中读取的数据全部为一致的。\nshrinking阶段不允许添加任何的锁，释放S X锁就会向shrinking进行转变。\n可串行化\n最严格的隔离级别，简单来说就是在该隔离级别下，事务的执行可以等价成一次不存在并发的串行执行。最简单的判断是否为可串行化的方法就是交换一系列不冲突的指令，从而看原本的事务调度是否可以被分为两个在时间上完全错开的串行调度，简单用读写来表示，一次通过交换或重拍转换成串行结果如下：\n相比于可重复读，主要需要解决幻读的问题，当前成熟的商业数据库通常采用两阶段锁 + 索引锁（谓词锁）的方式进行解决，而一种比较简单粗暴的解决方式即为直接上表锁，即可避免在间隙中的读写行为，从而保证可串行化。### 说一下事务的隔离级别\n四种隔离级别的差别主要聚焦于读操作上，对于写操作，为了保证数据库一致性，只有写入的锁只有数据提交时才能够释放，如果中途释放锁，写入的操作结果可能会被其他的覆盖，从而违反了数据库一致性。单独看写锁的话，都是采用了严格两阶段锁的策略，保证一致性，同时避免级联中止。\nREAD UNCOMMITTED\n最宽松的隔离级别，根据上表可以看到脏读 不可重复读 幻读都会发生\n在实现上读操作不需要加锁，写操作和其他的一致，因此所有读相关的锁都不支持，如果想尝试加[S,IS,SIX]，都会抛出异常。由于只支持写锁，在shrinking阶段，任何加锁操作都是不允许的，会抛出异常。\n因此并不保证读取到的数据一定是提交的数据，由于并没有加读锁和其他的事务进行互斥，其他的事务一旦写入之后就可以被读取，因此当读取到之后，如果写入数据的事务中止回滚，那么读取到的数据就变成了脏数据产生脏读。\nREAD COMMITTED\n相比于读未提交解决了脏读的问题，但是依旧会出现不可重复读和幻读\n在实现上读操作需要在Table上加IS，在Row上加S，并且当完成一次读取之后即可将锁释放，无需等待事务的提交。释放S锁并不会导致事务从growing向shrinking转变。\n由于通过S锁进行互斥，从而保证那些正在写入的事务在提交前不应被读取到，能够读取到的数据一定是提交之后稳定写入的数据。\nREPEATABLE READ\n在读提交的基础上又解决了不可重复读的问题，存在幻读。\n实现上加锁策略和READ COMMITTED并没有区别，但是在释放锁上读写锁全部采用严格两阶段锁的策略，此次事务会一直持有读锁，从而中途不可能被其他的事务重新写入更新，在一次事务当中读取的数据全部为一致的。\nshrinking阶段不允许添加任何的锁，释放S X锁就会向shrinking进行转变。\n可串行化\n最严格的隔离级别，简单来说就是在该隔离级别下，事务的执行可以等价成一次不存在并发的串行执行。最简单的判断是否为可串行化的方法就是交换一系列不冲突的指令，从而看原本的事务调度是否可以被分为两个在时间上完全错开的串行调度，简单用读写来表示，一次通过交换或重拍转换成串行结果如下：\n并发控制 锁的授予方式 首先考虑锁冲突的问题，最简单的为读写冲突，如果产生冲突自然无法进行加锁，之后可以考虑更加细粒度的锁，如对row加锁时需要先获取table上的锁。 不存在冲突时并不意味着可以直接获取锁如果当前T1持有S锁，T2试图获取X锁正在阻塞，此时T3想要获取S锁，则与T1不冲突可以直接获取，但是之后的事务都来获取S锁，那么长久以来就会出现T2事务starved。 因此合理的组织方式就是对于一个需要上锁的资源，通过一个队列进行管理，只有自身与当前持有的锁不冲突，并且在等待队列中排名第一时才能获取锁。保证了互斥与防止starved。此外还有锁升级的相关细节，可以参考bustub当中的实现。\n两阶段锁协议 首先明确概念，两阶段锁协议分为两个阶段：\n增长阶段(growing phase)，只能获取锁 缩减阶段(shrinking phase)一个事务可以释放锁，但是不能获取任何新锁 事务一旦释放了任何一个锁，就会从growing phase转换到shrinking phase。 2PL：读锁写锁都按照growing shrinking的方式来获取释放，最基础的两阶段锁所解决的不可重复读的问题，整个读取过程中持有锁，从而保证了不会有其他的事务来修改数据 S2PL：写锁只有在事务提交时才能够释放，读锁与原本保持不变，即写锁的shrinking阶段变成了一个点，2PL存在的问题是由于过早的释放锁，会导致出现脏读/脏写的问题，需要通过级联回滚的方法来解决，但是这个方案会产生较大的开销。S2PL只有在commit时才会释放锁，从而其他事务读取到的一定是commit了的数据，从而避免了级联中止的问题 SS2PL(rigorous two-phase locking protocol)：作为S2PL的变体，要求读锁也在commit时进行释放。 基于图的协议 对所有需要加锁的资源来构建一个偏序的关系（有向无环图），之后如果需要进行加锁，则按照该偏序关系进行加锁，并且，只构建带有根节点的形式。 通过该树形式，可以保证冲突可串行化和不会产生死锁，但是并不能够保证可恢复性和无级联性\n如果想要保证可恢复性，可以引入一个依赖关系，自身依赖的事务在commit前自身不能够进行commit 如果想要同时保证可恢复性和无级联性，则需要在事务结束前不释放排他锁。 死锁的预防 死锁的预防主要分为两种：\n一种是通过排序的方式，即上面的树状形式就是通过一种偏序的方式来进行死锁的预防，或者像之前godis所写的那样，通过对全局的key进行排序以预防死锁。 另外一种即为基于时间戳的方式，细分为抢占式的wound-wait和非抢占式的wait-die 此外最简单的方式即为通过锁超时的方式进行，超时即放弃事务回滚。 死锁检测与恢复 死锁检测的方法和之前冲突可串行化的判断方式相同，通过构建等待有向图的方式进行判断，如果在图中存在环路，则证明存在死锁/不可串行化。 死锁恢复： Victim：通过环路找到了死锁之后，考虑选择一个牺牲者事务进行回滚，来破除掉环路(死锁)，牺牲者的选择准则：\n事务执行时间，以及之后还要执行多久 事务已经使用的数据项，完成事务还需要多少数据项 回滚事务牵扯到的事务数量 回滚： 全部回滚 部分回滚，回滚到刚好可以打破死锁的位置，需要记录额外的信息。 幻读与索引锁 幻读是在事务并发执行时可能出现的一种现象，它指的是在一个事务内部，多次执行相同的查询操作，但在不同的查询中返回的行数却不一致。通常是由于其他事务并发地插入或删除了符合查询条件的数据所导致的。 主要原因为单独的行锁无法对不存在数据进行加锁，因此无法限制新数据的插入，从而产生了两次查询结果不一致的问题，解决方案也即为对“不存在”的数据进行上锁。常见的解决方法为索引锁或者谓词锁。 索引锁 索引锁以B+树为例，对叶子结点进行上锁，此次sql涉及到的所有的叶子结点均进行上锁，如果要在会涉及到的范围内插入新数据，则势必会引起冲突，即便是开区间如\n1 select * from a where a.col1 \u0026gt; 10000; 然后插入一个col1 = 20000的记录，也会落到最后一个叶子结点上，即便最后一个叶子结点已满，也需要先插入到其中再分裂。 谓词锁 谓词锁即为针对where后面的条件进行上锁，从逻辑上进行冲突判断\n时间戳排序协议 先介绍一下基本时间戳顺序协议\n首先通过时间戳来对事务进行标记，同时对于一个tuple 需要记录其 W-TS和R-TS，即最后一次对其进行读写操作的事务的时间戳，用于在一个事务要对一个tuple进行读写操作时判断是否有其他事务会和它产生冲突\n在基本时间戳顺序协议中，时间戳记录的是该事务开始时的时间，即通过BEGIN关键字声明开启事务时旧分配一个时间戳，但是DBMS本身还是并发的，因此可能会出现后续的事务在较早的事务之前执行，此时就可能存在问题，可能要考虑将较早的事务给撤销，并重新分配时间戳重新执行\n当一个事务访问到一个tuple时，发现TS \u0026lt; W-TS ，则证明未来的一次写入覆盖了目前正在读的数值，即出现了脏读问题，读取到的是旧值，需要终止\n对于读写，有以下的规则：\n读：\n如果TS($T_i$) \u0026lt; W-TS(X),则违反了时间戳顺序，读到了旧的值，该事务需要重启并分配新的时间戳\n否则：\n允许事务T去读取tuple X 更新R-TS（X）为$max(R-TS(X),TS(T_i))$ 生成一个本地副本用于该事务后续的重复读（如果再去读最新的数据则会产生两次读取不一致的不可重复读的问题） 写：\n如果$TS(T_i)\u0026lt;R-TS(X) or TS(T_i) \u0026lt; W-TS(X)$ 放弃$T_i$并重启$T_i$\n$TS(T_i)\u0026lt;R-TS(X)$会导致后续读取操作读取不到此次的写入 $TS(T_i) \u0026lt; W-TS(X)$会导致此次的写入无意义 否则：\n允许事务$T_i$去写入，并且更新W-TS(X) 生成一个本地副本用于可重复读 对于写操作，还可以通过Thomas Write Rule进行优化：\n$TS(T_i) \u0026lt;R-TS(X)$:放弃并重启$T_i$（与原本一致） $TS(T_i) \u0026lt;W-TS(X)$:忽略此次写入，并允许事务继续执行 其他情况则未违反协议，允许$T_i$写入，并更新$W-TS(X)$ 虽然本质上违反了协议，但是当前此次写后续会被覆盖掉，因此可以直接进行忽略\n总的来说，基本时间戳顺序协议的精髓为当一个事务创建时分配一个时间戳，由于事务的原子性，将事务当中所有的指令的发生时间全部集中于begin的这一时间戳上，之后如果有违反时间顺序的，事务则会中止回滚，并尝试重新开启事务，如读取时发现有未来的事务已经对其进行了写入。 可恢复性与无级联性 基础的时间戳排序协议是无法提供可恢复性与无级联性的，可以通过以下的方式来保证：\n通过在事务末尾一起执行所有的写操作可以保证可恢复性和无级联性 可恢复性与无级联性也可以通过将读操作推迟到更新该数据的事务提交之后进行来保证。 单独的可恢复性可以通过依赖追踪的方式进行解决，即只有依赖的事务提交了才会提交当前事务 Thomas写规则 Thomas写规则对基本时间戳协议进行了一些优化，经过优化后的写规则如下：\n如果$TS(T_i)$ \u0026lt; R-timestamp，则证明已经存在了一个更新的读取操作，此次写入是不被需要的，因此直接拒绝并回滚 如果$TS(T_i)$ \u0026lt; W-timestamp，则证明已经存在了一个更新的写入操作，当前的写入之后会被覆盖，因此什么也不需要做，忽略掉此次write 其他情况则正常写入 MVCC 严格来说，MVCC(Multiversion Concurrency Control)并不是一种并发手段，他并不能够完整的解决并发冲突问题，MVCC能够提供的只有读与写之间不产生冲突，而无法保证写与写之间的冲突问题，因此需要引入额外的并发控制手段，分别有两种，为与时间戳协议进行组合的多版本时间戳排序和与两阶段锁组合的多版本两阶段封锁。 最后用一句话来描述MVCC即为读不阻塞写，写不阻塞读。\n多版本时间戳排序 对于每个数据项，存在一个版本序列$\u0026lt;Q_1,Q_2,Q_3,\u0026hellip;,Q_k\u0026gt;$ ，并且每个版本$Q_k$包含三个字段：\nContent:当前版本的值 W-timestamp:创建$Q_k$版本的事务时间戳 R-timestamp：所有成功读取过的当中的最大的事务时间戳 当事务$T_i$携带时间戳$TS(T_i)$创建该版本时，使用该时间戳来初始化W-timestamp和R-timestamp 基于时间戳，读写有以下的规则： 如果事务T发出read(Q)，那么它能够读取到的为小于等于T的时间戳的最大的版本的值 如果事务发出write(Q)： 如果时间戳$TS(T_i)$ \u0026lt; R-timestamp，则证明存在更新的读取，回滚事务 如果时间戳$TS(T_i)$ \u0026lt; W-timestamp，则忽略掉此次写入 如果时间戳$TS(T_i)$ = W-timestamp，则覆盖掉此次版本的内容 如果时间戳$TS(T_i)$ \u0026gt; W-timestamp，则创建一个W-timestamp = $TS(T_i)$的新版本 由于应用了时间戳，因此本质上为乐观的并发控制协议，适用于冲突发生比较少的情况。 该方案不保证可恢复性和无级联性。可以按照对基本时间戳排序的方式进行扩展，使其具有可恢复性和无级联性。 多版本两阶段锁协议 将MVCC与两阶段锁协议结合起来，对于事务分为只读事务和更新事务。\n更新事务在创建时会获得一个逻辑时间戳，创建的新版本的时间戳为正无穷，之后在事务提交时更新为事务的时间戳 更新事务想要读取一个数据时需要获取一个S锁，尝试写入时需要获取X锁 只读事务无需加锁，只需要更新时间戳去读取对应的版本即可。 ","permalink":"http://itfischer.space/en/posts/tech/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA/","summary":"物理存储系统 这一部分重点并不多，主要是各种存储类型上的差异，比较了几种物理存储上的差异，以及SSD，HDD 之间的差异，最后又介绍了RAID的","title":"数据库系统概念"},{"content":"分布式系统模型 拜占庭将军问题 拜占庭将军主要用于解决共识问题，三个将军决定攻打拜占庭，只有三个人一同进行攻打才能够成功，否则则会失败，因此需要三者之间通过信使来进行通信，以求达成共识，整个系统是处于不信任状态的，即将军可能会是叛徒，向其他的将军传递错误的信息，信使也是不可靠的，可能会被俘虏或者数据为假。\n将军即对应分布式系统当中的节点，节点不仅会发生故障或者宕机，其中会有一些节点为叛徒，可能恶意篡改和传播错误信息，信使即对应着网络通信，在此条件下网络通信也为不可靠的。 系统模型分类 网络链路模型 可靠链路 ：可靠传递，没有重复，不会无中生有，但是对于消息可能会出现排序 公平损失链路： 公平损失：如果发送方和接收方都正常运行，发送方不断进行尝试，消息最终会送达 有限重复：消息只会重复发送有限次 不会无中生有 任意链路：最弱的网络模型，允许任意的网络链路执行任何的操作，可能会有软件恶意篡改数据包 节点故障模型 崩溃-停止(fail-stop) ：一个节点停止工作后就永远不会恢复 崩溃-恢复(fail-recover):允许节点重启后继续执行剩余的步骤，一般通过持久化存储的方式 拜占庭故障：故障节点不止会当机，还有可能以任意方式偏离算法，甚至恶意破坏系统 时间划分 可以分为同步和异步：\n同步指一个消息的响应时间在有限且已知的一个时间范围内 异步的消息的响应时间为无限的，不知道具体的消息到达时间 分布式系统数据基础 水平分区算法 范围分区\n根据指定的关键字拆分成若干连续范围，问题是无法对于分区关键字以外的关键字进行范围查询，数据本身分布不均时容易造成某些节点负载较高\n哈希分区\n对指定关键字使用哈希函数进行分区，传统的哈希函数的问题如果添加新的分区则需要对原本所有的数据进行全部rehash，对于hashtable来说，可以使用可扩展哈希表的方式降低扩展的负载。但是此处只是使用了哈希函数，因此需要对哈希函数进行改进采用一致性哈希\n一致性哈希\n将整个哈希值抽象成一个圆环，而节点分布在圆环上，数据存储在按照顺时针方向遇到的第一个节点上，此时如果添加或者移除一个节点，只会有相邻的另外一个节点收到影响，避免的全局rehash 一致性哈希同样存在数据倾斜的问题，即如果一个节点下线，其存储的数据会全部转移到相邻节点，导致该节点的负载过高，解决方法即为采用虚拟节点的方式，一个物理节点对应多个虚拟节点，使数据分布更加均匀。 复制 单主复制 数据首先写入到主节点上，之后再由主节点复制到从节点上，这个过程可以为同步的，也可以为异步的，同步可以保证从节点一定能够读取到最新的消息，但是对客户端的响应速度就减慢，即影响写请求的性能，异步则以牺牲数据一致性和持久性来换取响应速度。\n此外还有半同步的方式，即对于一个节点采用同步的方式，其他的节点为异步的方式，这样可以一定程度上保证响应速度，并且在主节点当机之后一定有一个和主节点完全一致的从节点能够直接顶替主节点。\n单主复制的性能瓶颈为只有一个主节点来响应写请求。\n多主复制 通过多个主节点来接受写入请求以提升写性能，显而易见会带来冲突问题，主要解决方式有：\n客户端解决，产生冲突时，将所有的冲突数据返回给客户端，由客户端选择合适的数据 最后写入胜利，通过每条请求上携带的时间戳或者自增ID 因果关系跟踪：happen - before 无主复制 平等的对待每个节点，每次写入尝试向所有的节点进行写入，读取也对所有的节点进行读取。基于大多数原理，即对于N个节点的情况，如果$W + R \u0026gt; N$ 则可以保证读与写之间存在一个交集，从而保证新写入的数据一定能够被读取到。\n在该配置下，如果一次写入有W个节点给予响应即认定写入成功，读取也发送给多个节点，由于存在冲突问题，客户端根据读取到的版本号来解决冲突，采用版本号最新的数据作为结果\n除了$W + R \u0026gt; N$之外，还需要$W \u0026gt; N/2$ ,来保证数据的串行化修改，即两个不同的写请求不能够同时成功修改同一份数据。每次写入要求写入大多数即可保证两次写入之间存在一个交集，从而发现冲突。但是即便$W \u0026lt; N/2$的话，那么就意味着$R \u0026gt; N/2$，那么在一次检索过程当中就一定能够发现冲突的数据，此时可以根据时间戳来进行修复，保留最新的数据\nCAP定理 在一个异步的网络环境中，对于一个分布式读写存储系统，只能满足一下三个特性之二\n一致性(Consistency) 可用性(Availability) 分区容错性(Partition Tolerance) 一致性为客户端进行一次读取，总能给获取到最新的数据（与数据库一致性进行区分）可用性为每次请求都能获得一个非错误的响应，但不保证获取的数据为最新的。分区容错性为节点由于网络分区而导致消息丢失的情况下，系统仍能正常运行。 CAP三角通常指的是三者无法得兼，只能选其中之二，但是对于实际的应用，所期待的是其能够抵抗网络分区，因此CAP又可以表述为在出现网络分区时保证分区容错性情况下，是要求一致性还是要求可用性。\n反之则为如果不出现网络分区，则可以同时保证一致性和可用性\n一致性模型 一致性模型首先需要与数据库的一致性约束进行区分。其次，一致性的概念也不是分布式系统当中独有的，（内存一致性）具体来说指的是：在并发编程中，系统和开发者之间的一种约定，如果在开发这遵循约定，则执行读操作或斜操作的结果是可预测的。\n线性一致性 最强的一致性级别，通常可以使用Linearizability表述，线性一致性的直观理解或者说非严格定义可以表述为所有操作看起来为原子的，整个系统看起来只有一个节点。\n线性一致性的严格定义为：给定一个执行理事，执行历史根据并发操作可以扩展为多个顺序历史，只要从当中找到一个合法的顺序历史，那么该执行理事就是线性一致性的。\n在执行历史当中又可以分为顺序的和并发的，顺序则为时间A的结束时间早于时间B的开始时间，而并发则是两个时间当中存在重叠的部分。在生成顺序历史时，顺序关系必须由先发生的指向后发生的，而并发的则可以进行编排，最后如果能够获取到一个读写请求都合法的顺序历史，则称其为线性一致性的。\n线性一致性最困难的就是需要一个全局时钟，从而来确定时间发生的时间顺序\n顺序一致性 只要求一个客户端的操作在排序后保持顺序不变，不同客户端之间的先后顺序可以任意改变。通过改变顺序之后，可以得到一个合乎逻辑的顺序历史。\n由于各个客户端之间的顺序可以改变，因此没有全局时钟的限制。\n在一个社交网络应用当中，通常不关心看到的所有朋友的帖子的顺序，但是对于某个具体的朋友，以正确的顺序去显示更加符合逻辑性。\n因果一致性 要求必须以相同的顺序看到因果相关的操作，没有因果关系的并发操作可以被不同的进程以不同的顺序观察到。如先发帖才能有该帖子的评论体现了发生于\u0026hellip;..之前(Happens-Before)关系。\n最终一致性 在某个阶段，系统个节点处理客户端的操作顺序可以不同，读操作也不需要返回最新的写操作的结果。\n只要在最终的状态下，即不再执行写操作，读操作将返回相同的，最新的结果。\n以客户端为中心的一致性模型 即不关注多个客户端操作的并发结果，只站在单一客户端的角度上进行分析，可以分为：\n单调读：如果客户端读到关键字x的值v，那么之后的读取只能够读取到比v更新的值 单调写：同一个客户端的写操作在不同的副本上必须以同样的顺序执行 读你所写(Read Your Write):写操作完成后，同一副本或者其他副本上的读操作必须能够读取到写入的值 PRAM(Pipelined RAM):也称为FIFO一致性，同一个客户端的多个写操作，将被所有的副本按照同样的执行顺序观察到 读后写(Write Follow Read)：如果先读到了写操作w1的结果v，那么一周的写操作w2保证给予v或比v更新的值 分布式共识 状态机 状态机包含一组状态、一组输入，一组输出，一个转换函数和一个输出函数，和一个独特的初始状态。状态机从初始状态开始，每个输入都被传入转换函数和输出函数，产生一个新的状态和输出，在收到新的输入前，状态机的状态保持不变。\n对于共识，就是在一个可能出现任意故障的分布式系统中的多个节点对某个值达成共识。由于状态机的特性，这个问题可以很好的使用状态进行解决：\n采用日志作为输入，从相同的位置以相同的顺序读取日志内容并执行，则会产生相同的输出，结束在相同的状态上，备份到不同的节点上并输入给状态机，从而即可使从节点达到和主节点一样的状态。\nPaxos 基本概念 每个节点通过提案(proposal)的方式来推进状态的改变，因此每个提案需要提案编号(proposal number)和提案值(proposal value)\npaxos通过轮次+服务器id的形式来组成一个唯一的提案值，即\u0026lt;n,server_id\u0026gt;，paxos解决冲突的方法即为根据提议号的大小，对同一个值只接受第一次的提议\n算法角色：\n客户端：客户端向分布式系统发起一个请求，并等待回应 提议者(Proposer)：提议者收到客户端的请求，提出相关的提案，试图让接收者去接受提案 接收者(Accepter)：也叫投票者(Voters)，投票接受或者拒绝提议者的提案，如果超过半数接受着接受，则提案批准 学习者：不参与决议提案，只学习提案值并执行 算法流程 这里讨论Basic Paxos，只针对一个值达成共识，而不是像raft那样进行完整的日志复制，即指对一个日志的entry进行共识，如果想要实现如raft的效果，需要使用multi paxos，但是multi paxos与raft还存在着一点差别。\n第一阶段\n第一阶段提议者发送RPC称为phase 1a，也叫做prepare阶段，在这个阶段只发送提议号，而不设计具体内容\n1 send Prepare(++n) 响应的过程称为phase 1b，称为promise阶段，在这个过程当中，主要根据提议号来判断是否存在冲突：\n如果prepare当中的n大于之前接受的所有提案编号，那么返回promise进行响应，并且承诺之后不再接受任何小于n的提议，如果对于该值接受了某个提案，那么就返回前一次的提案号与提案值 否则忽略该请求，恢复一个拒绝响应 1 2 3 4 5 6 7 8 9 10 if (n \u0026gt; max_n) { max_n = n if (proposal_accepted == true) { respond: PROMISE(n,accepted_N,accepted_VALUE) } else { respond:PROMISE(n) } } else { respond with fail message } 第二阶段\n第二阶段发送 rpc 时称为 phase 2a，Accept 阶段，如果提议者能够收到半数以上的 phase 1b 的正确回应，则进行 phase 2a 阶段，如果 phase 1b 的响应当中存在提议值，那么就使用该提议值，否则提议者可以自由决定提议值\n1 2 3 4 5 6 7 8 if (success \u0026gt; n/2) { if (contains_acc_val()) { val = accepted_VALUE } else { val = VALUE } send Accept(ID,val) } 第二阶段的响应被称为phase 2b，如果在此之前没有接受过比n大的提案，那么久接受n，保存提案\n1 2 3 4 5 6 7 8 9 if (n \u0026gt; max_n) { proposal_accepted = true max_n = n accepted_N = n accepted_VALUE = VALUE respond:Accepted(N,VALUE) to proposer and all learners } else { respond with fail message } 对于basic paxos，其过程和两阶段提交比较相像，即均存在一个prepare的过程，先去询问从节点是否能够执行命令或者进行写入，当收到肯定的答复之后，进行第二轮去真正执行命令。只不过二者的目的略有不同，分布式事务的两阶段提交意在所有节点共同参与，因此需要所有的节点均给予应答，因此存在阻塞的问题，而paxos意在形成共识，因此只需要有超过1/2的节点能够正确响应即可。\n活锁 系统中可能存在两个提议者不断交叉进行提议，A在proposal之后从节点又收到了B的proposal，从而A的accept请求被拒绝，之后A再进行proposal，产生了有个更大的proposal number，从而B的Accept请求也会被拒绝，因此系统产生一种活锁的状态，解决方法和raft一样，引入随机的超时时间来避免互相竞争导致的活锁问题。\nMulti Paxos paxos只是用于完成一次共识，而对于分布式存储的多条日志，则需要对于每条日志均运行一次paxos，从而让从节点能够拿到所有的日志，称为multi paxos。 后续有空再进行补充\nRaft raft已经比较熟悉了，这里就进行简单的补充\n安全性与活性 选举过程中需要保证安全性与活性：\n安全性只在一个任期当中，只有一个leader能够被选举出来，相关保证为： 一个节点在一个任期当中只能够进行一次投票，并且投票信息需要进行持久化存储 只有获得超过半数的选票之后才能够称为leader，从而保证不会同时有两个leader当选 活性指的是要确保系统最终能够选举出一个leader，因此需要在选不出leader时有的节点会超时触发新的一轮选举，并且为了防止陷入活锁问题，raft同样需要设置一个随机的超时时间(150ms-300ms) 延迟提交 考虑以下这种情况，上一任期的leader为s1，并且s1已经提交了index = 3的日志，此时s1宕机，重新进行leader election，S5当选为leader，之后S5进行日志复制时，就会覆盖掉S2 S3当中index = 3的日志，但是该日志已经在s1为leader时进行了提交，从而导致已经提交的日志丢失。 针对这种情况，需要进行额外的限制：\n日志必须存储在超过半数的节点以上 Leader 必须看到超过半数的节点上还存储着至少一条自己任期内的日志 这就是所谓的延迟提交，换句话说，leader不能够提交不属于自己任期的日志，对于之前的日志，leader只能在提交当前任期日志时，顺带进行提交。即只有以下的情况时，leader S1才能够对index = 3的日志提交 no-op空日志\n通过延迟提交的方式，可以解决掉已提交日志被覆盖的问题，但是存在的问题是leader需要等待客户端发来一条请求才能够生成对应的日志，否则index = 3的日志则会迟迟得不到提交，导致对应的请求阻塞，无法收到响应。\n解决方式即为当leader上线之后，立马向自身写入一条没有内容的空日志，并进行复制与提交，通过提交空日志来顺带提交之前的日志，从而推动整个系统\n处理旧的领导者 在不存在网络分区的情况下，对于旧的领导者处理很简单，只需要进行一次rpc通信，即可通过比较携带的term来完成。而对于网络分区的情况，由于raft在CAP当中选择了CP，因此处于较少分区的leader无法形成大多数的共识，因此无法提交日志和响应客户端，但是身份依旧为leader，客户端就会继续和他进行通信，因此最好让该leader及时退位。\n相关优化即为check quorum，如果leader发现与自己能保持通信的follower\u0026lt;= n/2，则主动退位\n实现线性一致性 线性一致性的实现方案主要有三种，最简单的为将读请求也写入到日志当中，之后即可按照日志的顺序来严格执行，从而保证顺序。但是会产生写日志和复制日志的额外开销。 相关优化有两种，分别为read index和lease read：\nReadIndex\n将当前自己的 commit index 记录到一个 local 变量 ReadIndex 里面。 向其他节点发起一次 heartbeat，如果大多数节点返回了对应的 heartbeat response，那么 leader 就能够确定现在自己仍然是 leader。 Leader 等待自己的状态机执行，直到 apply index 超过了 ReadIndex，这样就能够安全的提供 linearizable read 了。 Leader 执行 read 请求，将结果返回给 client。 在这个过程当中同样需要no-op来保证当前leader的commitIndex为集群当中最新。\n只有commit了日志才能够视为顺序关系，而对于那些在commitIndex之后写请求，将其视为与当前的读请求为并发关系，因此可以不读取到其中的内容，虽然与Log Read获取到了不一样的结果，但是依旧是满足线性一致性的。\n在使用ReadIndex时，可以采用在follower出进行读取的方式来提高吞吐量，即follower向leader去索取一个ReadIndex，后续等待自身的appliedIndex == ReadIndex时仅可进行读取，对于local read，还有其他的实现方案，留在etcd当中进行分析\nLeaseRead\nLeader使用正常的心跳机制来位于一个lease，心跳开始的时间标记为start,如果leader的心跳被多数派确认，那么在start+electionTimeout/clockDriftBound的时间内，Leader均为有效的leader，不会因为其他节点进行选举而退位，因此可以安全的处理只读查询，结果全部满足线性一致性。\n性能优化 raft涉及到的主要性能优化即为批处理和流水线，这两个都在etcd当中进行了实现，这里就不额外进行分析了\n分布式事务 分布式事务可以理解为单机事务的变体，主要分为两种类型，一种是同一份数据需要在多个副本上更新，一个事务需要更新所有的副本，而另外一种为事务所需要的数据存储在不同的节点上，一次事务操作需要跨越多个事务，保证在分布式情况下，同样能够令事务遵循ACID，其中持久性可以通过WAL的形式来解决，一致性通常不在讨论范围内，主要需要克服的就是原子性与隔离性，需要通过原子提交和并发控制来进行解决\n原子提交 协定性：所有进程决定出同一个值，即要么所有进程全部提交，要么所有进程一同中止事务 有效性：只要有一个事务决定中止事务，系统最终将中止事务 终止性：可以分为弱中止条件和强中止条件 弱中止条件：如果没有故障发生，所有进程最终都会做出决议 强中止条件没有发生故障的进程最终会做出决议 两阶段提交 角色上分为协调者(Coordinator)和参与者(Participants),协调者负责进行协调算法的各个阶段，参与者参与到事务执行的各个部分\n阶段：\n第一阶段称为准备阶段：\n协调者向所有参与者发起信息，询问是否可以提交事务，等待参与者响应\n参与者检查执行所需的条件，如果发现自身事务执行的都成功，则回应yes，反之no 第二阶段称为提交阶段：\n如果所有的参与者第一阶段都回应了yes，协调者向所有的参与者发送信息，请求提交事务\n参与者收到信息后进行事务的提交，并响应协调者\n协调者如果收到了全部的正确响应，确认事务成功提交\n如果其中第一阶段有一个参与者响应了no，则协调者指示参与者中止事务\n参与者利用第一阶段执行时保存的信息进行回滚\n中止完成后响应给协调者，收到所有参与者的信息后，确认事务的中止 相关故障\n第一阶段参与者回复协调者时超时，会导致协调者一直等待，解决方法为引入一个超时时间，超时则认为该参与者投了反对票（可解决） 协调者在向参与者发送消息后立刻发生了故障，除非协调者恢复，否则会一直阻塞下去 在第二阶段发生了网络分区，协调者只向一部分的参与者发送了提交信息，此时进行读取则会产生不一致性 在第二阶段，如果协调者发送信息之后宕机，该信息之发送给了一个参与者，而该参与者收到信息并执行后也宕机，此时则会完全不知道系统的状态，如果盲目提交或中止，宕机的参与者如果恢复过来并且状态不一样则会引发数据不一致问题 基本的2PC的根本问题是无法解决单点故障问题，一旦有某个节点宕机就会引发如阻塞或者数据不一致的问题，解决方法为通过raft等方式将原本的单个节点转换为集群的形式，避免单点故障\n三阶段提交 三阶段提交的提出用于解决两阶段提交的阻塞问题，在原本两个阶段的中间添加了一个预提交阶段(Pre-Commit)，在预提交阶段，将第一阶段的结果发送给所有的参与者，这样在协调者宕机之后，依旧可以获取集群的状态，判断应该中止还是提交。\n流程\n准备阶段：\n协调者向参与者发送信息询问是否能够执行提交事务 参与者判断自身是否能够执行事务，但是并不真正执行事务，只是判断基本条件，给予响应 预提交阶段 协调者发送预提交信息 参与者如果满足条件则执行事务，并记录日志，响应协调者 提交阶段 协调者发送提交信息 参与者提交事务并给予相应 通过额外的预提交，在提交阶段时，如果协调者发生宕机，则可以根据之前的信息重新选举出一个新的协调者，来避免协调者称为单点故障阻塞系统，而如果在预提交阶段发生宕机，则可以随意选出一个协调者，由于没有执行，从而不存在数据一致性的问题。\n但是三阶段提交无法解决所有问题，会收到网络分区的影响：在两个分区内会各存在一个协调者，从而导致数据不一致。 paxos提交 paxos提交主要用于解决原本三阶段提交在发生网络分区时，随意选举协调者的问题，通过共识算法来选举leader，从而避免数据一致性问题。此处仅是对一个值或者结果来形成共识，因此采用的为basic paxos\n主要分为三个角色：\n资源管理者：相当于2PC 3PC当中的参与者，负责事务的一部分执行与提交，而在paxos当中，充当提议者(非leader) 领导者：协调paxos算法的执行，可以视为paxos和资源管理者之间的桥梁，领导者同时也是接受者 接受者：与资源管理者组成paxos集群，存储结果，通过paxos来避免网络分区 paxos算法开始于某个资源管理者，资源管理者决定提交事务，向leader发送一个BeginCommit leader收到BeginCommit之后，响应一个prepare消息给所有的资源管理者 之后所有的资源管理者根据条件去判断自身是否提交事务，如果能够提交事务，则发起一个commit的提案，否则为abort的提案 Leader 根据提案的结果，判断此次事务是否能够提交，之后将消息发送给资源管理者，资源管理者根据 leader 的结果进行提交或者回滚 相比raft，basic paxos的结构更为松散一些，raft所追求的就是follower完全复制leader的日志，从而在状态机上达成共识。而basic paxos只是对于某个值达成共识，提案者的概念并不等同于leader，提案者仅仅是进行提案而已，本身可以不参与数据的决策。\npaxos提交的结构更像是在事务的执行者外接了一个paxos,paxos只是用来存储结果，和防止网络分区的问题，实际上并不参与事务的执行或者判断的过程。\nSaga事务 实现分布式事务的代价较高，并且可能存在一些长事务，其中甚至可能会涉及到手动输入等人工介入等问题。使用常见方式处理，通常性能较低。而对于一些不需要完全进行隔离的事务，可以采用Saga事务。\nSaga事务通过一些列的子事务组成，子事务按照原本的方式进行执行，保证ACID，Saga事务整体通过补偿事务的方式来保证整个事务的原子性。即如果由T1,T2组成，T1提交之后，T2需要回滚，则运行一个T1的补偿事务，来撤销掉T1做出的操作。\nSaga事务最大的问题是无法保证隔离性。\n","permalink":"http://itfischer.space/en/posts/tech/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/","summary":"分布式系统模型 拜占庭将军问题 拜占庭将军主要用于解决共识问题，三个将军决定攻打拜占庭，只有三个人一同进行攻打才能够成功，否则则会失败，因此需要","title":"深入理解分布式系统"}]